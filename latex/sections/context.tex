\chapter{The context of the project}\label{ch:context}

This master's thesis took place at the \gls{u2is}, the computer science laboratory of \gls{ensta} (Paris Saclay Campus).
The laboratory provides access to a range of robots and sensors, each with specific configurations and capabilities.

During the thesis, the team was also involved in the CoHoMa\footnote{\textit{Collaboration Homme Machine}, Human Machin Collaboration} robotics challenge, which
focuses on deploying a fleet of \glspl{uav} and \glspl{ugv} to navigate and detect red boxes in unstructured outdoor environments.


\section{Robots}\label{sec:robots}

The primary \gls{ugv}s in the lab are currently two Barakuda from SharkRobotics~\cite{noauthor_barakuda_nodate}
and the main \gls{uav}s are two Tundra 1 from Hexadrone~\cite{noauthor_tundra_nodate}.
Before acquiring the Barakuda robots, the lab used several Husky from Clearpath Robotics~\cite{noauthor_husky_nodate} for
outdoors robotics experiments.

\subsection{Barakuda}\label{subsec:barakuda}

Multiple sensors have been integrated onto the Barakuda robots to enhance their perception capabilities.
Among them, two main cameras have been installed: the ZED 2i stereo \gls{rgbd} camera~\cite{noauthor_zed_nodate} and the AXIS Q62 PTZ camera~\cite{noauthor_axis_nodate}.
The ZED 2i is mounted at the front of the robot and is used to capture \gls{rgbd} images of the immediate environment, providing depth information for navigation and mapping tasks.
The AXIS Q62 PTZ camera is mounted on a mast approximately 30 cm above the robot's center and can be remotely controlled along the pitch and yaw axes, enabling wide-area visual coverage.
This is particularly useful during teleoperation, as it provides the operator with improved awareness of the \gls{ugv}'s footprint relative to its environment.
In addition to the two cameras, a \gls{lidar} sensor is mounted at the front of each robot: one Barakuda is equipped with an Ouster OSDome~\cite{noauthor_ouster_nodate},
while the other uses an OS1~\cite{noauthor_os1_nodate}.
Furthermore, an SBG Ellipse-D~\cite{noauthor_ellipse-d_nodate} is fixed to the robot in order to precisely measure its position.
As shown by \Cref{fig:context:barakuda_schematics}, all sensors are connected to a Jetson AGX Orin~\cite{noauthor_nvidia_nodate}.
\Cref{fig:appendix:barakuda_image} shows the physical positioning of the discussed components.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.99\linewidth]{illustrations/context/barakuda_schema}
    \caption{How the different elements interconnect between them.
    In blue, the remote controller of the manufacturer.
    In green, the components that were added at the lab.
    In orange, the original platform provided by the manufacturer.
    Dashed lines correspond to wireless connection.}
    \label{fig:context:barakuda_schematics}
\end{figure}

Docker containers running \gls{ros} Noetic are deployed on the Jetson.
These containers are used to manage the software environment, which can be challenging to configure directly on the Jetson, particularly for library dependencies.
Within the containers, \gls{ros} Noetic nodes handle sensing, data processing, and interaction with commands and the environment.

\subsection{Tundra}\label{subsec:tundra}

Tundra drones are equipped with a ViewPro Q10F~\cite{noauthor_q10f-single_nodate} gimbal-mounted camera
offering stabilized imaging with remote control over roll, pitch, yaw, and zoom.
A \gls{gnss} antenna is also integrated for precise positioning.
These components are placed as shown in \Cref{fig:appendix:tundra_image}.

Low-level control is managed by a Cube Orange~\cite{noauthor_cubepilot_nodate} flight controller,
which communicates with the onboard computer, sensors (\textit{e.g.}\ \gls{gnss} antenna), and remote control.
It runs the open-source ArduPilot firmware~\cite{noauthor_ardupilotardupilot_2025}.
% TODO add pixhawk?

\begin{figure}[ht!]
    \centering
    \includegraphics[width=.99\linewidth]{illustrations/context/tundra_schema}
    \caption{How the different elements interconnect between them.
    In blue, the remote controller of the manufacturer.
    In green, the components that were added at the lab.
    In orange, the original platform provided by the manufacturer.
    Dashed lines correspond to wireless connection.}
    \label{fig:context:tundra_schematics}
\end{figure}

An NVIDIA Jetson Orin Nano~\cite{noauthor_nvidia_nodate} is embedded on each Tundra to handle higher-level processing.
The Jetson is interconnected with other sensors and controller as describe by the schema in \Cref{fig:context:tundra_schematics}.
Unlike the Barakuda robots, no Docker containers are used.
\gls{ros} Noetic runs natively and manages sensing, data processing, and autonomy, though autonomous functions are currently limited.

\subsection{Wireless connection, teleoperation, monitoring}\label{subsec:wireless-connection-teleoperation-monitoring}

To enable inter-robot communication and remote control, the robots are interconnected via a mesh Wi-Fi network.
For teleoperation, each robot supports both the manufacturer's control solution and an in-house developed
remote controller integrated into a custom monitoring software.

The mesh Wi-Fi follows the IEEE 802.11s~\cite{noauthor_ieee_2021} standard and operates on the 5 GHz band to ensure high-speed communication.
Each robot is equipped with a compatible, properly configured router connected to its onboard computer,
allowing both the robots and our remote controller to access the \gls{ros} environment and retrieve relevant
data (\textit{e.g.}, \gls{gnss} coordinates, battery level, robot status, etc.). A key advantage over traditional 5 GHz Wi-Fi is that each router can extend the network range.
For example, an \gls{uav}, which more easily has line of sight to both the operator and the \gls{ugv}, can act as a relay to maintain communication and control of the \gls{ugv}
.

The teleoperation and monitoring are done via the lab solution, called mirador.
It is a Node.js app that can subscribe and publish \gls{ros} messages on each robot.
This makes it possible to see all the robot in the same \gls{ui} and control them in a centralized manner.
Mirador is usually used when to lab participate in robotics challenges.


\section{Current lab activities}\label{sec:current-lab-activities}

During my stay at the \gls{u2is}, the team finished the first milestone of Mobilex and was preparing the CoHoMa challenge.
Theses two robotics challenge have different goals while still sharing many of the same problematics.

Mobilex stands for \textit{MOBILit√© en environnement complEXe} (Mobility in complex environement) and is a
challenge organised by a French state entity that aims to foster defense innovation in France.
Mobilex consist of three milestone, increasingly difficult, that aims to develop autonomous mobile robotics
in complex environment.
It focuses on one \gls{ugv} and aims to make it able to perceive complex terrain (\textit{e.g.}\ slopes, unstructured roads, landslides, etc.) in
order to manage its local trajectory.
The automation of such tasks would allow to reduce cognitive burden on operators using robots in construction,
fire fighting or military vehicle.

CoHoMa, \textit{Collaboration Homme Machine} (Human Machin Collaboration), on the other hand, is a challenge organised
by a section of the French army.
In this challenge, teams have to prepare at least two pairs of \gls{uav} and \gls{ugv}.
For the third challenge, the goal was to protect a static camp from other, teleoperated, robots.
The team's robot could go explore the surrounding to prepare for the enemy robots.
All traps are red cubes while enemy robots have red cubes on them.
On the red cubes are QR Codes that contain code to deactivate traps and enemy robots.
Conversely, to Mobilex, the only constraint is to have strictly fewer operators than robots.
Hence, CoHoMa does not aim for full autonomy but of course reward teams that automate as much as possible tedious
and time-consuming tasks as it gives operators more time to do more complex overarching operation, harder to
automate properly.
For an autonomous navigation standpoint, the constraints and needs are very similar to Mobilex making it possible
reuse working technical bricks.

These two challenges are used by the lab as landmark to tests the current capabilities of its mobile robots.
It confronts theory with the real world and allows to better understand shortcomings and possible improvements
for the algorithms used on the robots.


\section{Current perception and autonomous capabilities}\label{sec:current-perception-and-autonomous-capabilities}

On these platform, the \gls{u2is} has tried to develop perception to allow for autonomous navigation on unstructured terrain.
Some work, created on Husky~\cite{noauthor_husky_nodate}---the previous
\gls{ugv} platform of the lab---used the front camera and the inertial information to infer the traversability of
possible paths~\cite{thomas_whojospatially-coherent-costmap_2024}.
In some ways it is similar to Fortin's \gls{uav}-assisted terrain awareness~\cite{fortin_uav-assisted_2024} as it uses \gls{imu} data to predict from a monocular image but
it does not try to create actual \gls{imu} data maps.
Although this work gave promising results, it is not the current navigation stack used on the robots.

Currently, the Barakuda robots use \gls{lidar} data to determine traversable areas.
Using the \gls{ros} package \textit{Elevation Mapping cupy} from \textcite{miki_elevation_2022}, the \gls{lidar} point cloud is processed to generate a 2.5D elevation map of the surroundings.
To accurately estimate the ground surface, the cloth simulation filtering algorithm is applied to the inverted point cloud.
The resulting 2.5D maps are converted into multiple occupancy grids using various height thresholds.
These grids are then aggregated and passed to a TEB planner~\cite{noauthor_teb_local_planner_nodate} for local planning, while
global path planning is handled by the basic Carrot planner~\cite{noauthor_carrot_planner_nodate}, which only provides a straight-line path toward the goal.
Regarding the low level control of the Barakuda's wheels, additional linear control steps were added as the manufacturer's
control only gave decent results when the motors were not underload.
Those added linear control algorithms are not ideal as they are constrained by the available input/output of the
Barakuda itself.

The main limitation for local autonomous navigation is currently the significant drift in the output of the SBG device (which uses \gls{gnss} and \gls{imu} data).
There is ongoing work to identify and resolve the issue, which may come from erroneous raw sensor data, poor calibration, incorrect configuration, or faulty data fusion.

%% TODO dlio ?
