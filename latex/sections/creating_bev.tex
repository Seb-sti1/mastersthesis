\chapter{Creating bird's-eye view from unmanned ground vehicule's image}\label{ch:creating-bev-from-ugv's-image}


As discussed in~\Cref{ch:analysis}, one of the main challenge of
finding correspondences between \gls{ugv} and \gls{uav} data is the difference in perspective.
Indeed, the \gls{ugv} sees from the ground and is inevitably close to it while the \gls{uav} has more a god's eye
perspective and has a more global picture of its surrounding.
This very difficulty is also what makes so interesting to create a system that can leverage both point of view.

In the literature, the usual way to reduce the gap in perspective in the imagery between the \gls{ugv} and the \gls{uav} is to transform
the images in what is called a \glsreset{bev}\gls{bev}.
As its name implies, the goal is to transform the image so that it appears like it was taken from above, like what a
bird's eye would see.\\
There are several methods to do this, the following subsections explore two tries of creating a \gls{bev} from the
\gls{ugv}'s image.


\section{Pointcloud reprojection of an RGBD image}

One possibility to obtain a \gls{bev} image is to use a \gls{rgbd} image, project its associated point cloud and then
flatten the point cloud from the top perspective.
Here, this is possible as the ground robots have a ZED 2i embedded on the front.
The intrinsics parameters of the ZED $f_x$, $f_y$, $c_x$, $c_y$, respectively the focal along x/y-axes of the camera
and the optical center (all in pixels), are given by the camera as additional metadata.\\
Using a pinhole camera model, the associated coordinates in the camera frame of each pixel are computed as follows.
Let the origin of the coordinate system be the center of projection, then the image plane, where the image forms, is at $z = f$.
Using the intercept theorem the point $\begin{bmatrix} % TODO add triangle intercept?
                                           x & y & z
\end{bmatrix}^T$ is projected at $\begin{bmatrix}
                                      \dfrac{f}{z} x & \dfrac{f}{z} y & f
\end{bmatrix}^T$.
Let the 2D coordinates on the image plane be $u$, $v$ along, respectively, the x and y axes and $i$, $j$ be the pixel
coordinates.
Finally, let $s_x$ and $s_y$ be, respectively the horizontal and vertical dimension of one pixel of the sensor array.
Using these notations and the previous results give \Cref{eq:pcd_rgbd:u} and \Cref{eq:pcd_rgbd:v}.
\begin{align}
    u &= \dfrac{f}{z} x & \dfrac{u}{s_x} &= (j - c_x) \label{eq:pcd_rgbd:u} \\
    v &= \dfrac{f}{z} y & \dfrac{v}{s_y} &= (i - c_y) \label{eq:pcd_rgbd:v}
\end{align}
Combining each set of equations and that by definition $f_x = \dfrac{f}{s_x}$ and $f_y = \dfrac{f}{s_y}$ gives the final result.
\begin{align}
    x &= (j - c_x) \dfrac{z}{f_x} \label{eq:pcd_rgbd:x} \\
    y &= (i - c_y) \dfrac{z}{f_y} \label{eq:pcd_rgbd:y}
\end{align}
As underlined by \Cref{eq:pcd_rgbd:x} and \Cref{eq:pcd_rgbd:y}, only the coordinates of the pixel in the image, the intrinsics
parameters and the depth are required to compute the actual coordinates in the camera frame.
That is why using a \gls{rgbd} camera is important as it gives the depth of each pixel.
These equations result in, for instance, the generated colorized point cloud visible in \Cref{fig:pcd_rgbd:pcd} using
\Cref{fig:pcd_rgbd:rgb} and \Cref{fig:pcd_rgbd:depth} as input data.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{illustrations/bev/zed_rgbd_rgb}
        \caption{The RGB image.}
        \label{fig:pcd_rgbd:rgb}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{illustrations/bev/zed_rgbd_d}
        \caption{The depth image.}
        \label{fig:pcd_rgbd:depth}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{illustrations/bev/zed_rgbd_pcd}
        \caption{The resulting colorized point cloud.}
        \label{fig:pcd_rgbd:pcd}
    \end{subfigure}
    \caption{Example of reconstruction of the point cloud using an \gls{rgbd} image.}
    \label{fig:pcd_rgbd:pcd_construction}
\end{figure}

Once the point cloud is computed, it needs to be flattened to a 2D image from the top perspective.
A simple way to do that is to consider rectangle columns with a square base.
The points, representing pixels, in the columns will be aggregated into one pixel.
The smaller the base is, the more detailed the resulting \gls{bev} is.

One parameter that can greatly influence the result is the aggregation function used.
The main goal here is to ensure that resulting \gls{bev} image keeps as much of the recognisable details.
The choice that was made is to use an average of all the pixels in the column.
On the one hand, one of the advantage of using an average, unlike for instance using the highest pixel, is that a single outlier will not affect the end results.
On the other hand, it will give new colors, not existing in the original image, when a column has different objects present in it, which is not ideal.
Other aggregation functions, like median or histograms to select the most representative pixel, were considered but the first (and only)
attempt was using an average.

The first major inconvenient arise from the use of python.
Python is very good to quickly create small snippet of code but is far from perfect when trying to obtain an efficient algorithm.
The current implementation, using numpy and other libraries, is taking slightly less than 9s on average for each input image with a base width of ten centimeters.
Using Numba~\cite{lam_numba_2015} to parallelize the computation of each column reduced it down to about 750 ms, corresponding to an 11.8x speedup.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{illustrations/bev/zed_rgbd_bev_0.05}
    \caption{The \gls{bev} obtained with a base width of 0.05m}
    \label{fig:pcd_rgbd:bev_0.05}
\end{figure}

Following all the described steps with \Cref{fig:pcd_rgbd:rgb} and \Cref{fig:pcd_rgbd:depth} results in \Cref{fig:pcd_rgbd:bev_0.05}.
Particular objects present in the image can be distinguished in the \gls{bev}: the white walls, the shadow of the tree and the tree itself.
The \gls{bev} is very (very) noisy, which is not satisfactory at all.
For instance, noise in the point cloud close to the tree branches create a white trace starting around the branches and ending at the right wall.
Another unacceptable issue is the sparseness of the image as the density of pixels drastically fall after about six meters and a half.

So, to try and fix these issue, a new version of the technique was implemented.
This time instead of only using points from one image, it accumulated them across multiple images similar to one of the weekly project
done in \textit{Perception for Autonomous Systems}. % 34759 Perception for autonomous systems Weekly Project Day 9 2024
To ensure proper alignment between two consecutive point cloud, ICP~\cite{besl_method_1992} is used to line them up.
Then two filtering steps are added to reduce the final size of the point cloud.
The first one exclude points too far away from the center and the second apply a voxel sampling with a size of $\dfrac{\text{column base width}}{2}$.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.4\textwidth]{illustrations/bev/zed_rgbd_merged_cloud_bev_0.05}
    \caption{A \gls{bev} generated using an accumulated point cloud and a column base width of 0.05m}
    \label{fig:pcd_rgbd:accumulated_bev_0.05}
\end{figure}

Unfortunately, after a few hundred iterations---corresponding to about twenty seconds---the point cloud grows from 100\ 000
points to 450\ 000 points, reducing the processing rate from around 1Hz to 0.5Hz.
Meanwhile, the ICP algorithm fails to maintain sufficient alignment of the accumulated data.
As a result, the generated \gls{bev} is of poor quality, as illustrated in \Cref{fig:pcd_rgbd:accumulated_bev_0.05}.
While the accumulation addresses the issue of sparseness, it introduces significant alignment errors, causing structures
like walls and grass to blend improperly.

Therefore, as these techniques did not yield the expected results, a new simpler technique based on using a homography to transform the image was implemented.
This approach offers a more straightforward and computationally efficient way to generate a birdâ€™s-eye view, facilitating further processing steps.
The following section details the implementation and evaluation of this method.


\section{Traditional homography}\label{sec:traditional-homography}

A more common approach at creating \gls{bev} images, is to use homography.
In this specific case, a homography is a function that will shift, rotate and scale the image.
Moreover, with the assumption of a locally flat ground, it is possible to compute the real world
position on the ground of specific pixels.
Once computed the pixel coordinates and the real world coordinates on the ground can be used to determine the
homography that needs to be done to transform the image from the \gls{ugv} to a \gls{bev}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{illustrations/bev/homography}
    \caption{The camera of the \gls{ugv} is slightly rotated toward the ground. As it is rotated and under
    the assumption of a locally flat ground, the pixels below the vertical middle will intersect the ground.
    A, B, the vertical middle points, C, D, the bottom corners of the image are determined to create the \gls{bev}.}
    \label{fig:homography:homography}
\end{figure}


\Cref{fig:homography:homography} shows the setup of the camera on the \gls{ugv}.
In the case of the lab, the ZED 2i is rotated by 15.5 degrees.
For a given point $P$ of the ground in the camera's field of view, let $x_c$, $y_c$, $z_c$ and $x_r$, $y_r$, $z_r$ be its
coordinates in, respectively, the camera and robot frame.
Using the locally flat ground hypothesis (and defining the ground in the robot frame as $z=0$), $z_r = 0$.
Furthermore, as the geometry of the robot is fix and determined, there is a transformation $T_{c\rightarrow r}$ that
transforms the position vector in the camera frame to its counterpart in the robot frame.
Finally, as $P$ is in the camera's field of view, \Cref{eq:pcd_rgbd:x} and \Cref{eq:pcd_rgbd:y} still holds, which gives
the \Cref{eq:homography:complete}.

\begin{align}
    \label{eq:homography:complete}
    \begin{bmatrix}
        x_r \\
        y_r \\
        0   \\
        1
    \end{bmatrix} &= T_{c\rightarrow r} \times
    \begin{bmatrix}
    (j - c_x)
        \dfrac{z_c}{f_x}\\
        (j - c_y) \dfrac{y_c}{f_y} \\
        z_c                        \\
        1
    \end{bmatrix}
\end{align}

\Cref{eq:homography:complete} is solvable, as $z_c$ can be determined using the third row, which then leads to $x_r$ and $y_r$.
Doing this for A, B, C, D (see \Cref{fig:homography:homography}), gives the projected corners of the bottom half of the image.
The last step is to convert the units from meters to pixels, flipping and shifting the origin and axes to transform from the robot's frame
to the \gls{bev}'s frame.
In the \gls{bev}'s frame the origin is in the top-left corner while the x and y axes are in the opposite direction.


Now that the original pixel coordinates and the new pixel coordinates are computed, the homography can be computed.
Here, it is done using \textit{OpenCV}'s implementation~\cite{bradski_opencv_nodate} which uses \textit{RANSAC}~\cite{fischler_random_1981}.
The final step of transforming the image using the homography is also done by a function from \textit{OpenCV}~\cite{bradski_opencv_nodate}.


\begin{figure}[ht!]
    \begin{subfigure}[t]{0.35\textwidth}
        \includegraphics[width=0.99\textwidth]{illustrations/bev/homography_zed}
        \caption{The raw image from the ZED 2i.}
        \label{fig:homography:zed}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.58\textwidth}
        \includegraphics[width=0.99\textwidth]{illustrations/bev/homography_zed_bev}
        \caption{The resulting \gls{bev}.}
        \label{fig:homography:zed_bev}
    \end{subfigure}
    \hfill
    \caption{Result of the transformation to a \gls{bev} using an homography.}
    \label{fig:homography:example}
\end{figure}

The final result, an example of which is shown in \Cref{fig:homography:example}, is clearer than expected and is the one used in the rest of this work.
It also highlights limitations of this technique.
First, the more an object is for the camera, the more it is deformed and blured.
Second, any object that is not on the flat ground will be deformed compared to how it should be in a \textit{perfect} \gls{bev}.