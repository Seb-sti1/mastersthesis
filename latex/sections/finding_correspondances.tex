\chapter{Finding correspondences between unmanned ground vehicle and unmanned aerial vehicule data}\label{ch:finding-correspondances-between-ugv-and-uav-data}

The previous section aims at reducing the gap in terms of perspective between the \gls{ugv} and the \gls{uav}.
The next step is to use the available data to create algorithms making the \gls{ugv} recognize what the \gls{uav} has seen.
After some initial research in different directions, both of dense and sparse correspondence, as XFeat~\cite{potje_xfeat_2024}
and OmniGlue~\cite{jiang_omniglue_2024} seemed promising, the testing and implementation focused on those algorithms.

To test different versions of the created algorithms, I used the dataset of~\textcite{fortin_uav-assisted_2024} kindly provided by Jean Michel Fortin.
In the absence of an \gls{uav}/\gls{ugv} dataset at the \gls{u2is}, this greatly helped as it was very close the ideal
dataset for the tested algorithms.
The \gls{ugv} used is a Warthog of Clearpath Robotics with a ZED X, a Xsens Mti-30 \gls{imu} and has \gls{gnss} with RTK localization.
The \gls{uav} is a DJI Mavic 3E.
Jean-Michel Fortin shared 2055 aerial and 2082 ground images (with their \gls{bev} projection already computed), along
with telemetric data, like \gls{imu}, \gls{gnss}, odometry etc.
The dataset was created at the Montmorency forest, Québec, Canada\footnote{Precisly at 47°19'14.9"N 71°8'52.2"W.}.
The main limitation came from the difference in objective of the dataset:~\textcite{fortin_uav-assisted_2024} benefits from
having the \gls{ugv} in the \gls{uav}'s frame while this work would prefer not having the \gls{ugv} in frame.
This is mainly to simulate more closely to the actual use case.


\section{First tests along the unmanned ground vehicle's path}

XFeat~\cite{potje_xfeat_2024} is a lightweight CNN architecture designed for sparse keypoint detection and semi-dense
feature matching, optimizing robustness, accuracy, and efficiency.
It processes high-resolution grayscale images through six convolutional blocks that reduce spatial resolution while increasing channel depth.
The network outputs dense descriptors, reliability, and keypoint heatmaps.
Keypoint detection uses a novel parallel branch, and matching employs a lightweight refinement module.
XFeat is trained in a supervised manner with pixel-level ground truth correspondences from the MegaDepth and COCO\_20k subset of the COCO2017 dataset.
Additionally, a lighter version of LightGlue (lighterglue) was trained using features from XFeat.\\
OmniGlue~\cite{jiang_omniglue_2024}, on the other hand, is designed with generalization as a core principle.
It first extracts fine-grained features with SuperPoint and broad features with DINOv2.
It then builds intra- and inter-image keypoint graphs, using DINOv2 to guide inter-image associations.
Information is propagated via attention, disentangling positional and appearance signals to improve generalization.
Finally, refined descriptors are optimally matched to find correspondences between keypoints.
OmniGlue is also trained in a supervised manner using ground truth from previous work.


The initial try to benchmark XFeat and OmniGlue aimed at testing that given an \gls{ugv}'s picture and an \gls{uav}'s picture
the number of correspondences between the two images would be close to zero when the images do not overlap, would
progressively increase as they overlap more and more until reaching a maximum when they would capture the same area.
Of course, if the number of correspondences, regardless as if it is high or low, is unchanged for any pair of images,
it would then prove that these algorithms are not able to find real correspondences.
Therefore, a 6x2 grid of patches in front of the \gls{uav} and the \gls{ugv} were generated and the \gls{gnss} coordinates of each patch was determined.
Then XFeat and OmniGlue generated the correspondences.
Sadly, the shear quantity of patches generated (around $6 \times 10^3$ for each robot) and the number of possible comparisons
(with a time complexity of $\mathcal{O}(n^2)$ in the number of patches), made it impossible to make enough comparison to
obtain any meaningful data.


To solve this, new method was introduced: this time the aims is to ensure that the algorithms are \textit{not} not working.
Using XFeat and OmniGlue on patches in front of the \gls{ugv} in both the \gls{ugv} and \gls{uav} images, if no correspondences
are found then it would mean that these algorithm can not work at all.
First, the patches are both selected so that they roughly have the same real world size.
Second, as they are created in front of the \gls{ugv} in a dataset where the \gls{uav} can see the \gls{ugv} it ensure
that two patches are nearly the same area.
Third, as the \gls{uav} sees the \gls{ugv}, it is possible to align the patches extracted in the \gls{uav} image to the
\gls{ugv} ensuring that both patches are roughly aligned.
In a sense the algorithms are tested in the best case scenario, to make sure that they can actually work with that.
Although, this working would not mean that these \gls{ai}s are sufficiently robust to work in a more representative scenario.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{illustrations/find_correspondances/x_feat_found3_kp}
        \caption{XFeat's results.}
        \label{fig:find_corr:along_traj:xfeat}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{illustrations/find_correspondances/omniglue_found1_kp}
        \caption{OmniGlue's results.}
        \label{fig:find_corr:along_traj:omniglue}
    \end{subfigure}
    \caption{Results of both XFeat and OmniGlue when generating patches in front of the \gls{ugv}.
    The left one is from the \gls{ugv}'s bev, the right one is from the \gls{uav}.
    The lines show the correspondences found by the model.
    Using those correspondences, it is possible to compute an homography transforming a point in the \gls{ugv}'s patch
    to a point in the \gls{uav}'s patch.
    Using this homography the border of the \gls{ugv}'s patch are drawn on the \gls{uav}'s patch as a green quadrilateral.}
    \label{fig:find_corr:along_traj:results}
\end{figure}

% TODO plot correspond count along path

Luckily, XFeat gives very promising results while OmniGlue gives more mixed results.
As shown by \Cref{fig:find_corr:along_traj:xfeat}, XFeat found many correspondences and the resulting homography places
the \gls{ugv}'s patch border on a very reasonable area.
Other examples show that it also works with different type of terrains. % TODO appendix
On the contrary, \Cref{fig:find_corr:along_traj:omniglue} shows one of the very few coherent correspondences found by OmniGlue.
It also has a tendency of generating incoherent matches as more often than not, the reprojection of the \gls{ugv}'s patch
border result in a physically impossible quadrilateral given the constraints and method of generating the patches.
For instance, some are crossed quadrilateral, quadrilateral where three points are nearly aligned or unreasonably small quadrilateral.

%\section{Define metrics to investigate performances} % TODO add this section?

Before going further into benchmarking, better performance indicator and metrics are required.
The first one if a very simple \textit{obviously wrong} indicator.
To properly define lets first define two areas.
The first one is obtained by projecting the \gls{ugv}'s patch borders in the \gls{uav}'s patch and then clipping it so that it
is contained inside the aerial patch.
The area in pixel of the obtained quadrilateral is the first surface.
Then second surface is obtained by using the clipped quadrilateral and by reprojecting in the ground patch.
The resulting quadrilateral's area is the second surface.
A correspondence is considered \textit{obviously wrong} when it verifies any of the following conditions:
\begin{itemize}
    \item the first or second areas previously defined are less than $10000$ pixels$^2$ ;
    \item any of the quadrilaterals is a crossed quadrilateral ;
    \item when stretching the quadrilaterals of the ground and aerial patches to a square, the number of completely black pixel is more than 20 pixels.
\end{itemize}
These conditions are here to ensure next indicators can properly work.
The reason will be made clear shortly.

Speaking of the other indicators, the \gls{mse} and the \gls{ssim} are additionally used.
The \gls{mse} is, as indicated by the name, the mean of the squared difference between the two patches.
The \gls{ssim} aims at solving some shortcoming of the \gls{mse} by also taking texture into account\footnote{A
very explanative picture on \href{https://scikit-image.org/docs/0.25.x/auto_examples/transform/plot_ssim.html}{scikit's website}
shows that a noise, that impacts the texture of the image, will be better detected compare to the addition of a constant,
    that don't impact the texture of the image as much, by \gls{ssim} than \gls{mse}}.
That being said, both indicators in the current context can give false positive results very easily.
This all comes down to the method that is used to obtain the images that are compared.
The previous paragraph described how to obtain quadrilateral corresponding to the border of the ground patch clipped to the
border of the aerial patch, in both the ground and aerial patch.
Those two quadrilateral are transformed to square images of the same size, and they correspond to the common area between the ground and the aerial patch.
After a normalization step to reduce the impact of using different cameras with different colorimetries, the resulting patches are compared.

In the average case where the algorithms provide reasonable match, these indicators work great.
Issues arise when absurd correspondences are found, like unreasonably small quadrilaterals, cross quadrilaterals, etc.
For instance, \gls{ssim} rewards the existence of similar homogeneous texture, which is exactly what an unreasonably small quadrilaterals
will generate when being stretch to a square.
This is the purpose of the \textit{obviously wrong} indicator: flag any obviously wrong matches that are likely
to obtain good values when tested using \gls{mse} and \gls{ssim}.


\section{Test robust to relative rotations}

In the Montmorency dataset, the \gls{ugv} has an aruco tag on it, making it very easy to detect its orientation relative
to the \gls{uav} frame.
This is how generating aerial patch aligned to the ground patch is possible.
When it comes to the ground patches they are simply generated as a square at the center of the \gls{ugv}'s \gls{bev}.
To expand the benchmark further, the comparison of aerial/ground patches was also made between aerial and 90°/180°/270°
rotation of the ground.
This will make it possible to measure if the algorithms are robust to rotation.
Also, these angles are interesting as they leave the values of the pixels unchanged contrary to any other rotation.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.90\linewidth]{illustrations/find_correspondances/big_angles_obviously_wrong}
    \caption{The ratio of \textit{obviously wrong} correspondance for each algorithm tested.
        $0\%$ is the best, $100\%$ is the worst.}
    \label{fig:find_corr:big_angles:obviously_wrong}
\end{figure}

For three different versions of xfeat, OmniGlue and orb, couple hundred comparisons (for each angle 0°, 90°, 180° and 270°) were made.
For each comparison, the indicators were computed.
\Cref{fig:find_corr:big_angles:obviously_wrong} shows the \textit{obviously wrong} values of each algorithm tested and each angle.
It indicates that any version of xfeat outperforms OmniGlue and orb.
Furthermore, it clearly shows that the algorithms have more problems with angled images and that traditional method like orb just don't work on the problem at hand.
By definition of the \textit{obviously wrong} indicator, it is hard to extract more information out of this graph, as only having
a high \textit{obviously wrong} ratio can be interpreted as bad, having a low \textit{obviously wrong} ratio does not give enough
information to conclude anything.

Continuing, the \gls{ssim} and the \gls{mse} can be graphed, here in function of the number of detected correspondences.
\gls{ssim} and \gls{mse} will never be at their best values, respectively 1 and 0, given the images that are compared.
They are not from the same camera, not necessary taken at the same time, etc.
Through manual verification, it was determined that \gls{ssim} values around 0.3 and \gls{mse} below 2500 usually correspond
to correct matches.
Looking at \Cref{fig:find_corr:big_angles:mse_ssim_xfeat+lg_0}, XFeat (lighterglue version) shows good \gls{ssim} values
between 0.2 and 0.4 for the best matches.
In addition, when over 600 matches, a clear correlation between the number of matches and better indicators values
can be observed.
Nevertheless, still with XFeat (lighterglue version), rotating patches by 90° gives poor results as most comparison as
\gls{ssim} below 0.2 (see \Cref{fig:find_corr:big_angles:mse_ssim_xfeat+lg_90}).
This result is not an outlier as any non-zero angled benchmark gives insufficient \gls{ssim} and \gls{mse} values.
Regarding OmniGlue, \Cref{fig:find_corr:along_traj:omniglue} is a good example showing that it generally generates fewer correspondences.
It is not, in principle, a problem but here \gls{ssim} values are also notably lower.
As a consequence, it is fair to say that XFeat is better than OmniGlue and that none are robust to general rotations (from 0° to 360°).
% TODO add more info if possible to have average/statistics on indicators

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/find_correspondances/big_angle_0_scores_xfeat+lighterglue}
        \caption{XFeat (ligherglue version) with both patches aligned.}
        \label{fig:find_corr:big_angles:mse_ssim_xfeat+lg_0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/find_correspondances/big_angle_90_scores_xfeat+lighterglue}
        \caption{XFeat (ligherglue version) with a rotational shift of 90°.}
        \label{fig:find_corr:big_angles:mse_ssim_xfeat+lg_90}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/find_correspondances/big_angle_0_scores_omniglue}
        \caption{OmniGlue with a rotation with both patches aligned.}
        \label{fig:find_corr:big_angles:mse_ssim_omniglue_0}
    \end{subfigure}
    \caption{The \gls{ssim} and \gls{mse} values for comparisons made by different algorithms at a given angle of rotation of the \gls{ugv} \gls{bev} patch.
    For clarity reasons, the \textit{obviously wrong} matches are disregarded.
    For \gls{mse}, the lower, the better. For \gls{ssim}, 1 is the best, 0 is the worst.}
    \label{fig:find_corr:big_angles:mse_ssim}
\end{figure}

Not being robust to general rotations seemed problematic to solve the problem at hand.
Indeed, the \gls{uav} will collect the data before the \gls{ugv}'s trajectory is decided.
Worse than not knowing the orientation of the \gls{ugv} when it will navigate, it is not yet defined.
However, the hypothesis that can be made is that both know their orientation and can agree, ahead of time with a reference
orientation defining how the patches should be oriented when created.
Now, XFeat and OmniGlue would only have to be robust to small rotation of -10° to +10° to handle imprecision in the orientation of the robots.
In order to test this, the absolute orientation of the \gls{ugv} and \gls{uav} in the Montmorency dataset
were computed with the available telemetric data.
A reference orientation was chosen and patches were extracted accordingly.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.90\linewidth]{illustrations/find_correspondances/small_angles_obviously_wrong}
    \caption{The ratio of \textit{obviously wrong} correspondance for each algorithm tested.
        $0\%$ is the best, $100\%$ is the worst.}
    \label{fig:find_corr:small_angles:obviously_wrong}
\end{figure}

Again, it is possible to look at the \textit{obviously wrong} ratio (see \Cref{fig:find_corr:small_angles:obviously_wrong}),
which reveals that there are no significant disadvantage of having small rotation.
Going further and analysing the \gls{mse} and \gls{ssim}, \Cref{fig:find_corr:small_angles:mse_ssim} shows that xfeat (lighterglue version)
manage rotations really well giving good indicator values.
Another aspect is that the correlation between a good match (high \gls{ssim} and low \gls{mse}) and the number of correspondences
makes it possible to create a confidence degree related to how good the match is.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/find_correspondances/small_angle_0_scores_xfeat+lighterglue}
        \caption{Both patches aligned.}
        \label{fig:find_corr:small_angles:mse_ssim_xfeat+lg_0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/find_correspondances/small_angle_5_scores_xfeat+lighterglue}
        \caption{Rotational shift of 5°.}
        \label{fig:find_corr:small_angles:mse_ssim_xfeat+lg_5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/find_correspondances/small_angle_-10_scores_xfeat+lighterglue}
        \caption{Rotational shift of -10°.}
        \label{fig:find_corr:small_angles:mse_ssim_xfeat+lg_-10}
    \end{subfigure}
    \caption{The \gls{ssim} and \gls{mse} values for comparisons made by XFeat (ligherglue version) at a given rotational shift betwen both patches.
    For clarity reasons, the \textit{obviously wrong} matches are disregarded.
    For \gls{mse}, the lower, the better. For \gls{ssim}, 1 is the best, 0 is the worst.}
    \label{fig:find_corr:small_angles:mse_ssim}
\end{figure}

Moreover, OmniGlue gives decent robustness to rotations but is still worse than XFeat.
Also, another aspect to consider is the computational time required to execute these algorithms.
While xfeat with lighterglue is at a comfortable 0.05s on average, OmniGlue is at about 0.5s with some iterations reaching 1s.
Regarding the other version of XFeat (star, and vanilla), while they are faster, respectively 5 and 2.5 times, their performances
at finding are also worse.
Here the XFeat with lighterglue, is fast enough: detecting if an intermediate goal of the global navigation is reached,
don't require high frequencies, 0.5Hz to 1Hz would suffice.
In this interval, without even using parallelization, twenty to forty comparison can be done.
%TODO check if this the matching time that is measured or all of it
Hence, given all the results discussed above the next part of this work will study XFeat (lighterglue version) in the context of simulating
the reconnaissance flight of an \gls{uav} followed by an \gls{ugv} navigating while trying to detect location along its trajectory previously
recognized by the \gls{uav}.


