\chapter{Topological map creation}\label{ch:topological-map-creation}

As discussed in~\ref{sec:problem-at-hand}, the main purpose of using a topological map is to retain only the necessary information about the environment.
A traditional map scales quadratically with the areaâ€™s width, whereas a topological map grows linearly with the number of nodes.
These nodes, typically representing intersections, are the key locations where information is stored.
The number of generated node is an important subject as it is the main parameter to regulate the final map size.
This approach relies on the assumption that the \gls{ugv} can use local navigation to reach a target when the required trajectory is approximately a straight line.


\section{Fast Color/Texture Segmentation For Outdoor Robots}

In~\cite{rasmussen_appearance_2009}, the team at the university of Delaware, uses the method describe in~\cite{rufus_blas_fast_2008},
lidar data and geometric constraints, in order to detect a forest trail and make an \gls{ugv} follow it.\\
As both articles seemed to gave promising results, a first attempt to segment \gls{uav} imagery using the method describe in
~\cite{rufus_blas_fast_2008}.

As, at this point, as no dataset using U2IS' \gls{uav} was available, the publicly available~\cite{noauthor_aukerman_nodate} was used.
It is a dataset of the Aukerman Park\footnote{15561 York Rd, North Royalton, OH 44133, United States}, containing 78
4896 x 3672 images captured with a Sony DSC-WX220.
It was chosen as it contains trails, grass, trees, gravel/sand paths and asphalt roads from an aerial perspective which
seemed like a good variety of terrains (see~\cref{ch:aukerman-park-overview}).

\cite{rufus_blas_fast_2008} article's method, starts by converting the input RGB image to the \textit{CIE-Lab} space.
Then for each pixel, descriptors are computed using~\cref{eq:fast_color:descriptor} where the first three features
describe color while the remaining eight describe the local texture.
\begin{align}
    \label{eq:fast_color:descriptor}
    p_{i,j} = \begin{bmatrix}
                  W_1 \times L_c         \\
                  W_2 \times a_c         \\
                  W_2 \times b_c         \\
                  W_3 \times (L_1 - L_c) \\
                  \vdots                 \\
                  W_3 \times (L_8 - L_c) \\
    \end{bmatrix}
\end{align}
The pixel descriptors are then clusterized using the well-known k-means algorithm, which returns clusters of similar
pixel (in the sense of close in the descriptor space) and their centroid, called \textit{textons}.
At this time a filtering of the outliers is applied to remove descriptor too far away from their \textit{textons}.
For each pixel, a new feature vector is created by looking at the repartition/histogram of the clusters in a local
window centered on the pixel.
In a sense the \textit{textons} describe the image at a micro level while the histograms describe the composition in terms
of density of each \textit{textons} in a local area.
Furthermore, the histograms are clustered again use k-means and a final step uses the \gls{emd} to
merge histogram cluster having a centroid too close to each other.

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[t]{.29\textwidth}
        \includegraphics[width=.99\textwidth]{illustrations/topological_map_creation/fast_color_raw1}
        \caption{Raw image used as input}
        \label{fig:fast_color:raw_image}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.69\textwidth}
        \includegraphics[width=0.99\textwidth]{illustrations/topological_map_creation/fast_color_hist1}
        \caption{2D histogram for three given texons. The greener, the higher the density of the texton.}
        \label{fig:fast_color:histograms}
    \end{subfigure}
    \caption{Intermediate results of the algorithm.}
    \label{fig:fast_color:intermediate_histograms}
\end{figure}

The first step of find the textons ensures that the algorithm is able to segment very different environment, by
creating a solution tailored to its specificities.
The second step of creating histograms is where the intermediate results already reveal structural information of the image.
\Cref{fig:fast_color:intermediate_histograms} shows carefully selected histogram of three textons obtain when using % TODO don't like  carefully selected
\Cref{fig:fast_color:raw_image} as input.
\Cref{fig:fast_color:histograms} shows that the density of texton reveals interesting areas of the image.
For instance, the first histogram gives high density to grass and low density to the paths, while the second one
seems to give higher average values for trees and the last one gives higher average density for the grass.

The final steps start from the histograms and create new feature vectors for each pixel, the features being the
value of each histogram at that given pixel.
With this mechanism, the last clusterization occurs on features representing the composition of a local area not the
local patterns themselves.
The very final step of the \gls{emd} is here to merge clusters too close to each other as the output of the
k-means algorithm will always be k clusters which is not necessary the number of clusters in the image.
By varying the minimal distance under which the clusters are merged together, it gives different segmentation as shown by
\cref{fig:fast_color:final_cluters_comparaison}.

For this particular example, The cluster one, two, four seem to correspond to the path and grass while other clusters
are have meaning.
Making the parameters vary (number of textons, number of cluster for the histogram, histogram window size) gave different
results, none seemed clear enough to be used to properly segment traversable \\
This seems like the main limitation of using k-means to find textons to find very specific areas in a picture.
Given a properly and exhaustive dataset, an interesting change to try would be to define textons for different surfaces
and create the initial cluster using KNN algorithm.

% TODO another limitation it is that is slow

\begin{figure}
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=0.99\textwidth]{illustrations/topological_map_creation/fast_color_before_emd}
        \caption{The clusters before merging using the \gls{emd}. minimal distance of 0.}
        \label{fig:fast_color:before_emd}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=0.99\textwidth]{illustrations/topological_map_creation/fast_color_emd_10}
        \caption{Final clusters using \gls{emd} minimal distance of 10.}
        \label{fig:fast_color:emd_10}
    \end{subfigure}
    \hfill
    \caption{Final clusters given two values of minimal \gls{emd}.}
    \label{fig:fast_color:final_cluters_comparaison}
\end{figure}


\section{Detecting trees in order to find traversable region}

As mentioned in the previous section, having a dataset to improve different method by applying machine learning, seems
like an interesting idea to try.
For instance,~\cite{zhang_dual-bev_2025} uses a retrained version of~\cite{ronneberger_u-net_2015} to segment satellite imagery.
Sadly, after some research, in the available timeframe, nothing seemed to correspond to what was needed in order to
apply the same technic to this subject.
Fortunately, during those research, the already pretrained model~\cite{bosch_journal_2020} was found.
It applies an already existing technics to segment tree/non-tree pixels.
Therefore, the goal becomes to segment trees and consider that everywhere else, the robot would be able to traverse
or at least find a traversable path avoiding not segmented out obstacles (e.g.\ negative obstacles, rocks, etc.).

The \gls{ai} model proposed is train using supervised learning using the following steps.
First, images are split in tiles and in order to reduce cost of labelling tiles while keeping variety in the dataset,
using GIST descriptors, k means clusters the tiles.
k is chosen so that the number of clusters is about 1\% the number of tiles.
Then only the tiles closest to the centroids are labelled, either manually or using other available data (e.g.\ LiDAR measurements)
Using these labelled tiles as ground truth, a binary Adaboost classifier is train.
The input vector is twenty-seven features long, six encoding color, 18 texture and 3 entropy.
Finally, a refinement method is used to avoid sparse pixels.

\begin{figure}
    \begin{subfigure}[t]{.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/topological_map_creation/detect_tree_b50_r10000}
        \caption{Segmentation of detect tree using the default refinement parameters ($\beta = 50$, $\text{rescale} = 10000$)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/topological_map_creation/detect_tree_b1000_r10000}
        \caption{Segmentation of detect tree using $\beta = 1000$, $\text{rescale} = 10000$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.32\textwidth}
        \includegraphics[width=0.99\linewidth]{illustrations/topological_map_creation/detect_tree_b1000_r30000}
        \caption{Segmentation of detect tree using $\beta = 1000$, $\text{rescale} = 30000$}
    \end{subfigure}
    \caption{Segmentation of detect tree using \cref{fig:fast_color:raw_image} as input.}
    \label{fig:detect_tree:results}
\end{figure}


The authors provide a version of their model trained Zurich's Orthofoto Sommer 2014/15.
Inferring with this model and \cref{fig:fast_color:raw_image} as input gives the results in \cref{fig:detect_tree:results}.
Furthermore, it shows different refinement parameters and how it affects the final results of the segmentation.
A higher value of $\beta$ seems to make the classification less noisy while a higher value does the opposite.

% TODO performance
% TODO manually label trees in aukerman and make a metric


% TODO post-processing (refined using erosion dilation)
% TODO on satellite map (not good enough)

Even though detect tree gave very promising results on \gls{uav} imagery, many more steps are required to be able
to use this in order to create the backbone of the topological map (i.e.\ placement of node and vertices).
As, with a carefully though \gls{ui}, this task can be done by a user in matter of minutes if not seconds, it seemed
more reasonable to focus the remaining time on developing methods to gather \gls{uav} data in chosen nodes and
make than recognisable by the \gls{ugv}.
