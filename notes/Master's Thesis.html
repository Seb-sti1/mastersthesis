<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta http-equiv="Content-Security-Policy" content="default-src 'none'; img-src data:; style-src 'unsafe-inline' data:" />
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,Ym9keSB7Cgljb2xvci1zY2hlbWU6IGxpZ2h0IGRhcms7CgkvKiBUaGVzZSBzaG91bGQgYmUgdGhlIGRlZmF1bHRzLCBidXQganVzdCBpbiBjYXNlOiAqLwoJYmFja2dyb3VuZDogQ2FudmFzOwoJY29sb3I6IENhbnZhc1RleHQ7Cn0KCmEgewoJdGV4dC1kZWNvcmF0aW9uOiB1bmRlcmxpbmU7Cn0KCmJvZHkgewoJcGFkZGluZzogMDsKfQoKdWwucmVwb3J0IGxpLml0ZW0gewoJYm9yZGVyLXRvcDogNHB4IHNvbGlkICM1NTU7CglwYWRkaW5nLXRvcDogMWVtOwoJcGFkZGluZy1sZWZ0OiAxZW07CglwYWRkaW5nLXJpZ2h0OiAxZW07CgltYXJnaW4tYm90dG9tOiAyZW07Cn0KCmgxLCBoMiwgaDMsIGg0LCBoNSwgaDYgewoJZm9udC13ZWlnaHQ6IG5vcm1hbDsKfQoKaDIgewoJbWFyZ2luOiAwIDAgLjVlbTsKfQoKaDIucGFyZW50SXRlbSB7Cglmb250LXdlaWdodDogNjAwOwoJZm9udC1zaXplOiAxZW07CglwYWRkaW5nOiAwIDAgLjVlbTsKCWJvcmRlci1ib3R0b206IDFweCBzb2xpZCAjY2NjOwp9CgovKiBJZiBjb21iaW5pbmcgY2hpbGRyZW4sIGRpc3BsYXkgcGFyZW50IHNsaWdodGx5IGxhcmdlciAqLwp1bC5yZXBvcnQuY29tYmluZUNoaWxkSXRlbXMgaDIucGFyZW50SXRlbSB7Cglmb250LXNpemU6IDEuMWVtOwoJcGFkZGluZy1ib3R0b206IC43NWVtOwoJbWFyZ2luLWJvdHRvbTogLjRlbTsKfQoKaDIucGFyZW50SXRlbSAudGl0bGUgewoJZm9udC13ZWlnaHQ6IG5vcm1hbDsKfQoKaDMgewoJbWFyZ2luLWJvdHRvbTogLjZlbTsKCWZvbnQtd2VpZ2h0OiA2MDAgIWltcG9ydGFudDsKCWZvbnQtc2l6ZTogMWVtOwoJZGlzcGxheTogYmxvY2s7Cn0KCi8qIE1ldGFkYXRhIHRhYmxlICovCnRoIHsKCXZlcnRpY2FsLWFsaWduOiB0b3A7Cgl0ZXh0LWFsaWduOiByaWdodDsKCXdpZHRoOiAxNSU7Cgl3aGl0ZS1zcGFjZTogbm93cmFwOwp9Cgp0ZCB7CglwYWRkaW5nLWxlZnQ6IC41ZW07Cn0KCgp1bC5yZXBvcnQsIHVsLm5vdGVzLCB1bC50YWdzIHsKCWxpc3Qtc3R5bGU6IG5vbmU7CgltYXJnaW4tbGVmdDogMDsKCXBhZGRpbmctbGVmdDogMDsKfQoKLyogVGFncyAqLwpoMy50YWdzIHsKCWZvbnQtc2l6ZTogMS4xZW07Cn0KCnVsLnRhZ3MgewoJbGluZS1oZWlnaHQ6IDEuNzVlbTsKCWxpc3Qtc3R5bGU6IG5vbmU7Cn0KCnVsLnRhZ3MgbGkgewoJZGlzcGxheTogaW5saW5lOwp9Cgp1bC50YWdzIGxpOm5vdCg6bGFzdC1jaGlsZCk6YWZ0ZXIgewoJY29udGVudDogJywgJzsKfQoKCi8qIENoaWxkIG5vdGVzICovCmgzLm5vdGVzIHsKCWZvbnQtc2l6ZTogMS4xZW07Cn0KCnVsLm5vdGVzIHsKCW1hcmdpbi1ib3R0b206IDEuMmVtOwp9Cgp1bC5ub3RlcyA+IGxpOmZpcnN0LWNoaWxkIHAgewoJbWFyZ2luLXRvcDogMDsKfQoKdWwubm90ZXMgPiBsaSB7CglwYWRkaW5nOiAuN2VtIDA7Cn0KCnVsLm5vdGVzID4gbGk6bm90KDpsYXN0LWNoaWxkKSB7Cglib3JkZXItYm90dG9tOiAxcHggI2NjYyBzb2xpZDsKfQoKCnVsLm5vdGVzID4gbGkgcDpmaXJzdC1jaGlsZCB7CgltYXJnaW4tdG9wOiAwOwp9Cgp1bC5ub3RlcyA+IGxpIHA6bGFzdC1jaGlsZCB7CgltYXJnaW4tYm90dG9tOiAwOwp9CgovKiBBZGQgcXVvdGF0aW9uIG1hcmtzIGFyb3VuZCBibG9ja3F1b3RlICovCnVsLm5vdGVzID4gbGkgYmxvY2txdW90ZSBwOm5vdCg6ZW1wdHkpOmJlZm9yZSwKbGkubm90ZSBibG9ja3F1b3RlIHA6bm90KDplbXB0eSk6YmVmb3JlIHsKCWNvbnRlbnQ6ICfigJwnOwp9Cgp1bC5ub3RlcyA+IGxpIGJsb2NrcXVvdGUgcDpub3QoOmVtcHR5KTpsYXN0LWNoaWxkOmFmdGVyLApsaS5ub3RlIGJsb2NrcXVvdGUgcDpub3QoOmVtcHR5KTpsYXN0LWNoaWxkOmFmdGVyIHsKCWNvbnRlbnQ6ICfigJ0nOwp9CgovKiBQcmVzZXJ2ZSB3aGl0ZXNwYWNlIG9uIHBsYWludGV4dCBub3RlcyAqLwp1bC5ub3RlcyBsaSBwLnBsYWludGV4dCwgbGkubm90ZSBwLnBsYWludGV4dCwgZGl2Lm5vdGUgcC5wbGFpbnRleHQgewoJd2hpdGUtc3BhY2U6IHByZS13cmFwOwp9CgovKiBEaXNwbGF5IHRhZ3Mgd2l0aGluIGNoaWxkIG5vdGVzIGlubGluZSAqLwp1bC5ub3RlcyBoMy50YWdzIHsKCWRpc3BsYXk6IGlubGluZTsKCWZvbnQtc2l6ZTogMWVtOwp9Cgp1bC5ub3RlcyBoMy50YWdzOmFmdGVyIHsKCWNvbnRlbnQ6ICcgJzsKfQoKdWwubm90ZXMgdWwudGFncyB7CglkaXNwbGF5OiBpbmxpbmU7Cn0KCnVsLm5vdGVzIHVsLnRhZ3MgbGk6bm90KDpsYXN0LWNoaWxkKTphZnRlciB7Cgljb250ZW50OiAnLCAnOwp9CgoKLyogQ2hpbGQgYXR0YWNobWVudHMgKi8KaDMuYXR0YWNobWVudHMgewoJZm9udC1zaXplOiAxLjFlbTsKfQoKdWwuYXR0YWNobWVudHMgbGkgewoJcGFkZGluZy10b3A6IC41ZW07Cn0KCnVsLmF0dGFjaG1lbnRzIGRpdi5ub3RlIHsKCW1hcmdpbi1sZWZ0OiAyZW07Cn0KCnVsLmF0dGFjaG1lbnRzIGRpdi5ub3RlIHA6Zmlyc3QtY2hpbGQgewoJbWFyZ2luLXRvcDogLjc1ZW07Cn0KCmRpdiB0YWJsZSB7Cglib3JkZXItY29sbGFwc2U6IGNvbGxhcHNlOwp9CgpkaXYgdGFibGUgdGQsIGRpdiB0YWJsZSB0aCB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJYm9yZGVyLWNvbGxhcHNlOiBjb2xsYXBzZTsKCXdvcmQtYnJlYWs6IGJyZWFrLWFsbDsKfQoKZGl2IHRhYmxlIHRkIHA6ZW1wdHk6OmFmdGVyLCBkaXYgdGFibGUgdGggcDplbXB0eTo6YWZ0ZXIgewoJY29udGVudDogIlwwMGEwIjsKfQoKZGl2IHRhYmxlIHRkICo6Zmlyc3QtY2hpbGQsIGRpdiB0YWJsZSB0aCAqOmZpcnN0LWNoaWxkIHsKCW1hcmdpbi10b3A6IDA7Cn0KCmRpdiB0YWJsZSB0ZCAqOmxhc3QtY2hpbGQsIGRpdiB0YWJsZSB0aCAqOmxhc3QtY2hpbGQgewoJbWFyZ2luLWJvdHRvbTogMDsKfQo="/>
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmFueS1saW5rIHsKCWNvbG9yOiAjOTAwOwp9CgphOmhvdmVyLCBhOmFjdGl2ZSB7Cgljb2xvcjogIzc3NzsKfQoKQG1lZGlhIChwcmVmZXJzLWNvbG9yLXNjaGVtZTogZGFyaykgewoJYTphbnktbGluayB7CgkJY29sb3I6ICNmMDA7Cgl9CgoJYTpob3ZlciwgYTphY3RpdmUgewoJCWNvbG9yOiAjOTk5OwoJfQp9CgoKdWwucmVwb3J0IHsKCWZvbnQtc2l6ZTogMS40ZW07Cgl3aWR0aDogNjgwcHg7CgltYXJnaW46IDAgYXV0bzsKCXBhZGRpbmc6IDIwcHggMjBweDsKfQoKLyogTWV0YWRhdGEgdGFibGUgKi8KdGFibGUgewoJYm9yZGVyOiAxcHggI2NjYyBzb2xpZDsKCW92ZXJmbG93OiBhdXRvOwoJd2lkdGg6IDEwMCU7CgltYXJnaW46IC4xZW0gYXV0byAuNzVlbTsKCXBhZGRpbmc6IDAuNWVtOwp9Cg=="/>
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiBpbmhlcml0OwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg=="/>
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_IIMPXUZE" class="item conferencePaper">
			<h2>2D Topological Map Building by UAVs for Ground Robot Navigation</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yuqian Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuetao Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yisha Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yan Zhuang</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In air-ground cooperation, unmanned aerial vehicles (UAVs) are used to build a priori map of the ground environment from an aerial perspective, which is conducive to improve the navigation ability of ground robots. This paper proposes a 2D topology map building method from the air view, which can be effectively used for global path planning of ground mobile robots. To realize the 3D reconstruction of the ground from the air view, the 3D Euclidean Signed Distance Field (ESDF) is constructed incrementally from the Truncated Signed Distance Field (TSDF). The ESDF map occupies a large storage space, which is inconvenient to transmit to ground robots. To solve this problem, a lighter topology map is constructed on the basis of the ESDF map. Since ground robots can only move in the 2D plane, the 2D topological map is generated at any height from the ground to represent the topological structure of the ground robots working environment. The experimental results on the dataset and simulation environment shows the effectiveness of the proposed method.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-12</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/9739395/?arnumber=9739395">https://ieeexplore.ieee.org/document/9739395/?arnumber=9739395</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>14/02/2025, 15:47:04</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>663-668</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/ROBIO54168.2021.9739395">10.1109/ROBIO54168.2021.9739395</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>14/02/2025, 15:47:04</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>17/02/2025, 11:21:49</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>3D terrain reconstruction</li>
					<li>UGV</li>
					<li>UAV</li>
					<li>Topological Map</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_CJ33TWNX">
<div><div data-schema-version="9"><h1>My notes</h1>
<p>- Use of a 3D TSDF to stores the "distance" to the obstacle (is it a weight from 1 (far in front) to 0 (surface of object) to -1 (far behind))</p>
<p>- Updating a cell to a new values ("Weighting") is done by doing a weighted average of the previous weight and distance and new weight and distance.</p>
<p>- Grouping points in sensor data ("Merging") is done by grouped ray-casting</p>
<p>- ESDF from TSDF using wave propagation</p>
<p>- Then ESDF is used to generate the 2D topology map</p>
<p></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9IGV7ILJ">Full Text PDF					</li>
					<li id="item_GEBZ428Y">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_Y6UZTCEZ" class="item bookSection">
			<h2>A Collaborative Aerial-Ground Robotic System for Fast Exploration</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luqi Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Daqian Cheng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fei Gao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fengyu Cai</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jixin Guo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mengxiang Lin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shaojie Shen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Autonomous exploration of unknown environments has been widely applied in inspection, surveillance, and search and rescue. In exploration task, the basic requirement for robots is to detect the unknown space as fast as possible. In this paper, we propose an autonomous collaborative system consists of an aerial robot and a ground vehicle to explore in unknown environments. We combine the frontier based method and the harmonic field to generate a path. Then, For the ground robot, a minimum jerk piecewise Bezier curve which can guarantee safety and dynamical feasibility is generated amid obstacles. For the aerial robot, a motion primitive method is adopted for local path planning. We implement the proposed framework on an autonomous collaborative aerial-ground system. Extensive field experiments as well as simulations are presented to validate the method and demonstrate its higher efficiency against each single vehicle.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1806.02487">http://arxiv.org/abs/1806.02487</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/02/2025, 11:34:14</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>DOI: 10.1007/978-3-030-33950-0_6
arXiv:1806.02487 [cs]</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>11</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>59-71</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 11:34:14</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:48:44</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Exploration</li>
					<li>UGV</li>
					<li>UAV</li>
					<li>Collaboration</li>
					<li>Ignored</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_LMPBM3KF">
<p class="plaintext">Comment: 12 pages, 16 figures</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9KK8FJ2T">PDF					</li>
				</ul>
			</li>


			<li id="item_SUTRTIT9" class="item journalArticle">
			<h2>A Comprehensive Review of UAV-UGV Collaboration: Advancements and Challenges</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Isuru Munasinghe</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Asanka Perera</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ravinesh C. Deo</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) have rapidly evolved, becoming integral to various applications such as environmental monitoring, disaster response, and precision agriculture. This paper provides a comprehensive review of the advancements and the challenges in UAV-UGV collaboration and its potential applications. These systems offer enhanced situational awareness and operational efficiency, enabling complex tasks that are beyond the capabilities of individual systems by leveraging the complementary strengths of UAVs and UGVs. Key areas explored in this review include multi-UAV and multi-UGV systems, collaborative aerial and ground operations, and the communication and coordination mechanisms that support these collaborative efforts. Furthermore, this paper discusses potential limitations, challenges and future research directions, and considers issues such as computational constraints, communication network instability, and environmental adaptability. The review also provides a detailed analysis of how these issues impact the effectiveness of UAV-UGV collaboration.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-11-28</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>A Comprehensive Review of UAV-UGV Collaboration</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.mdpi.com/2224-2708/13/6/81">https://www.mdpi.com/2224-2708/13/6/81</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>14/05/2025, 15:49:39</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>https://creativecommons.org/licenses/by/4.0/</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>81</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Sensor and Actuator Networks</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.3390/jsan13060081">10.3390/jsan13060081</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>6</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>JSAN</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2224-2708</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>14/05/2025, 15:49:39</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>18/06/2025, 15:20:45</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UGV</li>
					<li>UAV</li>
					<li>Collaboration</li>
					<li>Not read</li>
					<li>To read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P8SVMNU5">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_DGVXHTWS" class="item journalArticle">
			<h2>A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alessandro Giusti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jérôme Guzzi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dan C. Cireşan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fang-Lin He</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Juan P. Rodríguez</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Flavio Fontana</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matthias Faessler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Forster</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jürgen Schmidhuber</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gianni Di Caro</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Davide Scaramuzza</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca M. Gambardella</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2016-07</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/7358076">https://ieeexplore.ieee.org/document/7358076</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/02/2025, 12:29:32</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Conference Name: IEEE Robotics and Automation Letters</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>661-667</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Robotics and Automation Letters</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/LRA.2015.2509024">10.1109/LRA.2015.2509024</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2377-3766</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 12:29:32</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 12:30:15</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UAV</li>
					<li>Dataset</li>
					<li>Traversability</li>
					<li>Trail classification</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_WMXUKA4P">
<div><div data-schema-version="9"><h1>My notes</h1>
<ul>
<li>
Clever technic to generate dataset: human with 3 cameras oriented left, center, right walks on a forest trails making&nbsp;sure to face the direction of the path. It generates, respectively, images with trails on the right side, center and left side
</li>
<li>
The model predict if the trail is on the right side, center or left side
</li>
</ul>
<p>15+ GB of training testing dataset available (images left, center and right of forest trails)</p>
<p></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4XE5ZKK3">Full Text PDF					</li>
					<li id="item_LKUJAWGM">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_9W66UU9V" class="item journalArticle">
			<h2>A method for registration of 3-D shapes</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>P.J. Besl</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Neil D. McKay</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of &apos;shape complexity&apos;, one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.&lt;&gt;</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>1992-02</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/121791/citations">https://ieeexplore.ieee.org/document/121791/citations</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>24/06/2025, 12:18:05</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>14</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>239-256</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Transactions on Pattern Analysis and Machine Intelligence</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/34.121791">10.1109/34.121791</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1939-3539</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>24/06/2025, 12:18:05</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>24/06/2025, 12:18:05</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Convergence</li>
					<li>Inspection</li>
					<li>Iterative algorithms</li>
					<li>Iterative closest point algorithm</li>
					<li>Iterative methods</li>
					<li>Motion estimation</li>
					<li>Quaternions</li>
					<li>Shape measurement</li>
					<li>Solid modeling</li>
					<li>Testing</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BWZER356">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_SW3HUC3Y" class="item conferencePaper">
			<h2>A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>D. Scharstein</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Szeliski</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Zabih</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can be easily extended to include new algorithms. We have also produced several new multiframe stereo data sets with ground truth, and are making both the code and data sets available on the Web.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2001-12</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/988771/?arnumber=988771">https://ieeexplore.ieee.org/document/988771/?arnumber=988771</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>17/02/2025, 14:33:33</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>131-140</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/SMBV.2001.988771">10.1109/SMBV.2001.988771</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>17/02/2025, 14:33:33</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>17/02/2025, 14:33:58</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>To read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BL7ATB99">Full Text PDF					</li>
					<li id="item_DKGADFLK">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_TD8C39HN" class="item conferencePaper">
			<h2>A two-stage level set evolution scheme for man-made objects detection in aerial images</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Guo Cao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xin Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhihong Mao</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A novel two-stage level set evolution method for detecting man-made objects in aerial images is described. The method is based on a modified Mumford-Shah model and it uses a two-stage curve evolution strategy to get a preferable detection. It applies fractal error metric, developed by Cooper, et al. (1994) at the first curve evolution stage and adds additional constraint texture edge descriptor that is defined by using DCT (discrete cosine transform) coefficients on the image at the next stage. Man-made objects and natural areas are optimally differentiated by evolving the partial differential equation. The method artfully avoids selecting a threshold to separate the fractal error image, while an improper threshold often results in great segmentation errors. Experiments of the segmentation show that the proposed method is efficient.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2005-07-20</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>ResearchGate</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-0-7695-2372-9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>474-479 vol. 1</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/CVPR.2005.52">10.1109/CVPR.2005.52</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 12:27:15</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 12:27:47</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UAV</li>
					<li>Man-made segmentation</li>
					<li>Ignored</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_A65SW6XJ">Full Text PDF					</li>
					<li id="item_EJXIP8Y3">ResearchGate Link					</li>
				</ul>
			</li>


			<li id="item_BJACI2PR" class="item conferencePaper">
			<h2>Accurate Quadrifocal Tracking for Robust 3D Visual Odometry</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A.I. Comport</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>E. Malis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>P. Rives</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper describes a new image-based approach to tracking the 6dof trajectory of a stereo camera pair using a corresponding reference image pairs instead of explicit 3D feature reconstruction of the scene. A dense minimisation approach is employed which directly uses all grey-scale information available within the stereo pair (or stereo region) leading to very robust and precise results. Metric 3D structure constraints are imposed by consistently warping corresponding stereo images to generate novel viewpoints at each stereo acquisition. An iterative non-linear trajectory estimation approach is formulated based on a quadrifocal relationship between the image intensities within adjacent views of the stereo pair. A robust M-estimation technique is used to reject outliers corresponding to moving objects within the scene or other outliers such as occlusions and illumination changes. The technique is applied to recovering the trajectory of a moving vehicle in long and difficult sequences of images.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2007-04</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067">https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>14/02/2025, 15:40:47</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 1050-4729</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>40-45</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings 2007 IEEE International Conference on Robotics and Automation</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>Proceedings 2007 IEEE International Conference on Robotics and Automation</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/ROBOT.2007.363762">10.1109/ROBOT.2007.363762</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>14/02/2025, 15:40:47</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>18/02/2025, 11:04:25</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Dense correspondences</li>
					<li>Quadrifocal warping</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_BPFZEIDT">
<div><div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22This%20paper%20describes%20a%20new%20image-based%20approach%20to%20tracking%20the%206dof%20trajectory%20of%20a%20stereo%20camera%20pair%20using%20a%20corresponding%20reference%20image%20pairs%20instead%20of%20explicit%203D%20feature%20reconstruction%20of%20the%20scene.%20A%20dense%20minimisation%20approach%20is%20employed%20which%20directly%20uses%20all%20grey-scale%20information%20available%20within%20the%20stereo%20pair%20(or%20stereo%20region)%20leading%20to%20very%20robust%20and%20precise%20results.%20Metric%203D%20structure%20constraints%20are%20imposed%20by%20consistently%20warping%20corresponding%20stereo%20images%20to%20generate%20novel%20viewpoints%20at%20each%20stereo%20acquisition.%20An%20iterative%20non-linear%20trajectory%20estimation%20approach%20is%20formulated%20based%20on%20a%20quadrifocal%20relationship%20between%20the%20image%20intensities%20within%20adjacent%20views%20of%20the%20stereo%20pair.%20A%20robust%20M-estimation%20technique%20is%20used%20to%20reject%20outliers%20corresponding%20to%20moving%20objects%20within%20the%20scene%20or%20other%20outliers%20such%20as%20occlusions%20and%20illumination%20changes.%20The%20technique%20is%20applied%20to%20recovering%20the%20trajectory%20of%20a%20moving%20vehicle%20in%20long%20and%20difficult%20sequences%20of%20images.%22%2C%22container-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22DOI%22%3A%2210.1109%2FROBOT.2007.363762%22%2C%22event-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22note%22%3A%22ISSN%3A%201050-4729%22%2C%22page%22%3A%2240-45%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Accurate%20Quadrifocal%20Tracking%20for%20Robust%203D%20Visual%20Odometry%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4209067%2F%3Farnumber%3D4209067%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Comport%22%2C%22given%22%3A%22A.I.%22%7D%2C%7B%22family%22%3A%22Malis%22%2C%22given%22%3A%22E.%22%7D%2C%7B%22family%22%3A%22Rives%22%2C%22given%22%3A%22P.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222007%22%2C4%5D%5D%7D%7D%7D%2C%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FNGJ5PVX7%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FNGJ5PVX7%22%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Stereo%20vision%20is%20an%20attractive%20passive%20sensing%20technique%20for%20obtaining%20three-dimensional%20(3-D)%20measurements.%20Recent%20hardware%20advances%20have%20given%20rise%20to%20a%20new%20class%20of%20real-time%20dense%20disparity%20estimation%20algorithms.%20This%20paper%20examines%20their%20suitability%20for%20intelligent%20vehicle%20(IV)%20applications.%20In%20order%20to%20gain%20a%20better%20understanding%20of%20the%20performance%20and%20the%20computational-cost%20tradeoff%2C%20the%20authors%20created%20a%20framework%20of%20real-time%20implementations.%20This%20consists%20of%20different%20methodical%20components%20based%20on%20single%20instruction%20multiple%20data%20(SIMD)%20techniques.%20Furthermore%2C%20the%20resulting%20algorithmic%20variations%20are%20compared%20with%20other%20publicly%20available%20algorithms.%20The%20authors%20argue%20that%20existing%20publicly%20available%20stereo%20data%20sets%20are%20not%20very%20suitable%20for%20the%20IV%20domain.%20Therefore%2C%20the%20authors'%20evaluation%20of%20stereo%20algorithms%20is%20based%20on%20novel%20realistically%20looking%20simulated%20data%20as%20well%20as%20real%20data%20from%20complex%20urban%20traffic%20scenes.%20In%20order%20to%20facilitate%20future%20benchmarks%2C%20all%20data%20used%20in%20this%20paper%20is%20made%20publicly%20available.%20The%20results%20from%20this%20study%20reveal%20that%20there%20is%20a%20considerable%20influence%20of%20scene%20conditions%20on%20the%20performance%20of%20all%20tested%20algorithms.%20Approaches%20that%20aim%20for%20(global)%20search%20optimization%20are%20more%20affected%20by%20this%20than%20other%20approaches.%20The%20best%20overall%20performance%20is%20achieved%20by%20the%20proposed%20multiple-window%20algorithm%2C%20which%20uses%20local%20matching%20and%20a%20left-right%20check%20for%20a%20robust%20error%20rejection.%20Timing%20results%20show%20that%20the%20simplest%20of%20the%20proposed%20SIMD%20variants%20are%20more%20than%20twice%20as%20fast%20than%20the%20most%20complex%20one.%20Nevertheless%2C%20the%20latter%20still%20achieves%20real-time%20processing%20speeds%2C%20while%20their%20average%20accuracy%20is%20at%20least%20equal%20to%20that%20of%20publicly%20available%20non-SIMD%20algorithms%22%2C%22container-title%22%3A%22IEEE%20Transactions%20on%20Intelligent%20Transportation%20Systems%22%2C%22DOI%22%3A%2210.1109%2FTITS.2006.869625%22%2C%22ISSN%22%3A%221558-0016%22%2C%22issue%22%3A%221%22%2C%22note%22%3A%22event-title%3A%20IEEE%20Transactions%20on%20Intelligent%20Transportation%20Systems%22%2C%22page%22%3A%2238-50%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Real-time%20dense%20stereo%20for%20intelligent%20vehicles%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F1603551%2F%3Farnumber%3D1603551%22%2C%22volume%22%3A%227%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Mark%22%2C%22given%22%3A%22W.%22%2C%22non-dropping-particle%22%3A%22van%20der%22%7D%2C%7B%22family%22%3A%22Gavrila%22%2C%22given%22%3A%22D.M.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C17%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222006%22%2C3%5D%5D%7D%7D%7D%5D" data-schema-version="9"><h1>My notes</h1>
<p>Given geometric constraints two <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F56EYZP6B%22%2C%22pageLabel%22%3A%2242%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B239.91911688000027%2C365.74307202%2C298.7911090600003%2C374.50019741999995%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22locator%22%3A%2242%22%7D%7D">“trifocal tensor”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22locator%22%3A%2242%22%2C%22itemData%22%3A%7B%22id%22%3A11%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22This%20paper%20describes%20a%20new%20image-based%20approach%20to%20tracking%20the%206dof%20trajectory%20of%20a%20stereo%20camera%20pair%20using%20a%20corresponding%20reference%20image%20pairs%20instead%20of%20explicit%203D%20feature%20reconstruction%20of%20the%20scene.%20A%20dense%20minimisation%20approach%20is%20employed%20which%20directly%20uses%20all%20grey-scale%20information%20available%20within%20the%20stereo%20pair%20(or%20stereo%20region)%20leading%20to%20very%20robust%20and%20precise%20results.%20Metric%203D%20structure%20constraints%20are%20imposed%20by%20consistently%20warping%20corresponding%20stereo%20images%20to%20generate%20novel%20viewpoints%20at%20each%20stereo%20acquisition.%20An%20iterative%20non-linear%20trajectory%20estimation%20approach%20is%20formulated%20based%20on%20a%20quadrifocal%20relationship%20between%20the%20image%20intensities%20within%20adjacent%20views%20of%20the%20stereo%20pair.%20A%20robust%20M-estimation%20technique%20is%20used%20to%20reject%20outliers%20corresponding%20to%20moving%20objects%20within%20the%20scene%20or%20other%20outliers%20such%20as%20occlusions%20and%20illumination%20changes.%20The%20technique%20is%20applied%20to%20recovering%20the%20trajectory%20of%20a%20moving%20vehicle%20in%20long%20and%20difficult%20sequences%20of%20images.%22%2C%22container-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22DOI%22%3A%2210.1109%2FROBOT.2007.363762%22%2C%22event-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22note%22%3A%22ISSN%3A%201050-4729%22%2C%22page%22%3A%2240-45%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Accurate%20Quadrifocal%20Tracking%20for%20Robust%203D%20Visual%20Odometry%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4209067%2F%3Farnumber%3D4209067%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Comport%22%2C%22given%22%3A%22A.I.%22%7D%2C%7B%22family%22%3A%22Malis%22%2C%22given%22%3A%22E.%22%7D%2C%7B%22family%22%3A%22Rives%22%2C%22given%22%3A%22P.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222007%22%2C4%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Comport et al., 2007, p. 42</span>)</span> can be computed from the two references images and left/right current images.</p>
<p>In case of the two ref images and the left current images, the <span class="highlight" data-annotation="%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F56EYZP6B%22%2C%22pageLabel%22%3A%2242%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B365.40863176000005%2C618.2830315399998%2C557.9817047199998%2C627.0401569399999%5D%2C%5B313.19962646000005%2C606.5650214199998%2C548.2701622399991%2C615.3221468199998%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22locator%22%3A%2242%22%7D%7D">“the warping from the left reference image to the left current image [is done] via a plane in the right reference image”</span> <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22locator%22%3A%2242%22%2C%22itemData%22%3A%7B%22id%22%3A11%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22This%20paper%20describes%20a%20new%20image-based%20approach%20to%20tracking%20the%206dof%20trajectory%20of%20a%20stereo%20camera%20pair%20using%20a%20corresponding%20reference%20image%20pairs%20instead%20of%20explicit%203D%20feature%20reconstruction%20of%20the%20scene.%20A%20dense%20minimisation%20approach%20is%20employed%20which%20directly%20uses%20all%20grey-scale%20information%20available%20within%20the%20stereo%20pair%20(or%20stereo%20region)%20leading%20to%20very%20robust%20and%20precise%20results.%20Metric%203D%20structure%20constraints%20are%20imposed%20by%20consistently%20warping%20corresponding%20stereo%20images%20to%20generate%20novel%20viewpoints%20at%20each%20stereo%20acquisition.%20An%20iterative%20non-linear%20trajectory%20estimation%20approach%20is%20formulated%20based%20on%20a%20quadrifocal%20relationship%20between%20the%20image%20intensities%20within%20adjacent%20views%20of%20the%20stereo%20pair.%20A%20robust%20M-estimation%20technique%20is%20used%20to%20reject%20outliers%20corresponding%20to%20moving%20objects%20within%20the%20scene%20or%20other%20outliers%20such%20as%20occlusions%20and%20illumination%20changes.%20The%20technique%20is%20applied%20to%20recovering%20the%20trajectory%20of%20a%20moving%20vehicle%20in%20long%20and%20difficult%20sequences%20of%20images.%22%2C%22container-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22DOI%22%3A%2210.1109%2FROBOT.2007.363762%22%2C%22event-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22note%22%3A%22ISSN%3A%201050-4729%22%2C%22page%22%3A%2240-45%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Accurate%20Quadrifocal%20Tracking%20for%20Robust%203D%20Visual%20Odometry%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4209067%2F%3Farnumber%3D4209067%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Comport%22%2C%22given%22%3A%22A.I.%22%7D%2C%7B%22family%22%3A%22Malis%22%2C%22given%22%3A%22E.%22%7D%2C%7B%22family%22%3A%22Rives%22%2C%22given%22%3A%22P.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222007%22%2C4%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Comport et al., 2007, p. 42</span>)</span>. A mathematical relation describe (depending on the transformation between the ref and current) the warping in term of coordinates between the left ref and left curr images.</p>
<p>Similar relation can be computed using the two ref images and the right images.</p>
<p>These relations can be used on the pixels of 3d coordinates seen in both ref images (found using dense correspondence <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FNGJ5PVX7%22%5D%2C%22itemData%22%3A%7B%22id%22%3A93%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Stereo%20vision%20is%20an%20attractive%20passive%20sensing%20technique%20for%20obtaining%20three-dimensional%20(3-D)%20measurements.%20Recent%20hardware%20advances%20have%20given%20rise%20to%20a%20new%20class%20of%20real-time%20dense%20disparity%20estimation%20algorithms.%20This%20paper%20examines%20their%20suitability%20for%20intelligent%20vehicle%20(IV)%20applications.%20In%20order%20to%20gain%20a%20better%20understanding%20of%20the%20performance%20and%20the%20computational-cost%20tradeoff%2C%20the%20authors%20created%20a%20framework%20of%20real-time%20implementations.%20This%20consists%20of%20different%20methodical%20components%20based%20on%20single%20instruction%20multiple%20data%20(SIMD)%20techniques.%20Furthermore%2C%20the%20resulting%20algorithmic%20variations%20are%20compared%20with%20other%20publicly%20available%20algorithms.%20The%20authors%20argue%20that%20existing%20publicly%20available%20stereo%20data%20sets%20are%20not%20very%20suitable%20for%20the%20IV%20domain.%20Therefore%2C%20the%20authors'%20evaluation%20of%20stereo%20algorithms%20is%20based%20on%20novel%20realistically%20looking%20simulated%20data%20as%20well%20as%20real%20data%20from%20complex%20urban%20traffic%20scenes.%20In%20order%20to%20facilitate%20future%20benchmarks%2C%20all%20data%20used%20in%20this%20paper%20is%20made%20publicly%20available.%20The%20results%20from%20this%20study%20reveal%20that%20there%20is%20a%20considerable%20influence%20of%20scene%20conditions%20on%20the%20performance%20of%20all%20tested%20algorithms.%20Approaches%20that%20aim%20for%20(global)%20search%20optimization%20are%20more%20affected%20by%20this%20than%20other%20approaches.%20The%20best%20overall%20performance%20is%20achieved%20by%20the%20proposed%20multiple-window%20algorithm%2C%20which%20uses%20local%20matching%20and%20a%20left-right%20check%20for%20a%20robust%20error%20rejection.%20Timing%20results%20show%20that%20the%20simplest%20of%20the%20proposed%20SIMD%20variants%20are%20more%20than%20twice%20as%20fast%20than%20the%20most%20complex%20one.%20Nevertheless%2C%20the%20latter%20still%20achieves%20real-time%20processing%20speeds%2C%20while%20their%20average%20accuracy%20is%20at%20least%20equal%20to%20that%20of%20publicly%20available%20non-SIMD%20algorithms%22%2C%22container-title%22%3A%22IEEE%20Transactions%20on%20Intelligent%20Transportation%20Systems%22%2C%22DOI%22%3A%2210.1109%2FTITS.2006.869625%22%2C%22ISSN%22%3A%221558-0016%22%2C%22issue%22%3A%221%22%2C%22note%22%3A%22event-title%3A%20IEEE%20Transactions%20on%20Intelligent%20Transportation%20Systems%22%2C%22page%22%3A%2238-50%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Real-time%20dense%20stereo%20for%20intelligent%20vehicles%22%2C%22volume%22%3A%227%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Mark%22%2C%22given%22%3A%22W.%22%2C%22non-dropping-particle%22%3A%22van%20der%22%7D%2C%7B%22family%22%3A%22Gavrila%22%2C%22given%22%3A%22D.M.%22%7D%5D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222006%22%2C3%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Mark and Gavrila, 2006</span>)</span>) to compare luminescence (images are in grayscale) between ref and current images. This quantity needs to be minimized (describe in Section 4). </p>
<p></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_56EYZP6B">Full Text PDF					</li>
					<li id="item_CZJX3AKX">IEEE Xplore Abstract Record					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_LVSI353T">Statistically robust 2-D visual servoing</li>
					<li id="item_NGJ5PVX7">Real-time dense stereo for intelligent vehicles</li>
				</ul>
			</li>


			<li id="item_NQXQ2CHK" class="item journalArticle">
			<h2>Active Autonomous Aerial Exploration for Ground Robot Path Planning</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeffrey Delmerico</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elias Mueggler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Julia Nitsch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Davide Scaramuzza</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We address the problem of planning a path for a ground robot through unknown terrain, using observations from a flying robot. In search and rescue missions, which are our target scenarios, the time from arrival at the disaster site to the delivery of aid is critically important. Previous works required exhaustive exploration before path planning, which is time-consuming but eventually leads to an optimal path for the ground robot. Instead, we propose active exploration of the environment, where the flying robot chooses regions to map in a way that optimizes the overall response time of the system, which is the combined time for the air and ground robots to execute their missions. In our approach, we estimate terrain classes throughout our terrain map, and we also add elevation information in areas where the active exploration algorithm has chosen to perform 3-D reconstruction. This terrain information is used to estimate feasible and efficient paths for the ground robot. By exploring the environment actively, we achieve superior response times compared to both exhaustive and greedy exploration strategies. We demonstrate the performance and capabilities of the proposed system in simulated and real-world outdoor experiments. To the best of our knowledge, this is the first work to address ground robot path planning using active aerial exploration.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017-04</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/7812671/?arnumber=7812671">https://ieeexplore.ieee.org/document/7812671/?arnumber=7812671</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>14/02/2025, 09:30:10</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Conference Name: IEEE Robotics and Automation Letters</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>664-671</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Robotics and Automation Letters</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/LRA.2017.2651163">10.1109/LRA.2017.2651163</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2377-3766</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>14/02/2025, 09:30:10</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>25/02/2025, 13:42:40</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Exploration</li>
					<li>Terrain classification</li>
					<li>3D terrain reconstruction</li>
					<li>UGV</li>
					<li>UAV</li>
					<li>Traversability</li>
					<li>Collaboration</li>
					<li>Global path</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_MPAG89F3">
<div><div data-schema-version="9"><h1>My notes</h1>
<p>1. Initial manual fly to see goal. Camera imagery is also used to obtain initial classification of the terrain.<br>2. Then vision-guided flights (to a series of waypoints) chosen actively. For each waypoint 3D reconstruction and ground<br>robot path (to optimize total duration of the mission).<br>3. Repeat 2. until path is complete</p>
<p>- visual odometry for loc&amp;nav [^3], [^4]<br>- [^3] also seems to mention matching of FPV (ground drone) and BEV (aerial drone) (p191)<br>- classifier is trained on the spot [^1]. Two models type tried, feature-based and CNN. CNN is significantly slower<br>without giving significant better result (surprising).<br>- in the 2. not exhaustive exploration just along the _global path_ from the manually generated map.<br>The next waypoints for the aerial drone are chosen as to minimize<br>$T_{\text{ground robot}, s \rightarrow b_i} + T_{\text{ground robot}, b_i \rightarrow g} + T_{extend 3d reconstructed region}$,<br>respectively _time of ground robot from s to next ground robot waypoint (uses 3d reconstructed area)_, the _time of<br>ground robot from next ground robot waypoint (uses initial partial map)_, the _time to extend the 3d reconstructed<br>region (in the correct direction)_.<br>- 7,8,9 are ref for high altitude, high resolution aerial images<br>- use of monocular camera to reconstruct 3D ground in real time [^2] (could be used with move_base_flex).<br>- use of [ANYbotics Grid Map](https://github.com/ANYbotics/grid_map) [^5]<br>- Terrain classification, dense 3D reconstruction and exploration algorithm run on a laptop computer not on the drone.</p>
<p></p>
<p>[^1]: https://rpg.ifi.uzh.ch/docs/ISER16_Delmerico.pdf</p>
<p>[^2]: https://rpg.ifi.uzh.ch/docs/ICRA14_Pizzoli.pdf</p>
<p>[^3]: https://rpg.ifi.uzh.ch/docs/PhD16_Forster.pdf</p>
<p>[^4]: https://ieeexplore.ieee.org/document/6906584</p>
<p>[^5]: https://www.researchgate.net/publication/284415855_A_Universal_Grid_Map_Library_Implementation_and_Use_Case_for_Rough_Terrain_Navigation</p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3N3L352T">Full Text PDF					</li>
					<li id="item_LNNWMXVZ">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_XDL2HUPR" class="item journalArticle">
			<h2>An Aerial-Ground Robotic System for Navigation and Obstacle Mapping in Large Outdoor Areas</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mario Garzón</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>João Valente</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Zapata</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Antonio Barrientos</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>There are many outdoor robotic applications where a robot must reach a goal position or explore an area without previous knowledge of the environment around it. Additionally, other applications (like path planning) require the use of known maps or previous information of the environment. This work presents a system composed by a terrestrial and an aerial robot that cooperate and share sensor information in order to address those requirements. The ground robot is able to navigate in an unknown large environment aided by visual feedback from a camera on board the aerial robot. At the same time, the obstacles are mapped in real-time by putting together the information from the camera and the positioning system of the ground robot. A set of experiments were carried out with the purpose of verifying the system applicability. The experiments were performed in a simulation environment and outdoor with a medium-sized ground robot and a mini quad-rotor. The proposed robotic system shows outstanding results in simultaneous navigation and mapping applications in large outdoor environments.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2013/1</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>www.mdpi.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.mdpi.com/1424-8220/13/1/1247">https://www.mdpi.com/1424-8220/13/1/1247</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:43:56</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>http://creativecommons.org/licenses/by/3.0/</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 1
Publisher: Multidisciplinary Digital Publishing Institute</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>13</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1247-1267</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Sensors</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.3390/s130101247">10.3390/s130101247</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1424-8220</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:43:56</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:44:11</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>To read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5L7LJAU6">Full Text PDF					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_FSB8VHAN">Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images</li>
				</ul>
			</li>


			<li id="item_P55T832W" class="item journalArticle">
			<h2>An effective approach to unmanned aerial vehicle navigation using visual topological map in outdoor and indoor environments</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tao Han</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jefferson S. Almeida</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Suane Pires P. da Silva</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Paulo Honório Filho</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Antonio W. de Oliveira Rodrigues</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Victor Hugo C. de Albuquerque</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pedro P. Rebouças Filho</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Unmanned Aerial Vehicles are constantly being using in professional activities that require higher precision in navigating and positioning the aircraft during operation. Advanced location technologies such as Global Navigation Satellite System and Real-Time Kinematic are widely used, however, they depend on an area with transmission coverage. In this approach, this article presents a visual navigation methodology based on topological maps. We compared the performance of consolidated classifiers such as Bayesian classifier, k-nearest neighbor, Multilayer Perceptron, Optimal Path Forest and Support Vector Machines (SVM). They are evaluated with attributes returned by last generation resource extractors such as Fourier, Gray Level Co-Occurrence and Local Binary Patterns (LBP). After analyzing the results we found that the combination of LBP and SVM obtained the best values in the evaluation metrics considered, among them, 99.99% Specificity and 99.98% Precision in the navigation process. SVM reached 5.49787 s in combination with LBP completes the training in 5.49787 s. Concerning the testing time, SVM achieving 80.91 ms in association with LBP.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-01-15</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sciencedirect.com/science/article/pii/S0140366419307868">https://www.sciencedirect.com/science/article/pii/S0140366419307868</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:40:39</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>150</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>696-702</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Computer Communications</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.comcom.2019.12.026">10.1016/j.comcom.2019.12.026</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Computer Communications</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0140-3664</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:40:39</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>18/06/2025, 15:28:04</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Topological Map</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LCZV2YWL">PDF					</li>
					<li id="item_6LDNVZ6W">ScienceDirect Snapshot					</li>
				</ul>
			</li>


			<li id="item_SMMHC9IS" class="item conferencePaper">
			<h2>Appearance contrast for fast, robust trail-following</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher Rasmussen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yan Lu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mehmet Kocamaz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We describe a framework for ﬁnding and tracking “trails” for autonomous outdoor robot navigation. Through a combination of visual cues and ladar-derived structural information, the algorithm is able to follow paths which pass through multiple zones of terrain smoothness, border vegetation, tread material, and illumination conditions. Our shape-based visual trail tracker assumes that the approaching trail region is approximately triangular under perspective. It generates region hypotheses from a learned distribution of expected trail width and curvature variation, and scores them using a robust measure of color and brightness contrast with ﬂanking regions. The structural component analogously rewards hypotheses which correspond to empty or low-density regions in a groundstrike-ﬁltered ladar obstacle map. Our system’s performance is analyzed on several long sequences with diverse appearance and structural characteristics. Ground-truth segmentations are used to quantify performance where available, and several alternative algorithms are compared on the same data.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>10/2009</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://ieeexplore.ieee.org/document/5354059/">http://ieeexplore.ieee.org/document/5354059/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>17/02/2025, 11:18:03</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>St. Louis, MO, USA</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>IEEE</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4244-3803-7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>3505-3512</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2009 IEEE/RSJ International Conference on Intelligent Robots and Systems</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2009)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/IROS.2009.5354059">10.1109/IROS.2009.5354059</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>17/02/2025, 11:18:03</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 12:32:54</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UGV</li>
					<li>Trail classification</li>
					<li>Local path</li>
					<li>Lidar</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_NE8EJHGP">
<div><div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F2IXKKLES%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F2IXKKLES%22%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22We%20present%20a%20fast%20integrated%20approach%20for%20online%20segmentation%20of%20images%20for%20outdoor%20robots.%20A%20compact%20color%20and%20texture%20descriptor%20has%20been%20developed%20to%20describe%20local%20color%20and%20texture%20variations%20in%20an%20image.%20This%20descriptor%20is%20then%20used%20in%20a%20two-stage%20fast%20clustering%20framework%20using%20K-means%20to%20perform%20online%20segmentation%20of%20natural%20images.%20We%20present%20results%20of%20applying%20our%20descriptor%20for%20segmenting%20a%20synthetic%20image%20and%20compare%20it%20against%20other%20state-of-the-art%20descriptors.%20We%20also%20apply%20our%20segmentation%20algorithm%20to%20the%20task%20of%20detecting%20natural%20paths%20in%20outdoor%20images.%20The%20whole%20system%20has%20been%20demonstrated%20to%20work%20online%20alongside%20localization%2C%203D%20obstacle%20detection%2C%20and%20planning.%22%2C%22container-title%22%3A%222008%20IEEE%2FRSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%22%2C%22DOI%22%3A%2210.1109%2FIROS.2008.4651086%22%2C%22event-title%22%3A%222008%20IEEE%2FRSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%22%2C%22note%22%3A%22ISSN%3A%202153-0866%22%2C%22page%22%3A%224078-4085%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Fast%20color%2Ftexture%20segmentation%20for%20outdoor%20robots%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4651086%2F%3Farnumber%3D4651086%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Rufus%20Blas%22%2C%22given%22%3A%22Morten%22%7D%2C%7B%22family%22%3A%22Agrawal%22%2C%22given%22%3A%22Motilal%22%7D%2C%7B%22family%22%3A%22Sundaresan%22%2C%22given%22%3A%22Aravind%22%7D%2C%7B%22family%22%3A%22Konolige%22%2C%22given%22%3A%22Kurt%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C17%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222008%22%2C9%5D%5D%7D%7D%7D%5D" data-schema-version="9"><h1>My notes</h1>
<ul>
<li>
CIE-Lab used
</li>
<li>
textons generated from the input image using <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F2IXKKLES%22%5D%2C%22itemData%22%3A%7B%22id%22%3A44%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22We%20present%20a%20fast%20integrated%20approach%20for%20online%20segmentation%20of%20images%20for%20outdoor%20robots.%20A%20compact%20color%20and%20texture%20descriptor%20has%20been%20developed%20to%20describe%20local%20color%20and%20texture%20variations%20in%20an%20image.%20This%20descriptor%20is%20then%20used%20in%20a%20two-stage%20fast%20clustering%20framework%20using%20K-means%20to%20perform%20online%20segmentation%20of%20natural%20images.%20We%20present%20results%20of%20applying%20our%20descriptor%20for%20segmenting%20a%20synthetic%20image%20and%20compare%20it%20against%20other%20state-of-the-art%20descriptors.%20We%20also%20apply%20our%20segmentation%20algorithm%20to%20the%20task%20of%20detecting%20natural%20paths%20in%20outdoor%20images.%20The%20whole%20system%20has%20been%20demonstrated%20to%20work%20online%20alongside%20localization%2C%203D%20obstacle%20detection%2C%20and%20planning.%22%2C%22container-title%22%3A%222008%20IEEE%2FRSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%22%2C%22DOI%22%3A%2210.1109%2FIROS.2008.4651086%22%2C%22event-title%22%3A%222008%20IEEE%2FRSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%22%2C%22note%22%3A%22ISSN%3A%202153-0866%22%2C%22page%22%3A%224078-4085%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Fast%20color%2Ftexture%20segmentation%20for%20outdoor%20robots%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4651086%2F%3Farnumber%3D4651086%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Rufus%20Blas%22%2C%22given%22%3A%22Morten%22%7D%2C%7B%22family%22%3A%22Agrawal%22%2C%22given%22%3A%22Motilal%22%7D%2C%7B%22family%22%3A%22Sundaresan%22%2C%22given%22%3A%22Aravind%22%7D%2C%7B%22family%22%3A%22Konolige%22%2C%22given%22%3A%22Kurt%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C17%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222008%22%2C9%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Rufus Blas et al., 2008</span>)</span> method
</li>
<li>
Then triangles are scored using a likelihood function
</li>
<li>
Add lidar info to enhance results
</li>
</ul>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BS4FH7ZP">PDF					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_2IXKKLES">Fast color/texture segmentation for outdoor robots</li>
				</ul>
			</li>


			<li id="item_D7CG8MFJ" class="item computerProgram">
			<h2>ArduPilot/ardupilot</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Software</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>ArduPlane, ArduCopter, ArduRover, ArduSub source</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-07-03T11:48:48Z</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>GitHub</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://github.com/ArduPilot/ardupilot">https://github.com/ArduPilot/ardupilot</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:35:00</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>GPL-3.0</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>original-date: 2013-01-09T00:58:52Z</td>
					</tr>
					<tr>
					<th>Company</th>
						<td>ArduPilot</td>
					</tr>
					<tr>
					<th>Prog. Language</th>
						<td>C++</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:35:00</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:35:00</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>arducopter</li>
					<li>ardupilot</li>
					<li>arduplane</li>
					<li>ardurover</li>
					<li>ardusub</li>
					<li>autopilot</li>
					<li>auv</li>
					<li>copter</li>
					<li>drone</li>
					<li>dronekit</li>
					<li>mavlink</li>
					<li>plane</li>
					<li>robotics</li>
					<li>ros</li>
					<li>rov</li>
					<li>rover</li>
					<li>sub</li>
					<li>uas</li>
					<li>uav</li>
					<li>ugv</li>
				</ul>
			</li>


			<li id="item_NZV9BYEI" class="item webpage">
			<h2>aukerman - DroneDB</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://hub.dronedb.app/r/odm/aukerman">https://hub.dronedb.app/r/odm/aukerman</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/06/2025, 13:38:21</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/06/2025, 13:38:21</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/06/2025, 13:38:21</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_S9GEW4GS">aukerman - DroneDB					</li>
				</ul>
			</li>


			<li id="item_RUIH28A6" class="item journalArticle">
			<h2>Autonomous Landing at Unprepared Sites by a Full-Scale Helicopter</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sebastian Scherer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lyle Chamberlain</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sanjiv Singh</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Helicopters are valuable since they can land at unprepared sites; however, current unmanned helicopters are unable to select or validate landing zones (LZs) and approach paths. For operation in unknown terrain it is necessary to assess the safety of a LZ. In this paper, we describe a lidar-based perception system that enables a full-scale autonomous helicopter to identify and land in previously unmapped terrain with no human input.We describe the problem, real-time algorithms, perception hardware, and results. Our approach has extended the state of the art in terrain assessment by incorporating not only plane fitting, but by also considering factors such as terrain/skid interaction, rotor and tail clearance, wind direction, clear approach/abort paths, and ground paths.In results from urban and natural environments we were able to successfully classify LZs from point cloud maps. We also present results from 8 successful landing experiments with varying ground clutter and approach directions. The helicopter selected its own landing site, approaches, and then proceeds to land. To our knowledge, these experiments were the first demonstration of a full-scale autonomous helicopter that selected its own landing zones and landed.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2012-12-01</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>ResearchGate</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>60</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1545–1562</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Robotics and Autonomous Systems</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.robot.2012.09.004">10.1016/j.robot.2012.09.004</a></td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Robotics and Autonomous Systems</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:28:16</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:28:42</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Not read</li>
					<li>Ignored</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_RMPHJ9LF">Full Text PDF					</li>
					<li id="item_N45Q4LYK">ResearchGate Link					</li>
				</ul>
			</li>


			<li id="item_SZTE7TRV" class="item webpage">
			<h2>AXIS Q62 PTZ Camera Series | Axis Communications</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.axis.com/products/axis-q62-series">https://www.axis.com/products/axis-q62-series</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:27:35</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:27:35</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:27:35</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_56URTKYK">AXIS Q62 PTZ Camera Series | Axis Communications					</li>
				</ul>
			</li>


			<li id="item_DJ5C7D4X" class="item webpage">
			<h2>Barakuda mule robot for security | Shark Robotics</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.shark-robotics.com/robots/barakuda-mule-robot">https://www.shark-robotics.com/robots/barakuda-mule-robot</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:26:46</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:26:46</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:26:46</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KXX2SUY5">Barakuda mule robot for security | Shark Robotics					</li>
				</ul>
			</li>


			<li id="item_WUVFWCM7" class="item conferencePaper">
			<h2>BEV-SLAM: Building a Globally-Consistent World Map Using Monocular Vision</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>James Ross</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Oscar Mendez</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Avishkar Saha</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mark Johnson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Richard Bowden</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The ability to produce large-scale maps for navigation, path planning and other tasks is a crucial step for autonomous agents, but has always been challenging. In this work, we introduce BEV-SLAM, a novel type of graph-based SLAM that aligns semantically-segmented Bird’s Eye View (BEV) predictions from monocular cameras. We introduce a novel form of occlusion reasoning into BEV estimation and demonstrate its importance to aid spatial aggregation of BEV predictions. The result is a versatile SLAM system that can operate across arbitrary multi-camera configurations and can be seamlessly integrated with other sensors. We show that the use of multiple cameras significantly increases performance, and achieves lower relative error than high-performance GPS.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-10-23</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>BEV-SLAM</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/9981258/">https://ieeexplore.ieee.org/document/9981258/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/02/2025, 14:40:30</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>https://doi.org/10.15223/policy-029</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Kyoto, Japan</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>IEEE</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-6654-7927-1</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>3830-3836</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/IROS47612.2022.9981258">10.1109/IROS47612.2022.9981258</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 14:40:30</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 14:43:03</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UGV</li>
					<li>Not read</li>
					<li>Ignored</li>
					<li>BEV</li>
					<li>SLAM</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XYGT8DIM">PDF					</li>
				</ul>
			</li>


			<li id="item_JXJ4ICAV" class="item conferencePaper">
			<h2>BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Corentin Sautier</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gilles Puy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alexandre Boulch</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Renaud Marlet</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Vincent Lepetit</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast [40], which uses a contrast at the level of points, to the state-of-the-art TARL [30], which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of implementation, it is surpassed by the latter, which however requires a costly pre-processing. In BEVContrast, we define our contrast at the level of 2D cells in the Bird’s Eye View plane. Resulting cell-level representations offer a good trade-off between the point-level representations exploited in PointContrast and segment-level representations exploited in TARL: we retain the simplicity of PointContrast (cell representations are cheap to compute) while surpassing the performance of TARL in downstream semantic segmentation. The code is available at github.com/valeoai/BEVContrast</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-03</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>BEVContrast</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/10550628/?arnumber=10550628">https://ieeexplore.ieee.org/document/10550628/?arnumber=10550628</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:27:02</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 2475-7888</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>559-568</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2024 International Conference on 3D Vision (3DV)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2024 International Conference on 3D Vision (3DV)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/3DV62453.2024.00017">10.1109/3DV62453.2024.00017</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:27:02</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:27:34</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Not read</li>
					<li>Ignored</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XEHKDXRE">Full Text PDF					</li>
					<li id="item_UYAINSGU">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_DMCJRPCW" class="item webpage">
			<h2>carrot_planner - ROS Wiki</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://wiki.ros.org/carrot_planner">http://wiki.ros.org/carrot_planner</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/06/2025, 12:35:13</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/06/2025, 12:35:13</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/06/2025, 12:35:13</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_X55ISGJX">carrot_planner - ROS Wiki					</li>
				</ul>
			</li>


			<li id="item_KAPF7HCP" class="item journalArticle">
			<h2>Compact and Efficient Topological Mapping for Large-Scale Environment with Pruned Voronoi Diagram</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yao Qi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Rendong Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Binbing He</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Feng Lu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Youchun Xu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Topological maps generated in complex and irregular unknown environments are meaningful for autonomous robots’ navigation. To obtain the skeleton of the environment without obstacle polygon extraction and clustering, we propose a method to obtain high-quality topological maps using only pure Voronoi diagrams in three steps. Supported by Voronoi vertex’s property of the largest empty circle, the method updates the global topological map incrementally in both dynamic and static environments online. The incremental method can be adapted to any fundamental Voronoi diagram generator. We maintain the entire space by two graphs, the pruned Voronoi graph for incremental updates and the reduced approximated generalized Voronoi graph for routing planning requests. We present an extensive benchmark and real-world experiment, and our method completes the environment representation in both indoor and outdoor areas. The proposed method generates a compact topological map in both small- and large-scale scenarios, which is defined as the total length and vertices of topological maps. Additionally, our method has been shortened by several orders of magnitude in terms of the total length and consumes less than 30% of the average time cost compared to state-of-the-art methods.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022/7</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>www.mdpi.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.mdpi.com/2504-446X/6/7/183">https://www.mdpi.com/2504-446X/6/7/183</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:37:21</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>http://creativecommons.org/licenses/by/3.0/</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 7
Publisher: Multidisciplinary Digital Publishing Institute</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>6</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>183</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Drones</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.3390/drones6070183">10.3390/drones6070183</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>7</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2504-446X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:37:21</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:38:08</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>To read</li>
					<li>BEV</li>
					<li>Topological Map</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9UU8VPB2">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_E74JNK55" class="item webpage">
			<h2>CubePilot | Autopilot-on-Module | Blue Manufactured in USA | Blue Assembled in USA | Pixhawk Original Team</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.cubepilot.com/#/cube/features">https://www.cubepilot.com/#/cube/features</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:34:45</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:34:45</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:34:45</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YH89N9B9">CubePilot | Autopilot-on-Module | Blue Manufactured in USA | Blue Assembled in USA | Pixhawk Original Team					</li>
				</ul>
			</li>


			<li id="item_MDT7TGFE" class="item preprint">
			<h2>Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jianfeng Zhang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hanlin Dong</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jian Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jiahui Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Shibo Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ke Li</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xuan Tang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xian Wei</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiong You</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Path planning with strong environmental adaptability plays a crucial role in robotic navigation in unstructured outdoor environments, especially in the case of low-quality location and map information. The path planning ability of a robot depends on the identification of the traversability of global and local ground areas. In real-world scenarios, the complexity of outdoor open environments makes it difficult for robots to identify the traversability of ground areas that lack a clearly defined structure. Moreover, most existing methods have rarely analyzed the integration of local and global traversability identifications in unstructured outdoor scenarios. To address this problem, we propose a novel method, Dual-BEV Nav, first introducing Bird’s Eye View (BEV) representations into local planning to generate high-quality traversable paths. Then, these paths are projected onto the global traversability map generated by the global BEV planning model to obtain the optimal waypoints. By integrating the traversability from both local and global BEV, we establish a dual-layer BEV heuristic planning paradigm, enabling long-distance navigation in unstructured outdoor environments. We test our approach through both public dataset evaluations and real-world robot deployments, yielding promising results. Compared to baselines, the DualBEV Nav improved temporal distance prediction accuracy by up to 18.7%. In the real-world deployment, under conditions significantly different from the training set and with notable occlusions in the global BEV, the Dual-BEV Nav successfully achieved a 65-meter-long outdoor navigation. Further analysis demonstrates that the local BEV representation significantly enhances the rationality of the planning, while the global BEV probability map ensures the robustness of the overall planning.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-01-30</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Dual-BEV Nav</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2501.18351">http://arxiv.org/abs/2501.18351</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>14/02/2025, 15:50:55</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2501.18351 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2501.18351">10.48550/arXiv.2501.18351</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2501.18351</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>14/02/2025, 15:50:55</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>17/02/2025, 11:24:13</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UGV</li>
					<li>Traversability</li>
					<li>Satellite imagery</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_39LJUZTQ">
<div><div data-schema-version="9"><h1>My notes</h1>
<p>LBPM Local BEV plannning model = local BEV perception encoder + task-driven goal decoder</p>
<h2>LBPM</h2>
<h3>Local BEV Perception encoder</h3>
<p>inputs: (i) context observation $o_{t-P:t-1}$ (ii) current observation $o_t$.</p>
<p>- BEV transformation on the observation</p>
<p>- Feature extract based on LSS method. Uses EfficientNet</p>
<p>- Uses LSS and BEVDet to predict discrete depth distribution for each pixel</p>
<p>- BEV transformation (If multiple feature -&gt; BEV pooling using BEVFusion)</p>
<p>_I'm confused about what is new compared to LSS. At least until the computation of the</p>
<p>BEV features (see Fig. 2), it is every similar. Also (this is one of the difference compared to LSS), I</p>
<p>don't understand the point of the traversability features, is it to make the system</p>
<p>keep in mind what was traversable?_</p>
<h3>Task-driven goal decoder</h3>
<p>based on the ViKiNG architecture</p>
<p>inputs: environmental context $o_{t-P}$, current observation $o_t$, goal observation $o_\omega$.</p>
<h2>GBPM</h2>
<p>Provide traversability hint and overall direction.</p>
<p>- Use an overhead map from satellite images</p>
<p>- segmentation that gradually increase as they approach impassable.</p>
<p>&gt; GBPM uses trajectory data to learn traversability in the</p>
<p>&gt; overhead map [...], the more easily accessible areas will</p>
<p>&gt; be covered by a larger number of trajectories.</p>
<p>- This step uses the probability predictions as a map (not a thresholds regulated</p>
<p> &nbsp;output). [U-net](https://github.com/milesial/Pytorch-UNet) is used to do the segmentation.</p>
<h3>Use GBPM with potential trajectories generated by LBPM</h3>
<p>&gt; First, the LBPM generates multiple potential traversable paths, providing information including temporal distance, GPS</p>
<p>&gt; offsets, and waypoints from the current position to the goal. The GBPM encodes the overhead map into a global</p>
<p>&gt; probability map, projecting traversable paths of LBPM onto this map.</p>
<p>&gt;</p>
<p>&gt; $cost = k \times score + (1-k) \times temporal\_distance$</p>
<p></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_9KJEWZPQ">PDF					</li>
				</ul>
			</li>


			<li id="item_K8ATYDBB" class="item webpage">
			<h2>Elevation Mapping for Locomotion and Navigation using GPU</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Takahiro Miki</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Lorenz Wellhausen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ruben Grandia</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Fabian Jenelten</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Timon Homberger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Marco Hutter</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Perceiving the surrounding environment is crucial for autonomous mobile robots. An elevation map provides a memory-efficient and simple yet powerful geometric representation for ground robots. The robots can use this information for navigation in an unknown environment or perceptive locomotion control over rough terrain. Depending on the application, various post processing steps may be incorporated, such as smoothing, inpainting or plane segmentation. In this work, we present an elevation mapping pipeline leveraging GPU for fast and efficient processing with additional features both for navigation and locomotion. We demonstrated our mapping framework through extensive hardware experiments. Our mapping software was successfully deployed for underground exploration during DARPA Subterranean Challenge and for various experiments of quadrupedal locomotion.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022/04/27</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://arxiv.org/abs/2204.12876v1">https://arxiv.org/abs/2204.12876v1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/06/2025, 12:28:07</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/06/2025, 12:28:07</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/06/2025, 12:28:07</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2T8GCHY6">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_ZNZ4HNN6" class="item blogPost">
			<h2>Ellipse-D</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Blog Post</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Get precise heading and centimeter level position accuracy with Ellipse-D, the smallest dual-antenna RTK INS with multi-band GNSS. Read more.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-US</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.sbg-systems.com/ins/ellipse-d/">https://www.sbg-systems.com/ins/ellipse-d/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:32:46</td>
					</tr>
					<tr>
					<th>Blog Title</th>
						<td>SBG Systems</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:32:46</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:32:46</td>
					</tr>
				</table>
			</li>


			<li id="item_EM5KF8EQ" class="item conferencePaper">
			<h2>Experimental Analysis of Overhead Data Processing To Support Long Range Navigation</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Silver</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Boris Sofman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicolas Vandapel</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>J. Andrew Bagnell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anthony Stentz</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Long range navigation by unmanned ground vehicles continues to challenge the robotics community. Efficient navigation requires not only intelligent on-board perception and planning systems, but also the effective use of prior knowledge of the vehicle&apos;s environment. This paper describes a system for supporting unmanned ground vehicle navigation through the use of heterogeneous overhead data. Semantic information is obtained through supervised classification, and vehicle mobility is predicted from available geometric data. This approach is demonstrated and validated through over 50 kilometers of autonomous traversal through complex natural environments</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2006-10</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754">https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/02/2025, 12:18:13</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 2153-0866</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2443-2450</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/IROS.2006.281686">10.1109/IROS.2006.281686</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 12:18:13</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 12:23:23</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UGV</li>
					<li>Satellite imagery</li>
					<li>Lidar</li>
					<li>Ignored</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_CXLXBQAD">
<div><div data-schema-version="9"><h1>My notes</h1>
<p>usage of LiDAR and imagery on (only) terrestrial robot and prior data (from variety of sources) to achieve robust navigation.</p>
<p></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MMVCY4JJ">Full Text PDF					</li>
					<li id="item_NM2AJAXS">IEEE Xplore Abstract Record					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_EBSKRPCJ">Terrain Classification from Aerial Data to Support Ground Vehicle Navigation</li>
				</ul>
			</li>


			<li id="item_2IXKKLES" class="item conferencePaper">
			<h2>Fast color/texture segmentation for outdoor robots</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Morten Rufus Blas</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Motilal Agrawal</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Aravind Sundaresan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kurt Konolige</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a fast integrated approach for online segmentation of images for outdoor robots. A compact color and texture descriptor has been developed to describe local color and texture variations in an image. This descriptor is then used in a two-stage fast clustering framework using K-means to perform online segmentation of natural images. We present results of applying our descriptor for segmenting a synthetic image and compare it against other state-of-the-art descriptors. We also apply our segmentation algorithm to the task of detecting natural paths in outdoor images. The whole system has been demonstrated to work online alongside localization, 3D obstacle detection, and planning.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2008-09</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086">https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>17/02/2025, 10:59:36</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 2153-0866</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>4078-4085</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/IROS.2008.4651086">10.1109/IROS.2008.4651086</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>17/02/2025, 10:59:29</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>17/02/2025, 11:26:16</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Terrain classification</li>
					<li>UGV</li>
					<li>Trail classification</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_S7EBMAWJ">
<div><div data-schema-version="9"><h1>My notes</h1>
<ul>
<li>
create a vector of LAB, and texture around the pixel
</li>
<li>
k-means to find 16 centroids “textons”
</li>
<li>
for each pixels do a histogram of the classification around the pixel
</li>
<li>
k-means on the histogram
</li>
<li>
EMD (using the distance between the textons) to merge clusters that are “too” close
</li>
</ul>
<p></p>
<p><a href="https://github.com/Seb-sti1/mastersthesis/blob/master/scripts/fast_colortexture_seg_outdoor_robots.py" rel="noopener noreferrer nofollow">https://github.com/Seb-sti1/mastersthesis/blob/master/scripts/fast_colortexture_seg_outdoor_robots.py</a></p>
<p><a href="https://github.com/tbjszhu/" rel="noopener noreferrer nofollow">https://github.com/tbjszhu/</a></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_3BSXZ9L2">Full Text PDF					</li>
					<li id="item_SBS4EJQG">IEEE Xplore Abstract Record					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_SMMHC9IS">Appearance contrast for fast, robust trail-following</li>
				</ul>
			</li>


			<li id="item_EWGJR88Y" class="item computerProgram">
			<h2>GStreamer/gstreamer</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Software</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>GStreamer open-source multimedia framework</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-07-02T07:43:45Z</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>GitHub</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://github.com/GStreamer/gstreamer">https://github.com/GStreamer/gstreamer</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>02/07/2025, 10:12:32</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>original-date: 2015-12-01T21:49:40Z</td>
					</tr>
					<tr>
					<th>Company</th>
						<td>GStreamer GitHub mirrors</td>
					</tr>
					<tr>
					<th>Prog. Language</th>
						<td>C</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>02/07/2025, 10:12:32</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>02/07/2025, 10:12:32</td>
					</tr>
				</table>
			</li>


			<li id="item_NTAMHZPU" class="item blogPost">
			<h2>Husky A300</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Blog Post</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Husky A300 is a rugged, customizable and open-source mobile robotic platform that accelerates robotics research and streamlines the development of robotic solutions for commerical deployment.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-US</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://clearpathrobotics.com/husky-a300-unmanned-ground-vehicle-robot/">https://clearpathrobotics.com/husky-a300-unmanned-ground-vehicle-robot/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:27:22</td>
					</tr>
					<tr>
					<th>Blog Title</th>
						<td>Clearpath Robotics</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:27:22</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:27:22</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_95ZTG7NX">Snapshot					</li>
				</ul>
			</li>


			<li id="item_7S452EMF" class="item journalArticle">
			<h2>ICP Algorithm: Theory, Practice And Its SLAM-oriented Taxonomy</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hao Bai</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The Iterative Closest Point (ICP) algorithm is one of the most important algorithms for geometric alignment of three-dimensional surface registration, which is frequently used in computer vision tasks, including the Simultaneous Localization And Mapping (SLAM) tasks. In this paper, we illustrate the theoretical principles of the ICP algorithm, how it can be used in surface registration tasks, and the traditional taxonomy of the variants of the ICP algorithm. As SLAM is becoming a popular topic, we also introduce a SLAM-oriented taxonomy of the ICP algorithm, based on the characteristics of each type of SLAM task, including whether the SLAM task is online or not and whether the landmarks are present as features in the SLAM task. We make a synthesis of each type of SLAM task by comparing several up-to-date research papers and analyzing their implementation details.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-3-22</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>ICP Algorithm</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2206.06435">http://arxiv.org/abs/2206.06435</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>24/06/2025, 12:10:57</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2206.06435 [cs]</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>10-21</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Applied and Computational Engineering</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.54254/2755-2721/2/20220512">10.54254/2755-2721/2/20220512</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>ACE</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2755-2721, 2755-273X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>24/06/2025, 12:10:57</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 10:53:12</td>
					</tr>
				</table>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_FYAGJPS3">
<p class="plaintext">Comment: Accepted by CONF-CDS&apos;22</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_FBELEI23">Preprint PDF					</li>
					<li id="item_L4QMAQGZ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_587YNYYP" class="item journalArticle">
			<h2>IEEE Standard for Information Technology–Telecommunications and Information Exchange between Systems - Local and Metropolitan Area Networks–Specific Requirements - Part 11: Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specifications</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Technical corrections and clarifications to IEEE Std 802.11 for wireless local area networks (WLANs) as well as enhancements to the existing medium access control (MAC) and physical layer (PHY) functions are specified in this revision. Amendments 1 to 5 published in 2016 and 2018 have also been incorporated into this revision.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2021-02</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>IEEE Standard for Information Technology–Telecommunications and Information Exchange between Systems - Local and Metropolitan Area Networks–Specific Requirements - Part 11</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/9363693">https://ieeexplore.ieee.org/document/9363693</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>16/06/2025, 18:27:22</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-4379</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Std 802.11-2020 (Revision of IEEE Std 802.11-2016)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/IEEESTD.2021.9363693">10.1109/IEEESTD.2021.9363693</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>16/06/2025, 18:27:22</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>18/06/2025, 15:24:51</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MN9IBNL6">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_DQSGYK2J" class="item journalArticle">
			<h2>Incremental dense semantic stereo fusion for large-scale semantic scene reconstruction.</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>V. Vineet</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>O. Miksik</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>M. Lidegaard</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>M. Nießner</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>S. Golodetz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>V. Prisacariu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>O. Kähler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>D. Murray</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>S. Izadi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>P. Pérez</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>P. Torr</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Our abilities in scene understanding, which allow us to perceive the 3D structure of our surroundings and intuitively recognise the objects we see, are things that we largely take for granted, but for robots, the task of understanding large scenes quickly remains extremely challenging. Recently, scene understanding approaches based on 3D reconstruction and semantic segmentation have become popular, but existing methods either do not scale, fail outdoors, provide only sparse reconstructions or are rather slow. In this paper, we build on a recent hash-based technique for large-scale fusion and an efficient mean-field inference algorithm for densely-connected CRFs to present what to our knowledge is the first system that can perform dense, large-scale, outdoor semantic reconstruction of a scene in (near) real time. We also present a &apos;semantic fusion&apos; approach that allows us to handle dynamic objects more effectively than previous approaches. We demonstrate the effectiveness of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense reconstruction and labelling of a number of scenes.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2015</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>ora.ox.ac.uk</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ora.ox.ac.uk/objects/uuid:0758dad0-6b33-40d1-bade-0cc2eb6f989a">https://ora.ox.ac.uk/objects/uuid:0758dad0-6b33-40d1-bade-0cc2eb6f989a</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:29:52</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISBN: 9781479969234
Publisher: Institute of Electrical and Electronics Engineers</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>2015-June</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>ICRA 2015: IEEE International Conference on Robotics and Automation</td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>June</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1050-4729</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:29:52</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:31:15</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>3D terrain reconstruction</li>
					<li>Ignored</li>
					<li>move_base_flex</li>
					<li>Dense</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_M54MZA6U">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_U8WQ978N" class="item webpage">
			<h2>Journal of Open Source Software: DetecTree: Tree detection from aerial imagery in Python</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martí Bosch</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://joss.theoj.org/papers/10.21105/joss.02172">https://joss.theoj.org/papers/10.21105/joss.02172</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>04/03/2025, 09:32:01</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>https://github.com/martibosch/detectree</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>04/03/2025, 09:32:01</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>04/03/2025, 09:44:25</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Terrain classification</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_KIXEKSHC">Journal of Open Source Software: DetecTree: Tree detection from aerial imagery in Python					</li>
				</ul>
			</li>


			<li id="item_UVY2NPV7" class="item blogPost">
			<h2>Le TUNDRA 2</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Blog Post</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Le TUNDRA 2 est un drone multirotor, véritable plateforme porte-outils, ultra modulable, multimétiers et plug &amp; play.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>fr-FR</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.hexadrone.fr/produits/drone-tundra/">https://www.hexadrone.fr/produits/drone-tundra/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:27:05</td>
					</tr>
					<tr>
					<th>Blog Title</th>
						<td>Hexadrone</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:27:05</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:27:05</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_B3UWB5MS">Snapshot					</li>
				</ul>
			</li>


			<li id="item_FSB8VHAN" class="item preprint">
			<h2>Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Travis Manderson</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stefan Wapnick</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Meger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gregory Dudek</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present a method for learning to drive on smooth terrain while simultaneously avoiding collisions in challenging off-road and unstructured outdoor environments using only visual inputs. Our approach applies a hybrid model-based and model-free reinforcement learning method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors. Notably, we provide both ﬁrst-person and overhead aerial image inputs to our model. We ﬁnd that the fusion of these complementary inputs improves planning foresight and makes the model robust to visual obstructions. Our results show the ability to generalize to environments with plentiful vegetation, various types of rock, and sandy trails. During evaluation, our policy attained 90% smooth terrain traversal and reduced the proportion of rough terrain driven over by 6.1 times compared to a model using only ﬁrstperson imagery. Video and project details can be found at www.cim.mcgill.ca/mrl/offroad driving/.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-04-09</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2004.04697">http://arxiv.org/abs/2004.04697</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>14/02/2025, 15:56:22</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2004.04697 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2004.04697">10.48550/arXiv.2004.04697</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2004.04697</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>14/02/2025, 15:56:22</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>25/02/2025, 13:42:02</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Reinforcement learning</li>
					<li>UGV</li>
					<li>UAV</li>
					<li>Traversability</li>
					<li>Local path</li>
					<li>Collaboration</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_ZNKP9WVA">
<div><div data-schema-version="9"><h1>My notes</h1>
<p></p>
<blockquote>
<p>In this paper, we present a system for learning a navigation policy that **preferentially chooses smooth terrain** [...]. The emphasis of the paper, however, is not road classification perse, but rather to propose an approach for **online adaptive self-supervised learning** for off-road driving in rough terrain and to explore the synthesis of aerial and first-person (ground) sensing in this context.</p>
</blockquote>
<ul>
<li>
From BEV, FPS images, use CNN to predict the rougher terrain for $H$ steps (supplying an action for every step)
</li>
<li>
<p>use of Value Prediction Networks, a hybrid model-based and model-free reinforcement learning architecture.</p>
<ul>
<li>
It is model-based as it implicitly learns a dynamics model for abstract states optimized for predicting future rewards and value functions.
</li>
<li>
It is also model-free as it maps these encoded abstract states to rewards and value functions using direct experience with the environment prior to the planning phase.
</li>
</ul>
</li>
<li>
In training terrain roughness is estimated using IMU and obstacles using short-range LiDAR.
</li>
<li>
Reward based on the difference between the prediction and the actual roughness
</li>
<li>
It is classification as type of terrain is associated with different number and the model tries to predict the correct one
</li>
</ul>
</div></div>
					</li>
					<li id="item_77UHAYN3">
<p class="plaintext">Comment: ICRA 2020. Video and project details can be found at http://www.cim.mcgill.ca/mrl/offroad_driving/</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_P5MEPZCE">PDF					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_EBSKRPCJ">Terrain Classification from Aerial Data to Support Ground Vehicle Navigation</li>
					<li id="item_PCHP437Z">The Focussed D* Algorithm for Real-Time Replanning</li>
					<li id="item_XDL2HUPR">An Aerial-Ground Robotic System for Navigation and Obstacle Mapping in Large Outdoor Areas</li>
				</ul>
			</li>


			<li id="item_KLJQ65XJ" class="item preprint">
			<h2>Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jonah Philion</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sanja Fidler</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eyeview grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’seye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eyeview cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2020-08-13</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Lift, Splat, Shoot</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2008.05711">http://arxiv.org/abs/2008.05711</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:32:23</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2008.05711 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2008.05711">10.48550/arXiv.2008.05711</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2008.05711</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:32:23</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:33:02</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Terrain classification</li>
					<li>UGV</li>
					<li>To read</li>
					<li>BEV</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_8JK5MZBG">
<p class="plaintext">Comment: ECCV 2020</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JIVLXPUU">PDF					</li>
				</ul>
			</li>


			<li id="item_SPBKWCV8" class="item conferencePaper">
			<h2>Numba: a LLVM-based Python JIT compiler</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Siu Kwan Lam</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Antoine Pitrou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Stanley Seibert</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>novembre 15, 2015</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Numba</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>ACM Digital Library</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://dl.acm.org/doi/10.1145/2833157.2833162">https://dl.acm.org/doi/10.1145/2833157.2833162</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>24/06/2025, 02:00:00</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>New York, NY, USA</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Association for Computing Machinery</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4503-4005-2</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1–6</td>
					</tr>
					<tr>
					<th>Series</th>
						<td>LLVM &apos;15</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/2833157.2833162">10.1145/2833157.2833162</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>24/06/2025, 10:00:18</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>24/06/2025, 10:00:18</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PPUKKP9G">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_7PTFDI8W" class="item webpage">
			<h2>NVIDIA Jetson AGX Orin</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Next-level AI performance for next-gen robotics.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-us</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:32:48</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>NVIDIA</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:32:48</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:32:48</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PY3TK7HT">Snapshot					</li>
				</ul>
			</li>


			<li id="item_FN3WEPUT" class="item preprint">
			<h2>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Hanwen Jiang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Arjun Karpur</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bingyi Cao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Qixing Huang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andre Araujo</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time. Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors. We perform comprehensive experiments on a suite of $7$ datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue&apos;s novel components lead to relative gains on unseen domains of $20.9\%$ with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by $9.5\%$ relatively.Code and model can be found at https://hwjiang1510.github.io/OmniGlue</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-05-21</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>OmniGlue</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2405.12979">http://arxiv.org/abs/2405.12979</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>25/06/2025, 09:28:39</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2405.12979 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2405.12979">10.48550/arXiv.2405.12979</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2405.12979</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>25/06/2025, 09:28:39</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>25/06/2025, 09:28:39</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_D4REESDJ">
<p class="plaintext">Comment: CVPR 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_93LNPGJS">Preprint PDF					</li>
					<li id="item_UV7QGYZW">Snapshot					</li>
				</ul>
			</li>


			<li id="item_DGKD5GX7" class="item bookSection">
			<h2>“On-the-Spot Training” for Terrain Classification in Autonomous Air-Ground Collaborative Teams</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Book Section</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Dana Kulić</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Yoshihiko Nakamura</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Oussama Khatib</td>
					</tr>
					<tr>
						<th class="editor">Editor</th>
						<td>Gentiane Venture</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jeffrey Delmerico</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Alessandro Giusti</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Elias Mueggler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luca Maria Gambardella</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Davide Scaramuzza</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We consider the problem of performing rapid training of a terrain classiﬁer in the context of a collaborative robotic search and rescue system. Our system uses a vision-based ﬂying robot to guide a ground robot through unknown terrain to a goal location by building a map of terrain class and elevation. However, due to the unknown environments present in search and rescue scenarios, our system requires a terrain classiﬁer that can be trained and deployed quickly, based on data collected on the spot. We investigate the relationship of training set size and complexity on training time and accuracy, for both feature-based and convolutional neural network classiﬁers in this scenario. Our goal is to minimize the deployment time of the classiﬁer in our terrain mapping system within acceptable classiﬁcation accuracy tolerances. So we are not concerned with training a classiﬁer that generalizes well, only one that works well for this particular environment. We demonstrate that we can launch our aerial robot, gather data, train a classiﬁer, and begin building a terrain map after only 60 seconds of ﬂight.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2017</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://link.springer.com/10.1007/978-3-319-50115-4_50">http://link.springer.com/10.1007/978-3-319-50115-4_50</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>18/06/2025, 14:52:27</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>http://www.springer.com/tdm</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Series Title: Springer Proceedings in Advanced Robotics
DOI: 10.1007/978-3-319-50115-4_50</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Cham</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>Springer International Publishing</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-3-319-50114-7 978-3-319-50115-4</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>574-585</td>
					</tr>
					<tr>
					<th>Book Title</th>
						<td>2016 International Symposium on Experimental Robotics</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>18/06/2025, 14:52:27</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>18/06/2025, 14:52:27</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_BRXE49RP">PDF					</li>
				</ul>
			</li>


			<li id="item_ETAP4JXR" class="item webpage">
			<h2>OS1: High-Res Mid-Range Lidar Sensor for Automation &amp; Security | Ouster</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Discover OS1 by Ouster: a mid-range lidar sensor designed for precision mapping, robotics, trucking, and security. Learn more here.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>OS1</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ouster.com/products/hardware/os1-lidar-sensor">https://ouster.com/products/hardware/os1-lidar-sensor</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:32:29</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:32:29</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:32:29</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_YJXAJVKP">Snapshot					</li>
				</ul>
			</li>


			<li id="item_UZL99JAW" class="item webpage">
			<h2>Ouster OSDome: 180º FOV Hemispherical Lidar for Warehouse &amp; Security | Ouster</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ouster.com/products/hardware/osdome-lidar-sensor">https://ouster.com/products/hardware/osdome-lidar-sensor</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:31:11</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:31:11</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:31:11</td>
					</tr>
				</table>
			</li>


			<li id="item_XDIUU67Y" class="item webpage">
			<h2>Q10F-Single Sensor Series-Viewpro Ltd</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.viewprotech.com/index.php?ac=article&amp;at=read&amp;did=279">https://www.viewprotech.com/index.php?ac=article&amp;at=read&amp;did=279</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:34:44</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:34:44</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:34:44</td>
					</tr>
				</table>
			</li>


			<li id="item_UMANRDA6" class="item journalArticle">
			<h2>Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Martin A. Fischler</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Robert C. Bolles</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>juin 1, 1981</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Random sample consensus</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>ACM Digital Library</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://dl.acm.org/doi/10.1145/358669.358692">https://dl.acm.org/doi/10.1145/358669.358692</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>29/06/2025, 15:51:17</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>24</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>381–395</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Commun. ACM</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/358669.358692">10.1145/358669.358692</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>6</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0001-0782</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>29/06/2025, 15:51:17</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>29/06/2025, 15:51:17</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ACIE7ENK">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_NGJ5PVX7" class="item journalArticle">
			<h2>Real-time dense stereo for intelligent vehicles</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>W. van der Mark</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>D.M. Gavrila</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Stereo vision is an attractive passive sensing technique for obtaining three-dimensional (3-D) measurements. Recent hardware advances have given rise to a new class of real-time dense disparity estimation algorithms. This paper examines their suitability for intelligent vehicle (IV) applications. In order to gain a better understanding of the performance and the computational-cost tradeoff, the authors created a framework of real-time implementations. This consists of different methodical components based on single instruction multiple data (SIMD) techniques. Furthermore, the resulting algorithmic variations are compared with other publicly available algorithms. The authors argue that existing publicly available stereo data sets are not very suitable for the IV domain. Therefore, the authors&apos; evaluation of stereo algorithms is based on novel realistically looking simulated data as well as real data from complex urban traffic scenes. In order to facilitate future benchmarks, all data used in this paper is made publicly available. The results from this study reveal that there is a considerable influence of scene conditions on the performance of all tested algorithms. Approaches that aim for (global) search optimization are more affected by this than other approaches. The best overall performance is achieved by the proposed multiple-window algorithm, which uses local matching and a left-right check for a robust error rejection. Timing results show that the simplest of the proposed SIMD variants are more than twice as fast than the most complex one. Nevertheless, the latter still achieves real-time processing speeds, while their average accuracy is at least equal to that of publicly available non-SIMD algorithms</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2006-03</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551">https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>17/02/2025, 17:14:39</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Conference Name: IEEE Transactions on Intelligent Transportation Systems</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>38-50</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Transactions on Intelligent Transportation Systems</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/TITS.2006.869625">10.1109/TITS.2006.869625</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1558-0016</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>17/02/2025, 17:14:39</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>17/02/2025, 17:15:22</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>To read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_QNS6ET8R">Full Text PDF					</li>
					<li id="item_AUIPZ59T">IEEE Xplore Abstract Record					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_BJACI2PR">Accurate Quadrifocal Tracking for Robust 3D Visual Odometry</li>
				</ul>
			</li>


			<li id="item_4ELK6RA3" class="item conferencePaper">
			<h2>REMODE: Probabilistic, monocular dense reconstruction in real time</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Matia Pizzoli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Forster</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Davide Scaramuzza</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper, we solve the problem of estimating dense and accurate depth maps from a single moving camera. A probabilistic depth measurement is carried out in real time on a per-pixel basis and the computed uncertainty is used to reject erroneous estimations and provide live feedback on the reconstruction progress. Our contribution is a novel approach to depth map computation that combines Bayesian estimation and recent development on convex optimization for image processing. We demonstrate that our method outperforms stateof-the-art techniques in terms of accuracy, while exhibiting high efﬁciency in memory usage and computing power. We call our approach REMODE (REgularized MOnocular Depth Estimation) and the CUDA-based implementation runs at 30Hz on a laptop computer.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>5/2014</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>REMODE</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://ieeexplore.ieee.org/document/6907233/">http://ieeexplore.ieee.org/document/6907233/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:31:06</td>
					</tr>
					<tr>
					<th>Place</th>
						<td>Hong Kong, China</td>
					</tr>
					<tr>
					<th>Publisher</th>
						<td>IEEE</td>
					</tr>
					<tr>
					<th>ISBN</th>
						<td>978-1-4799-3685-4</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>2609-2616</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2014 IEEE International Conference on Robotics and Automation (ICRA)</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2014 IEEE International Conference on Robotics and Automation (ICRA)</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/ICRA.2014.6907233">10.1109/ICRA.2014.6907233</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:31:06</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:31:54</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>3D terrain reconstruction</li>
					<li>Ignored</li>
					<li>move_base_flex</li>
					<li>Dense</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_AMKPZGUT">PDF					</li>
				</ul>
			</li>


			<li id="item_CGSGCNUU" class="item journalArticle">
			<h2>Road Similarity-Based BEV-Satellite Image Matching for UGV Localization</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhenping Sun</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chuang Yang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yafeng Bu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bokai Liu</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jun Zeng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xiaohui Li</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>To address the challenge of autonomous UGV localization in GNSS-denied off-road environments, this study proposes a matching-based localization method that leverages BEV perception image and satellite map within a road similarity space to achieve high-precision positioning.We first implement a robust LiDAR-inertial odometry system, followed by the fusion of LiDAR and image data to generate a local BEV perception image of the UGV. This approach mitigates the significant viewpoint discrepancy between ground-view images and satellite map. The BEV image and satellite map are then projected into the road similarity space, where normalized cross correlation (NCC) is computed to assess the matching score.Finally, a particle filter is employed to estimate the probability distribution of the vehicle’s pose.By comparing with GNSS ground truth, our localization system demonstrated stability without divergence over a long-distance test of 10 km, achieving an average lateral error of only 0.89 meters and an average planar Euclidean error of 3.41 meters. Furthermore, it maintained accurate and stable global localization even under nighttime conditions, further validating its robustness and adaptability.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>04/2025</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>18/06/2025, 15:48:21</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>18/06/2025, 16:48:37</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>To read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_SRLEFT3Y">PDF					</li>
				</ul>
			</li>


			<li id="item_6P62JPRP" class="item preprint">
			<h2>Spatiotemporal Contrastive Learning for Cross-View Video Localization in Unstructured Off-road Terrains</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhiyun Deng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dongmyeong Lee</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Amanda Adkins</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jesse Quattrociocchi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Ellis</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Joydeep Biswas</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Robust cross-view 3-DoF localization in GPS-denied, off-road environments remains a fundamental challenge due to two key factors: (1) perceptual ambiguities arising from repetitive vegetation and unstructured terrain that lack distinctive visual features, and (2) seasonal appearance shifts that alter scene characteristics well beyond chromatic variation, making it difficult to match current ground-view observations to outdated satellite imagery. To address these challenges, we introduce MoViX, a selfsupervised cross-view video localization framework that learns representations robust to viewpoint and seasonal variation, while preserving directional awareness critical for accurate localization. MoViX employs a pose-dependent positive sampling strategy to enhance directional discrimination and temporally aligned hard negative mining to discourage shortcut learning from seasonal appearance cues. A motion-informed frame sampler selects spatially diverse video frames, and a lightweight, quality-aware temporal aggregator prioritizes frames with strong geometric alignment while downweighting ambiguous ones. At inference, MoViX operates within a Monte Carlo Localization framework, replacing traditional handcrafted measurement models with a learned cross-view neural matching module. Belief updates are modulated through entropy-guided temperature scaling, allowing the filter to maintain multiple pose hypotheses under visual ambiguity and converge confidently when reliable evidence is observed. We evaluate MoViX on the TartanDrive 2.0 dataset, training on less than 30 minutes of driving data and testing over 12.29 km of trajectories. Despite using temporally mismatched satellite imagery, MoViX localizes within 25 meters of ground truth for 93% of the time and within 50 meters for 100% in unseen regions, outperforming state-of-the-art baselines without environment-specific tuning. We also showcase its generalization on a real-world off-road dataset collected in a geographically distinct location with a different robot platform. The code will be made publicly available upon publication. A demonstration video is available at: https://youtu.be/y5wL8nUEuH0.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-06-05</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2506.05250">http://arxiv.org/abs/2506.05250</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>18/06/2025, 15:48:32</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2506.05250 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2506.05250">10.48550/arXiv.2506.05250</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2506.05250</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>18/06/2025, 15:48:32</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>18/06/2025, 16:03:05</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>To read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_PI4TTIDP">PDF					</li>
				</ul>
			</li>


			<li id="item_LVSI353T" class="item journalArticle">
			<h2>Statistically robust 2-D visual servoing</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A.I. Comport</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>E. Marchand</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>F. Chaumette</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>A fundamental step towards broadening the use of real world image-based visual servoing is to deal with the important issue of reliability and robustness. In order to address this issue, a closed loop control law is proposed that simultaneously accomplishes a visual servoing task and is robust to a general class of image processing errors. This is achieved with the application of widely accepted statistical techniques such as robust M-estimation and LMedS. Experimental results are presented which demonstrate visual servoing tasks that resist severe outlier contamination.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>04/2006</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>DOI.org (Crossref)</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://ieeexplore.ieee.org/document/1618537/">http://ieeexplore.ieee.org/document/1618537/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>14/02/2025, 15:40:06</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>22</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>415-420</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Transactions on Robotics</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/TRO.2006.870666">10.1109/TRO.2006.870666</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>IEEE Trans. Robot.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1552-3098</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>14/02/2025, 15:40:06</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 12:24:03</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>M-estimator</li>
					<li>Robust matching</li>
					<li>Key points</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_NY7AVT7D">
<div><div data-citation-items="%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22This%20paper%20describes%20a%20new%20image-based%20approach%20to%20tracking%20the%206dof%20trajectory%20of%20a%20stereo%20camera%20pair%20using%20a%20corresponding%20reference%20image%20pairs%20instead%20of%20explicit%203D%20feature%20reconstruction%20of%20the%20scene.%20A%20dense%20minimisation%20approach%20is%20employed%20which%20directly%20uses%20all%20grey-scale%20information%20available%20within%20the%20stereo%20pair%20(or%20stereo%20region)%20leading%20to%20very%20robust%20and%20precise%20results.%20Metric%203D%20structure%20constraints%20are%20imposed%20by%20consistently%20warping%20corresponding%20stereo%20images%20to%20generate%20novel%20viewpoints%20at%20each%20stereo%20acquisition.%20An%20iterative%20non-linear%20trajectory%20estimation%20approach%20is%20formulated%20based%20on%20a%20quadrifocal%20relationship%20between%20the%20image%20intensities%20within%20adjacent%20views%20of%20the%20stereo%20pair.%20A%20robust%20M-estimation%20technique%20is%20used%20to%20reject%20outliers%20corresponding%20to%20moving%20objects%20within%20the%20scene%20or%20other%20outliers%20such%20as%20occlusions%20and%20illumination%20changes.%20The%20technique%20is%20applied%20to%20recovering%20the%20trajectory%20of%20a%20moving%20vehicle%20in%20long%20and%20difficult%20sequences%20of%20images.%22%2C%22container-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22DOI%22%3A%2210.1109%2FROBOT.2007.363762%22%2C%22event-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22note%22%3A%22ISSN%3A%201050-4729%22%2C%22page%22%3A%2240-45%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Accurate%20Quadrifocal%20Tracking%20for%20Robust%203D%20Visual%20Odometry%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4209067%2F%3Farnumber%3D4209067%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Comport%22%2C%22given%22%3A%22A.I.%22%7D%2C%7B%22family%22%3A%22Malis%22%2C%22given%22%3A%22E.%22%7D%2C%7B%22family%22%3A%22Rives%22%2C%22given%22%3A%22P.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222007%22%2C4%5D%5D%7D%7D%7D%5D" data-schema-version="9"><h1>My notes</h1>
<p>Using M-estimators allow to exclude outliers from the features.</p>
<p>This can be used with sparse features (e.g. key points) as shown in the paper or when dense method like in <span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22itemData%22%3A%7B%22id%22%3A11%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22This%20paper%20describes%20a%20new%20image-based%20approach%20to%20tracking%20the%206dof%20trajectory%20of%20a%20stereo%20camera%20pair%20using%20a%20corresponding%20reference%20image%20pairs%20instead%20of%20explicit%203D%20feature%20reconstruction%20of%20the%20scene.%20A%20dense%20minimisation%20approach%20is%20employed%20which%20directly%20uses%20all%20grey-scale%20information%20available%20within%20the%20stereo%20pair%20(or%20stereo%20region)%20leading%20to%20very%20robust%20and%20precise%20results.%20Metric%203D%20structure%20constraints%20are%20imposed%20by%20consistently%20warping%20corresponding%20stereo%20images%20to%20generate%20novel%20viewpoints%20at%20each%20stereo%20acquisition.%20An%20iterative%20non-linear%20trajectory%20estimation%20approach%20is%20formulated%20based%20on%20a%20quadrifocal%20relationship%20between%20the%20image%20intensities%20within%20adjacent%20views%20of%20the%20stereo%20pair.%20A%20robust%20M-estimation%20technique%20is%20used%20to%20reject%20outliers%20corresponding%20to%20moving%20objects%20within%20the%20scene%20or%20other%20outliers%20such%20as%20occlusions%20and%20illumination%20changes.%20The%20technique%20is%20applied%20to%20recovering%20the%20trajectory%20of%20a%20moving%20vehicle%20in%20long%20and%20difficult%20sequences%20of%20images.%22%2C%22container-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22DOI%22%3A%2210.1109%2FROBOT.2007.363762%22%2C%22event-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22note%22%3A%22ISSN%3A%201050-4729%22%2C%22page%22%3A%2240-45%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Accurate%20Quadrifocal%20Tracking%20for%20Robust%203D%20Visual%20Odometry%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4209067%2F%3Farnumber%3D4209067%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Comport%22%2C%22given%22%3A%22A.I.%22%7D%2C%7B%22family%22%3A%22Malis%22%2C%22given%22%3A%22E.%22%7D%2C%7B%22family%22%3A%22Rives%22%2C%22given%22%3A%22P.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222007%22%2C4%5D%5D%7D%7D%7D%5D%2C%22properties%22%3A%7B%7D%7D">(<span class="citation-item">Comport et al., 2007</span>)</span>.</p>
<p>The goal is to obtain robust behavior.</p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_GFRSXJ9F">PDF					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_BJACI2PR">Accurate Quadrifocal Tracking for Robust 3D Visual Odometry</li>
				</ul>
			</li>


			<li id="item_6XKV8IEF" class="item preprint">
			<h2>Survey on Datasets for Perception in Unstructured Outdoor Environments</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Peter Mortimer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Mirko Maehlisch</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Perception is an essential component of pipelines in field robotics. In this survey, we quantitatively compare publicly available datasets available in unstructured outdoor environments. We focus on datasets for common perception tasks in field robotics. Our survey categorizes and compares available research datasets. This survey also reports on relevant dataset characteristics to help practitioners determine which dataset fits best for their own application. We believe more consideration should be taken in choosing compatible annotation policies across the datasets in unstructured outdoor environments.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-29</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2404.18750">http://arxiv.org/abs/2404.18750</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>17/02/2025, 09:41:36</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2404.18750 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2404.18750">10.48550/arXiv.2404.18750</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2404.18750</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>17/02/2025, 09:41:36</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>17/02/2025, 11:20:25</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Dataset</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_DCT9RF72">
<div><div data-schema-version="9"><h1>My notes</h1>
<p>Minimize using M-estimators the sum of a given function of the error (the difference between the the two feature vectors)</p>
<p></p>
</div></div>
					</li>
					<li id="item_VW65UK6V">
<p class="plaintext">Comment: Accepted to the IEEE ICRA Workshop on Field Robotics 2024</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_2P2595RZ">Preprint PDF					</li>
					<li id="item_BGPQHQ9T">Snapshot					</li>
				</ul>
			</li>


			<li id="item_9XSCP73T" class="item webpage">
			<h2>teb_local_planner - ROS Wiki</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://wiki.ros.org/teb_local_planner">http://wiki.ros.org/teb_local_planner</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/06/2025, 12:34:21</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/06/2025, 12:34:21</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/06/2025, 12:34:21</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_LD67TVRV">teb_local_planner - ROS Wiki					</li>
				</ul>
			</li>


			<li id="item_EBSKRPCJ" class="item journalArticle">
			<h2>Terrain Classification from Aerial Data to Support Ground Vehicle Navigation</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Boris Sofman</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>J. Andrew Bagnell</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anthony Stentz</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nicolas Vandapel</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Sensory perception for unmanned ground vehicle navigation has received great attention from the robotics community. However, sensors mounted on the vehicle are regularly viewpoint impaired. A vehicle navigating at high speeds in off- road environments may be unable to react to negative obstacles such as large holes and cliffs. One approach to address this problem is to complement the sensing capabilities of an unmanned ground vehicle with overhead data gathered from an aerial source. This paper presents techniques to achieve accurate terrain classiﬁcation by utilizing high-density, colorized, three- dimensional laser data. We describe methods to extract relevant features from this sensor data in such a way that a learning algorithm can successfully train on a small set of labeled data in order to classify a much larger map and show experimental results. Additionally, we introduce a technique to signiﬁcantly reduce classiﬁcation errors through the use of context. Finally, we show how this algorithm can be customized for the intended vehicle’s capabilities in order to create more accurate a priori maps that can then be used for path planning.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2006/01/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>kilthub.cmu.edu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1">https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/02/2025, 12:20:58</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Publisher: Carnegie Mellon University</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1184/R1/6561173.v1">10.1184/R1/6561173.v1</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 12:20:58</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 12:25:22</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Terrain classification</li>
					<li>Satellite imagery</li>
					<li>Ignored</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_FB8IZXJW">Full Text PDF					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_EM5KF8EQ">Experimental Analysis of Overhead Data Processing To Support Long Range Navigation</li>
					<li id="item_FSB8VHAN">Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images</li>
				</ul>
			</li>


			<li id="item_PCHP437Z" class="item journalArticle">
			<h2>The Focussed D* Algorithm for Real-Time Replanning</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Anthony Stentz</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:43:39</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>04/03/2025, 09:48:38</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Ignored</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ADX6TQJD">PDF					</li>
				</ul>
				<h3 class="related">Related</h3>
				<ul class="related">
					<li id="item_FSB8VHAN">Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images</li>
				</ul>
			</li>


			<li id="item_MPDL8W2F" class="item webpage">
			<h2>The OpenCV Library</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Gary Bradski</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>OpenCV is an open-source, computer-vision library for extracting and processing meaningful data from images.</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://www.drdobbs.com/open-source/the-opencv-library/184404319">http://www.drdobbs.com/open-source/the-opencv-library/184404319</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>29/06/2025, 17:42:34</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>Dr. Dobb&apos;s</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>29/06/2025, 17:42:34</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>29/06/2025, 17:42:34</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_XWBHQP56">Snapshot					</li>
				</ul>
			</li>


			<li id="item_CNCGJYT2" class="item journalArticle">
			<h2>Topological map construction and scene recognition for vehicle localization</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Huei-Yung Lin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Chia-Wei Yao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Kai-Sheng Cheng</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Van Luan Tran</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper presents a vehicle localization method to assist vehicle navigation based on topological map construction and scene recognition. A topological map is constructed using omni-directional image sequences, and the node information of the topological map is used for place recognition and derivation of vehicle location. In topological map construction and scene change detection, we utilize the Extended-HCT method for semantic description and feature extraction. Content-based and feature-based image retrieval approaches are adopted for place recognition and vehicle localization on the real scene image dataset. The proposed technique is able to construct a real-time image retrieval system for navigation assistance and validate the correctness of the route. Experiments are carried out in both the indoor and outdoor environments using real world images.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2018-01-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s10514-017-9638-9">https://doi.org/10.1007/s10514-017-9638-9</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:25:28</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>42</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>65-81</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Autonomous Robots</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s10514-017-9638-9">10.1007/s10514-017-9638-9</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Auton Robot</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1573-7527</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:25:28</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>21/02/2025, 18:26:10</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>To read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_HAQ3R662">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_TZJPAB8F" class="item journalArticle">
			<h2>Topology-based UAV path planning for multi-view stereo 3D reconstruction of complex structures</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhexiong Shang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Zhigang Shen</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper introduces a new UAV path planning method for creating high-quality 3D reconstruction models of large and complex structures. The core of the new method is incorporating the topology information of the surveyed 3D structure to decompose the multi-view stereo path planning into a collection of overlapped view optimization problems that can be processed in parallel. Different from the existing state-of-the-arts that recursively select the vantage camera views, the new method iteratively resamples all nearby cameras (i.e., positions/orientations) together and achieves a substantial reduction in computation cost while improving reconstruction quality. The new approach also provides a higher-level automation function that facilitates field implementations by eliminating the need for redundant camera initialization as in existing studies. Validations are provided by measuring the variance between the reconstructions to the ground truth models. Results from three synthetic case studies and one real-world application are presented to demonstrate the improved performance. The new method is expected to be instrumental in expanding the adoption of UAV-based multi-view stereo 3D reconstruction of large and complex structures.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2023-02-01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>Springer Link</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1007/s40747-022-00831-5">https://doi.org/10.1007/s40747-022-00831-5</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/02/2025, 14:41:23</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>909-926</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Complex &amp; Intelligent Systems</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1007/s40747-022-00831-5">10.1007/s40747-022-00831-5</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>Complex Intell. Syst.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>2198-6053</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 14:41:23</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 14:42:32</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>3D terrain reconstruction</li>
					<li>UAV</li>
					<li>Not read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_V6DR8NUN">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_6IH6ELM3" class="item journalArticle">
			<h2>Tracking natural trails with swarm-based visual saliency</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Pedro Santana</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Luís Correia</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Ricardo Mendonça</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nelson Alves</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>José Barata</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper proposes a model for trail detection and tracking that builds upon the observation that trails are salient structures in the robot&apos;s visual field. Due to the complexity of natural environments, the straightforward application of bottom-up visual saliency models is not sufficiently robust to predict the location of trails. As for other detection tasks, robustness can be increased by modulating the saliency computation based on a priori knowledge about which pixel-wise visual features are most representative of the object being sought. This paper proposes the use of the object&apos;s overall layout as the primary cue instead, as it is more stable and predictable in natural trails. Bearing in mind computational parsimony and detection robustness, this knowledge is specified in terms of perception-action rules, which control the behavior of simple agents performing as a swarm to compute the saliency map of the input image. For the purpose of tracking, multiframe evidence about the trail location is obtained with a motion-compensated dynamic neural field. In addition, to reduce ambiguity between the trail and trail-like distractors, a simple appearance model is learned online and used to influence the agents&apos; activity. Experimental results on a large data set reveal the ability of the model to produce a success rate on the order of 97% at 20 Hz. The model is shown to be robust in situations where previous models would fail, such as when the trail does not emerge from the lower part of the image or when it is considerably interrupted. © 2012 Wiley Periodicals, Inc.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2013</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>Wiley Online Library</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21423">https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21423</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/02/2025, 14:38:29</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>© 2012 Wiley Periodicals, Inc.</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21423</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>30</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>64-86</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>Journal of Field Robotics</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1002/rob.21423">10.1002/rob.21423</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1556-4967</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 14:38:29</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 14:39:24</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UGV</li>
					<li>Traversability</li>
					<li>Trail classification</li>
					<li>Saliency</li>
					<li>Not read</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_V22WLQTV">Full Text PDF					</li>
					<li id="item_Z9YR84JZ">Snapshot					</li>
				</ul>
			</li>


			<li id="item_ACBEHHKE" class="item webpage">
			<h2>U-Net: Convolutional Networks for Biomedical Image Segmentation</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Olaf Ronneberger</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philipp Fischer</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Thomas Brox</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2015/05/18</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>U-Net</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://arxiv.org/abs/1505.04597v1">https://arxiv.org/abs/1505.04597v1</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/06/2025, 14:25:17</td>
					</tr>
					<tr>
					<th>Website Title</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/06/2025, 14:25:17</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/06/2025, 14:25:17</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_4KQG4UYU">Full Text PDF					</li>
				</ul>
			</li>


			<li id="item_QLAXMXPT" class="item preprint">
			<h2>UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jean-Michel Fortin</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Olivier Gamache</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>William Fecteau</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Effie Daum</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>William Larrivée-Hardy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>François Pomerleau</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Philippe Giguère</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Terrain awareness is an essential milestone to enable truly autonomous off-road navigation. Accurately predicting terrain characteristics allows optimizing a vehicle&apos;s path against potential hazards. Recent methods use deep neural networks to predict traversability-related terrain properties in a self-supervised manner, relying on proprioception as a training signal. However, onboard cameras are inherently limited by their point-of-view relative to the ground, suffering from occlusions and vanishing pixel density with distance. This paper introduces a novel approach for self-supervised terrain characterization using an aerial perspective from a hovering drone. We capture terrain-aligned images while sampling the environment with a ground vehicle, effectively training a simple predictor for vibrations, bumpiness, and energy consumption. Our dataset includes 2.8 km of off-road data collected in forest environment, comprising 13 484 ground-based images and 12 935 aerial images. Our findings show that drone imagery improves terrain property prediction by 21.37 % on the whole dataset and 37.35 % in high vegetation, compared to ground robot images. We conduct ablation studies to identify the main causes of these performance improvements. We also demonstrate the real-world applicability of our approach by scouting an unseen area with a drone, planning and executing an optimized path on the ground.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-09-26</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2409.18253">http://arxiv.org/abs/2409.18253</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>14/02/2025, 17:02:01</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2409.18253 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2409.18253">10.48550/arXiv.2409.18253</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2409.18253</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>14/02/2025, 17:02:01</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 11:37:27</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Terrain classification</li>
					<li>UGV</li>
					<li>UAV</li>
					<li>Dataset</li>
					<li>Traversability</li>
					<li>ResNet18</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_QZ3DMY6Z">
<div><div data-schema-version="9"><h1>My notes</h1>
<ul>
<li>
good documentation of related work
</li>
<li>
datasets created could be very useful
</li>
<li>
makes me think of the work done by Tom.
</li>
</ul>
<p>use of ResNet18 and a "homemade" MLP to predict $M_z$, $M_\omega$ and $M_p$</p>
<p>respectively vibration metric, bumpiness and electrical energy consumption.</p>
<p>BEV from UAV gives better prediction than FPS from UGV.</p>
<p>Using the BEV from UAV, can obtain maps (similar to occupancy grid) (see Fig5)</p>
<p>than can be used to choose "better" path.</p>
<p>-</p>
</div></div>
					</li>
					<li id="item_VPJZJ5YV">
<p class="plaintext">Comment: 7 pages, 5 figures, submitted to ICRA 2025</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_ZK3724GU">Preprint PDF					</li>
					<li id="item_QXD3RTTA">Snapshot					</li>
				</ul>
			</li>


			<li id="item_VY27GQ6G" class="item preprint">
			<h2>VGGT: Visual Geometry Grounded Transformer</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Jianyuan Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Minghao Chen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Nikita Karaev</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andrea Vedaldi</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christian Rupprecht</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>David Novotny</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2025-03-14</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>VGGT</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2503.11651">http://arxiv.org/abs/2503.11651</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>18/06/2025, 16:02:13</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2503.11651 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2503.11651">10.48550/arXiv.2503.11651</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2503.11651</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>18/06/2025, 16:02:13</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>01/07/2025, 14:50:47</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_DQ3LJFHG">
<p class="plaintext">Comment: CVPR 2025, Project Page: https://vgg-t.github.io/</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_5K6YE3HZ">Preprint PDF					</li>
					<li id="item_5ZG6TAAL">Snapshot					</li>
				</ul>
			</li>


			<li id="item_XZ3SQHUM" class="item conferencePaper">
			<h2>ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Dhruv Shah</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Sergey Levine</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Robotic navigation has been approached as a problem of 3D reconstruction and planning, as well as an end-to-end learning problem. However, long-range navigation requires both planning and reasoning about local traversability, as well as being able to utilize general knowledge about global geography, in the form of a roadmap, GPS, or other side information providing important cues. In this work, we propose an approach that integrates learning and planning, and can utilize side information such as schematic roadmaps, satellite maps and GPS coordinates as a planning heuristic, without relying on them being accurate. Our method, ViKiNG, incorporates a local traversability model, which looks at the robot&apos;s current camera observation and a potential subgoal to infer how easily that subgoal can be reached, as well as a heuristic model, which looks at overhead maps for hints and attempts to evaluate the appropriateness of these subgoals in order to reach the goal. These models are used by a heuristic planner to identify the best waypoint in order to reach the final destination. Our method performs no explicit geometric reconstruction, utilizing only a topological representation of the environment. Despite having never seen trajectories longer than 80 meters in its training dataset, ViKiNG can leverage its image-based learned controller and goal-directed heuristic to navigate to goals up to 3 kilometers away in previously unseen environments, and exhibit complex behaviors such as probing potential paths and backtracking when they are found to be non-viable. ViKiNG is also robust to unreliable maps and GPS, since the low-level controller ultimately makes decisions based on egocentric image observations, using maps only as planning heuristics. For videos of our experiments, please check out our project page https://sites.google.com/view/viking-release.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2022-06-27</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>ViKiNG</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2202.11271">http://arxiv.org/abs/2202.11271</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>21/02/2025, 18:33:44</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2202.11271 [cs]</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>Robotics: Science and Systems XVIII</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.15607/RSS.2022.XVIII.019">10.15607/RSS.2022.XVIII.019</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>21/02/2025, 18:33:44</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>22/04/2025, 09:47:42</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>UGV</li>
					<li>Local path</li>
					<li>Global path</li>
					<li>Vision based navigation</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_KL255QRB">
<p class="plaintext">Comment: Best Systems Paper Finalist at XVII Robotics: Science and Systems (RSS 2022), New York City, USA. Project page https://sites.google.com/view/viking-release</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_H5PV4867">Extension					</li>
					<li id="item_LCN29FN7">PDF					</li>
				</ul>
			</li>


			<li id="item_G3DIK4TB" class="item conferencePaper">
			<h2>Visual terrain classification by flying robots</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Yasir Niaz Khan</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andreas Masselli</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andreas Zell</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>In this paper we investigate the effectiveness of SURF features for visual terrain classification for outdoor flying robots. A quadrocopter fitted with a single camera is flown over different terrains to take images of the ground below. Each image is divided into a grid and SURF features are calculated at grid intersections. A classifier is then used to learn to differentiate between different terrain types. Classification results of the SURF descriptor are compared with results from other texture descriptors like Local Binary Patterns and Local Ternary Patterns. Six different terrain types are considered in this approach. Random forests are used for classification on each descriptor. It is shown that SURF features perform better than other descriptors at higher resolutions.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2012-05</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ieeexplore.ieee.org/document/6224988/?arnumber=6224988">https://ieeexplore.ieee.org/document/6224988/?arnumber=6224988</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/02/2025, 12:17:57</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 1050-4729</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>498-503</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2012 IEEE International Conference on Robotics and Automation</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2012 IEEE International Conference on Robotics and Automation</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/ICRA.2012.6224988">10.1109/ICRA.2012.6224988</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/02/2025, 12:17:57</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/02/2025, 12:18:51</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Terrain classification</li>
					<li>UAV</li>
					<li>TSURF</li>
					<li>LBP</li>
					<li>LTP</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_ZSJNAGKS">
<div><div data-schema-version="9"><h1>My notes</h1>
<p>Comparison of SURF descriptor, LBP, LTP for visual outdoor terrain classification</p>
<ul>
<li>
Random forest gives the best results
</li>
<li>
TSURF good with small grid size and lower training time than LBP, LTP
</li>
</ul>
<p></p>
</div></div>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_7AATA5UI">Full Text PDF					</li>
					<li id="item_7CHRVJQ5">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_7HLXEXMF" class="item computerProgram">
			<h2>Whojo/Spatially-Coherent-Costmap</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Software</td>
					</tr>
					<tr>
						<th class="programmer">Programmer</th>
						<td>Guillaume THOMAS</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-08-14T08:31:17Z</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>GitHub</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://github.com/Whojo/Spatially-Coherent-Costmap">https://github.com/Whojo/Spatially-Coherent-Costmap</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>19/06/2025, 12:11:24</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>MIT</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>original-date: 2023-09-19T07:35:28Z</td>
					</tr>
					<tr>
					<th>Prog. Language</th>
						<td>Jupyter Notebook</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>19/06/2025, 12:11:24</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>19/06/2025, 12:11:24</td>
					</tr>
				</table>
			</li>


			<li id="item_XJLFZGU9" class="item preprint">
			<h2>XFeat: Accelerated Features for Lightweight Image Matching</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Preprint</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Guilherme Potje</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Felipe Cadar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Andre Araujo</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Renato Martins</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Erickson R. Nascimento</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>We introduce a lightweight and accurate architecture for resource-efficient visual correspondence. Our method, dubbed XFeat (Accelerated Features), revisits fundamental design choices in convolutional neural networks for detecting, extracting, and matching local features. Our new model satisfies a critical need for fast and robust algorithms suitable to resource-limited devices. In particular, accurate image matching requires sufficiently large image resolutions - for this reason, we keep the resolution as large as possible while limiting the number of channels in the network. Besides, our model is designed to offer the choice of matching at the sparse or semi-dense levels, each of which may be more suitable for different downstream applications, such as visual navigation and augmented reality. Our model is the first to offer semi-dense matching efficiently, leveraging a novel match refinement module that relies on coarse local descriptors. XFeat is versatile and hardware-independent, surpassing current deep learning-based local features in speed (up to 5x faster) with comparable or better accuracy, proven in pose estimation and visual localization. We showcase it running in real-time on an inexpensive laptop CPU without specialized hardware optimizations. Code and weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2024-04-30</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>XFeat</td>
					</tr>
					<tr>
					<th>Library Catalogue</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/2404.19174">http://arxiv.org/abs/2404.19174</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>25/06/2025, 09:28:58</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>arXiv:2404.19174 [cs]</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.48550/arXiv.2404.19174">10.48550/arXiv.2404.19174</a></td>
					</tr>
					<tr>
					<th>Repository</th>
						<td>arXiv</td>
					</tr>
					<tr>
					<th>Archive ID</th>
						<td>arXiv:2404.19174</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>25/06/2025, 09:28:58</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>25/06/2025, 09:28:58</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Computer Science - Computer Vision and Pattern Recognition</li>
				</ul>
				<h3 class="notes">Notes:</h3>
				<ul class="notes">
					<li id="item_AG6GR3YX">
<p class="plaintext">Comment: CVPR 2024; Source code available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24</p>
					</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_NB9V4TGU">Preprint PDF					</li>
					<li id="item_6ZVBK5AB">Snapshot					</li>
				</ul>
			</li>


			<li id="item_4Q44M7RH" class="item webpage">
			<h2>ZED 2 - AI Stereo Camera | Stereolabs</h2>
				<table>
					<tr>
						<th>Item Type</th>
						<td>Web Page</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>The ZED 2 family is a next-generation series of USB 3.1 stereo cameras that seamlessly integrate advanced depth sensing with AI capabilities. This combination empowers you to develop cutting-edge spatial intelligence applications.</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-FR</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.stereolabs.com/en-fr/products/zed-2">https://www.stereolabs.com/en-fr/products/zed-2</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>03/07/2025, 14:27:25</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>03/07/2025, 14:27:25</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>03/07/2025, 14:27:25</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_EWDSGSHZ">Snapshot					</li>
				</ul>
			</li>

		</ul>
	</body>
</html>