---
theme: default
paginate: true
title: Collaborative aero-terrestrial navigation using Bird-eye view (BEV)
description: Slides on four paper using BEV to improve UGV navigation
author: Sébastien Kerbourc'h
style: |
  section {
    font-size: 20px;
  }
  .center {
    text-align: center;
  }
  .sidenote {
    color: gray;
    font-style: italic;
  }
header: 'Collaborative aero-terrestrial navigation using BEV'
footer: '<img height="50px" src="../../../latex/logo/enstaparis.png"/>&nbsp;<img height="50px" src="../../../latex/logo/dtu.png"/>
  &nbsp;&nbsp;
  _Slides available on my [GitHub](https://github.com/Seb-sti1/mastersthesis)._
  _11/03/25._'
---

# Collaborative aero-terrestrial navigation using Bird-eye view (BEV)

_Sébastien Kerbourc'h (and Msitral for the joke)_

---

1. [J. Zhang et al., "Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments", January 2025](https://arxiv.org/abs/2501.18351)
2. [J. Delmerico et al., "Active Autonomous Aerial Exploration for Ground Robot Path Planning", April 2017](https://ieeexplore.ieee.org/document/7812671?arnumber=7812671)
3. [J.-M. Fortin et al., "UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation", September 2024 - ICRA 2025](https://arxiv.org/abs/2409.18253)

<br/>
<br/>
<br/>

> __Why did the robot with a typo in its BEV navigation system get lost?__
> Because it was trying to follow the "BEV-erly" hills instead of the "BEV[-ery](https://en.wiktionary.org/wiki/-ery)"
> straight path!
> Msitral's joke


---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - J. Zhang et al., "Dual-BEV Nav [...]"'
-->

# J. Zhang et al., "Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments"

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - J. Zhang et al., "Dual-BEV Nav [...]"'
-->

- Local BEV Planning Model - (LBPM)
- Global BEV Planning Model - (GBPM)

<div class="center">
<img src="dual-bev-fig1.png" height="450px" />
</div>

<!--
BEV Local using the sensor of the UGV

BEV Global encodes into a probability map using satellite imaginary
-->

---

## Local BEV Planning Model - (LBPM)

- Local BEV Perception Encoder
    1. Uses _Lift, Splat, Shoot_ (LSS) [^1] & BEVDet [^2]
    2. Extract 3D image features
    3. Projected on BEV using BEV pooling optimization technique (e.g. BEVFusion [^3])
- Task-driven Goal Decoder
    1. Based on ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints [^4]

<div class="center">
<img src="dual-bev-fig2.png" height="300px" />
</div>

[^1]: https://arxiv.org/abs/2008.05711

[^2]: https://arxiv.org/abs/2112.11790

[^3]: https://arxiv.org/abs/2205.13542

[^4]: https://arxiv.org/abs/2202.11271


<!--
Get feature of traversibility in a BEV format to filter latent goal from ViKiNG

ViKiNG: uses vision of current/goal and geographic clues to reach the goal.

TODO LSS image at the end

TODO diff in ViKiNG with Dual-BEV
-->

---

## Global BEV Planning Model - (GBPM)

- Traversability in the overhead map learned from exploration
  > In the trajectories generated by the robot’s autonomous exploration, the more easily accessible areas will be
  covered by a larger number of trajectories.
- Segmentation done using U-Net [^5]

<div class="center">
<img src="dual-bev-fig3.png" width="400px" />
&nbsp;&nbsp;&nbsp;
<img src="dual-bev-fig4.png" width="400px" />
</div>

[^5]: https://arxiv.org/abs/1505.04597

---

## Integration of Local and Global BEV Planning

1. LBPM generates potential traversable paths <span class="sidenote">(temporal distance, GPS offsets, waypoints from
   current position to the goal)<span/>
2. GBPM encodes the overhead map into a global probability map
3. Score the paths ($cost = k \times score + (1 - k) \times temporal\_distance$)

## Results

<div class="center">
<img src="dual-bev-fig6.png" height="250px" />
&nbsp;&nbsp;&nbsp;
<img src="dual-bev-tableII-III.png" height="250px" />
</div>

<!--
Surprised that ViKiNG is "so bad" compared the results presented in the ViKiNG paper.
Maybe because the environment is more unstructured?

The overhead map orient/choose the correct local path.
-->

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - J. Delmerico et al., "Active Autonomous Aerial Exploration for Ground  Robot Path Planning"'
-->

# J. Delmerico et al., "Active Autonomous Aerial Exploration for Ground  Robot Path Planning"

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - J. Delmerico et al., "Active Autonomous Aerial Exploration for Ground  Robot Path Planning"'
-->

## Terrain & Elevation mapping (from UAV)

- Search and rescue scenario. **No map, no prior information** about terrain classes
- Terrain Classification: use their 'on-the-spot' classifier [^8]: operator manually classify few patches live
    - Feature-based classifier trained (60s training time) (https://youtu.be/yVyyhQch6bI?t=50). <span class="sidenote">
      CNN based classifier ~10-15min training time<span/>
- Elevation: monocular dense reconstruction (sequence of image to triangulate) using REMODE [^6]

<div class="center">
<img src="active-aerial-explo-fig2.png" height="300px" />
</div>

[^8]: https://rpg.ifi.uzh.ch/docs/ISER16_Delmerico.pdf

[^6]: https://rpg.ifi.uzh.ch/docs/ICRA14_Pizzoli.pdf

<!--
The probabilistic output of the classifier is accumulated for each cell in the map, and averaged over 
all of the classified patches that have projected to that cell. The most probable class is then used 
to represent the terrain for each cell.

Speed of UGV is defined for each class.
-->

---

## Map creation & exploration

1. UAV manual flight &#8594; terrain classification
2. UAV exploration to do the 3D reconstruction <span class="sidenote">(optimizing for shortest response time of UGV
   using D*)</span> &#8594; avoid hole/ditch
3. UGV navigation to target using computed path

<br/>

_Computation done on a separated laptop_

<!--
This expression is the sum of the time remaining to map a candidate path plus the time required to drive that path with the ground robot

By minimizing this quantity over all candidate paths that cross our known boundary, we select the next waypoint as the one that minimizes the overall response time for the ground robot to reach its goal.
-->

---

## Results

https://youtu.be/s2v6TICaukQ?t=12

<div class="center">
<img src="active-aerial-explo-fig8.png" height="200px" />
</div>

- Simulation: compared to a greedy D*, average speedup of 1.23 (few outlier).
- 8-10 faster than _"exhaustive mapping strategy"_.

<!--
greedy D* = no heuristic used, just take the best next point (given the current optimal path)
-->

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - J.-M. Fortin et al., "UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation"'
-->

# J.-M. Fortin et al., "UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation"

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - J.-M. Fortin et al., "UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation"'
-->

## Context, Dataset

- Warthog (ZED X, Xsens Mti-30 IMU, GNSS+RTK) & DJI Mavic 3E
- Dataset: 45min driving, 13484 ground images, 12935 aerial images. <span class="sidenote">J.-M. Fortin shared a
  subset of the dataset*</span>

### Model, Training

- Model: RestNet18 + 3-layer fully connected network <span class="sidenote">(to regress a single value)</span>
- Training: extraction of patches along the robot path
    - Label generation uses the IMU Z-axis acceleration, roll/pitch angular velocities <span class="sidenote">(based
      on [^7])</span>, rolling window of an approximation of the power consumption
    - Predict vibration $M_z$, bumpiness $M_\omega$, power consumption $M_p$

<div class="center">
<img src="uav-assisted-terrain-awareness-fig2.png" height="200px" />
</div>

[^7]: https://deepblue.lib.umich.edu/bitstream/handle/2027.42/50682/20113_ftp.pdf?sequence=1&isAllowed=y

<!--
Aruco to facilitate visual detection.

UGV synchronization using Precision Time Protocol over ethernet
UAV synchronized manually by finding the video frame where the vehicle initiates movement.

IMU Z-axis acceleration (Power Spectral Density), power  ($P = I \times V$)

1.5 m.s^−1 = 5.4 km/h 

2055 aerial, 2082 ground
-->

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - J.-M. Fortin et al., "UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation"'
-->

## Results

<div class="center">
<img src="uav-assisted-terrain-awareness-tableI.png" height="200px" />
<img src="uav-assisted-terrain-awareness-fig3.png" height="500px" />
<img src="uav-assisted-terrain-awareness-fig5.png" height="500px" />
</div>


[video](https://www.linkedin.com/posts/norlab_icra2025-robotics-science-activity-7296276394244788225-otCI?utm_source=share&utm_medium=member_desktop&rcm=ACoAACnRtr8BZLOSjEnlOms8ZweDMxAt8HerTak)

<!--
UAV better than the best UGV BEV projection

Prediction can 'see' hole 
-->

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - J.-M. Fortin et al. dataset parenthesis'
-->

## J.-M. Fortin et al. shared dataset

- 2055 aerial images, 2082 ground images (and BEV projection)
- IMU, Power, GNSS+RTK Odom, Wheel Odom
- Will be on the Alexandre's NAS (WD My Cloud EX2 Ultra)
    - Login information on the U2IS wiki (/tech/matos/nas)

<!--
Waiting for a reply from JM. Fortin.

`lecture`/`ecriture` users
-->

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - Thoughts'
-->

# Thoughts

- <span class="sidenote">Obviously: BEV UGV < BEV satellite < BEV UAV</span>
- _Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor
  Environments_:
    - Method to create overhead segmented map
    - Unsure of the improvement compared to ViKiNG, heavy for onboard realtime (?)
- _Active Autonomous Aerial Exploration for Ground Robot Path Planning_:
    - dataset on the spot (generalization error?)
    - 3D reconstruction not scalable, manual driving for initial map
- UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation
    - Dataset available, UAV can scout for UGV
    - Imply significant data transfer

<!--

Active Autonomous Aerial Exploration for Ground Robot Path Planning:
 - 3D reconstruction too heavy for onboard realtime
 - 3D reconstruction can be avoided if UGV have 
 - classification on the spot reduce dataset requirement

Dual-BEV
 - Unsure of the improvement compared to ViKiNG (probably better, viking is usually on roads)
 - U-Net navigation


-->

---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV'
-->

<div class="center">

\*
\*&nbsp;&nbsp;&nbsp;\*
</div>


---

<!--
header: 'Collaborative aero-terrestrial navigation using BEV - Appendix'
-->

[T. Manderson et al., "Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images", April 2020 - ICRA 2020](https://ieeexplore.ieee.org/document/9196879)

> Learning to drive on smooth terrain \[...\] \[using an\] hybrid model-based and model-free reinforcement learning
> method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors.

> Our key contributions include the use of IMU self-supervision as a proxy for driveability and the combined use of
> aligned aerial images and first-person on-board images. This multimodal input strengthens the predictive
> capabilities of the model by increasing the available field of view and making it robust to visual obstructions.

<div class="center">
<img src="learning-to-drive-fig10.png" width="450px"/>
</div>