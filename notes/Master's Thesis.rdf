<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/7812671/?arnumber=7812671">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2377-3766"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Delmerico</foaf:surname>
                        <foaf:givenName>Jeffrey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mueggler</foaf:surname>
                        <foaf:givenName>Elias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nitsch</foaf:surname>
                        <foaf:givenName>Julia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_8"/>
        <link:link rdf:resource="#item_4"/>
        <link:link rdf:resource="#item_3"/>
        <dc:subject>Exploration</dc:subject>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Collaboration</dc:subject>
        <dc:subject>Global path</dc:subject>
        <dc:title>Active Autonomous Aerial Exploration for Ground Robot Path Planning</dc:title>
        <dcterms:abstract>We address the problem of planning a path for a ground robot through unknown terrain, using observations from a flying robot. In search and rescue missions, which are our target scenarios, the time from arrival at the disaster site to the delivery of aid is critically important. Previous works required exhaustive exploration before path planning, which is time-consuming but eventually leads to an optimal path for the ground robot. Instead, we propose active exploration of the environment, where the flying robot chooses regions to map in a way that optimizes the overall response time of the system, which is the combined time for the air and ground robots to execute their missions. In our approach, we estimate terrain classes throughout our terrain map, and we also add elevation information in areas where the active exploration algorithm has chosen to perform 3-D reconstruction. This terrain information is used to estimate feasible and efficient paths for the ground robot. By exploring the environment actively, we achieve superior response times compared to both exhaustive and greedy exploration strategies. We demonstrate the performance and capabilities of the proposed system in simulated and real-world outdoor experiments. To the best of our knowledge, this is the first work to address ground robot path planning using active aerial exploration.</dcterms:abstract>
        <dc:date>2017-04</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/7812671/?arnumber=7812671</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 08:30:10</dcterms:dateSubmitted>
        <dc:description>Conference Name: IEEE Robotics and Automation Letters</dc:description>
        <bib:pages>664-671</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2377-3766">
        <prism:volume>2</prism:volume>
        <dc:title>IEEE Robotics and Automation Letters</dc:title>
        <dc:identifier>DOI 10.1109/LRA.2017.2651163</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 2377-3766</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_8">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;1. Initial manual fly to see goal. Camera imagery is also used to obtain initial classification of the terrain.&lt;br&gt;2. Then vision-guided flights (to a series of waypoints) chosen actively. For each waypoint 3D reconstruction and ground&lt;br&gt;robot path (to optimize total duration of the mission).&lt;br&gt;3. Repeat 2. until path is complete&lt;/p&gt;
&lt;p&gt;- visual odometry for loc&amp;amp;nav [^3], [^4]&lt;br&gt;- [^3] also seems to mention matching of FPV (ground drone) and BEV (aerial drone) (p191)&lt;br&gt;- classifier is trained on the spot [^1]. Two models type tried, feature-based and CNN. CNN is significantly slower&lt;br&gt;without giving significant better result (surprising).&lt;br&gt;- in the 2. not exhaustive exploration just along the _global path_ from the manually generated map.&lt;br&gt;The next waypoints for the aerial drone are chosen as to minimize&lt;br&gt;$T_{\text{ground robot}, s \rightarrow b_i} + T_{\text{ground robot}, b_i \rightarrow g} + T_{extend 3d reconstructed region}$,&lt;br&gt;respectively _time of ground robot from s to next ground robot waypoint (uses 3d reconstructed area)_, the _time of&lt;br&gt;ground robot from next ground robot waypoint (uses initial partial map)_, the _time to extend the 3d reconstructed&lt;br&gt;region (in the correct direction)_.&lt;br&gt;- 7,8,9 are ref for high altitude, high resolution aerial images&lt;br&gt;- use of monocular camera to reconstruct 3D ground in real time [^2] (could be used with move_base_flex).&lt;br&gt;- use of [ANYbotics Grid Map](https://github.com/ANYbotics/grid_map) [^5]&lt;br&gt;- Terrain classification, dense 3D reconstruction and exploration algorithm run on a laptop computer not on the drone.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;[^1]: https://rpg.ifi.uzh.ch/docs/ISER16_Delmerico.pdf&lt;/p&gt;
&lt;p&gt;[^2]: https://rpg.ifi.uzh.ch/docs/ICRA14_Pizzoli.pdf&lt;/p&gt;
&lt;p&gt;[^3]: https://rpg.ifi.uzh.ch/docs/PhD16_Forster.pdf&lt;/p&gt;
&lt;p&gt;[^4]: https://ieeexplore.ieee.org/document/6906584&lt;/p&gt;
&lt;p&gt;[^5]: https://www.researchgate.net/publication/284415855_A_Universal_Grid_Map_Library_Implementation_and_Use_Case_for_Rough_Terrain_Navigation&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_4">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=7812671&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 08:30:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_3">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/7812671/?arnumber=7812671</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 08:30:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://ieeexplore.ieee.org/document/1618537/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1552-3098"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Comport</foaf:surname>
                        <foaf:givenName>A.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marchand</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chaumette</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_109"/>
        <link:link rdf:resource="#item_9"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067"/>
        <dc:subject>M-estimator</dc:subject>
        <dc:subject>Robust matching</dc:subject>
        <dc:subject>Key points</dc:subject>
        <dc:title>Statistically robust 2-D visual servoing</dc:title>
        <dcterms:abstract>A fundamental step towards broadening the use of real world image-based visual servoing is to deal with the important issue of reliability and robustness. In order to address this issue, a closed loop control law is proposed that simultaneously accomplishes a visual servoing task and is robust to a general class of image processing errors. This is achieved with the application of widely accepted statistical techniques such as robust M-estimation and LMedS. Experimental results are presented which demonstrate visual servoing tasks that resist severe outlier contamination.</dcterms:abstract>
        <dc:date>04/2006</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/1618537/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:06</dcterms:dateSubmitted>
        <dc:rights>https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html</dc:rights>
        <bib:pages>415-420</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1552-3098">
        <prism:volume>22</prism:volume>
        <dc:title>IEEE Transactions on Robotics</dc:title>
        <dc:identifier>DOI 10.1109/TRO.2006.870666</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>IEEE Trans. Robot.</dcterms:alternative>
        <dc:identifier>ISSN 1552-3098</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_109">
        <rdf:value>&lt;div data-citation-items=&quot;%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22This%20paper%20describes%20a%20new%20image-based%20approach%20to%20tracking%20the%206dof%20trajectory%20of%20a%20stereo%20camera%20pair%20using%20a%20corresponding%20reference%20image%20pairs%20instead%20of%20explicit%203D%20feature%20reconstruction%20of%20the%20scene.%20A%20dense%20minimisation%20approach%20is%20employed%20which%20directly%20uses%20all%20grey-scale%20information%20available%20within%20the%20stereo%20pair%20(or%20stereo%20region)%20leading%20to%20very%20robust%20and%20precise%20results.%20Metric%203D%20structure%20constraints%20are%20imposed%20by%20consistently%20warping%20corresponding%20stereo%20images%20to%20generate%20novel%20viewpoints%20at%20each%20stereo%20acquisition.%20An%20iterative%20non-linear%20trajectory%20estimation%20approach%20is%20formulated%20based%20on%20a%20quadrifocal%20relationship%20between%20the%20image%20intensities%20within%20adjacent%20views%20of%20the%20stereo%20pair.%20A%20robust%20M-estimation%20technique%20is%20used%20to%20reject%20outliers%20corresponding%20to%20moving%20objects%20within%20the%20scene%20or%20other%20outliers%20such%20as%20occlusions%20and%20illumination%20changes.%20The%20technique%20is%20applied%20to%20recovering%20the%20trajectory%20of%20a%20moving%20vehicle%20in%20long%20and%20difficult%20sequences%20of%20images.%22%2C%22container-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22DOI%22%3A%2210.1109%2FROBOT.2007.363762%22%2C%22event-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22note%22%3A%22ISSN%3A%201050-4729%22%2C%22page%22%3A%2240-45%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Accurate%20Quadrifocal%20Tracking%20for%20Robust%203D%20Visual%20Odometry%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4209067%2F%3Farnumber%3D4209067%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Comport%22%2C%22given%22%3A%22A.I.%22%7D%2C%7B%22family%22%3A%22Malis%22%2C%22given%22%3A%22E.%22%7D%2C%7B%22family%22%3A%22Rives%22%2C%22given%22%3A%22P.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222007%22%2C4%5D%5D%7D%7D%7D%5D&quot; data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;Using M-estimators allow to exclude outliers from the features.&lt;/p&gt;
&lt;p&gt;This can be used with sparse features (e.g. key points) as shown in the paper or when dense method like in &lt;span class=&quot;citation&quot; data-citation=&quot;%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%7D%5D%2C%22properties%22%3A%7B%7D%7D&quot;&gt;(&lt;span class=&quot;citation-item&quot;&gt;Comport et al., 2007&lt;/span&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The goal is to obtain robust behavior.&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_9">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://inria.hal.science/inria-00350284/document</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings 2007 IEEE International Conference on Robotics and Automation</dc:title>
                <dc:identifier>DOI 10.1109/ROBOT.2007.363762</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Comport</foaf:surname>
                        <foaf:givenName>A.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malis</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rives</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_74"/>
        <link:link rdf:resource="#item_13"/>
        <link:link rdf:resource="#item_12"/>
        <dc:relation rdf:resource="http://ieeexplore.ieee.org/document/1618537/"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551"/>
        <dc:subject>Dense correspondences</dc:subject>
        <dc:subject>Quadrifocal warping</dc:subject>
        <dc:title>Accurate Quadrifocal Tracking for Robust 3D Visual Odometry</dc:title>
        <dcterms:abstract>This paper describes a new image-based approach to tracking the 6dof trajectory of a stereo camera pair using a corresponding reference image pairs instead of explicit 3D feature reconstruction of the scene. A dense minimisation approach is employed which directly uses all grey-scale information available within the stereo pair (or stereo region) leading to very robust and precise results. Metric 3D structure constraints are imposed by consistently warping corresponding stereo images to generate novel viewpoints at each stereo acquisition. An iterative non-linear trajectory estimation approach is formulated based on a quadrifocal relationship between the image intensities within adjacent views of the stereo pair. A robust M-estimation technique is used to reject outliers corresponding to moving objects within the scene or other outliers such as occlusions and illumination changes. The technique is applied to recovering the trajectory of a moving vehicle in long and difficult sequences of images.</dcterms:abstract>
        <dc:date>2007-04</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:47</dcterms:dateSubmitted>
        <dc:description>ISSN: 1050-4729</dc:description>
        <bib:pages>40-45</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings 2007 IEEE International Conference on Robotics and Automation</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_74">
        <rdf:value>&lt;div data-citation-items=&quot;%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22This%20paper%20describes%20a%20new%20image-based%20approach%20to%20tracking%20the%206dof%20trajectory%20of%20a%20stereo%20camera%20pair%20using%20a%20corresponding%20reference%20image%20pairs%20instead%20of%20explicit%203D%20feature%20reconstruction%20of%20the%20scene.%20A%20dense%20minimisation%20approach%20is%20employed%20which%20directly%20uses%20all%20grey-scale%20information%20available%20within%20the%20stereo%20pair%20(or%20stereo%20region)%20leading%20to%20very%20robust%20and%20precise%20results.%20Metric%203D%20structure%20constraints%20are%20imposed%20by%20consistently%20warping%20corresponding%20stereo%20images%20to%20generate%20novel%20viewpoints%20at%20each%20stereo%20acquisition.%20An%20iterative%20non-linear%20trajectory%20estimation%20approach%20is%20formulated%20based%20on%20a%20quadrifocal%20relationship%20between%20the%20image%20intensities%20within%20adjacent%20views%20of%20the%20stereo%20pair.%20A%20robust%20M-estimation%20technique%20is%20used%20to%20reject%20outliers%20corresponding%20to%20moving%20objects%20within%20the%20scene%20or%20other%20outliers%20such%20as%20occlusions%20and%20illumination%20changes.%20The%20technique%20is%20applied%20to%20recovering%20the%20trajectory%20of%20a%20moving%20vehicle%20in%20long%20and%20difficult%20sequences%20of%20images.%22%2C%22container-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22DOI%22%3A%2210.1109%2FROBOT.2007.363762%22%2C%22event-title%22%3A%22Proceedings%202007%20IEEE%20International%20Conference%20on%20Robotics%20and%20Automation%22%2C%22note%22%3A%22ISSN%3A%201050-4729%22%2C%22page%22%3A%2240-45%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Accurate%20Quadrifocal%20Tracking%20for%20Robust%203D%20Visual%20Odometry%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4209067%2F%3Farnumber%3D4209067%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Comport%22%2C%22given%22%3A%22A.I.%22%7D%2C%7B%22family%22%3A%22Malis%22%2C%22given%22%3A%22E.%22%7D%2C%7B%22family%22%3A%22Rives%22%2C%22given%22%3A%22P.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C14%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222007%22%2C4%5D%5D%7D%7D%7D%2C%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FNGJ5PVX7%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FNGJ5PVX7%22%2C%22type%22%3A%22article-journal%22%2C%22abstract%22%3A%22Stereo%20vision%20is%20an%20attractive%20passive%20sensing%20technique%20for%20obtaining%20three-dimensional%20(3-D)%20measurements.%20Recent%20hardware%20advances%20have%20given%20rise%20to%20a%20new%20class%20of%20real-time%20dense%20disparity%20estimation%20algorithms.%20This%20paper%20examines%20their%20suitability%20for%20intelligent%20vehicle%20(IV)%20applications.%20In%20order%20to%20gain%20a%20better%20understanding%20of%20the%20performance%20and%20the%20computational-cost%20tradeoff%2C%20the%20authors%20created%20a%20framework%20of%20real-time%20implementations.%20This%20consists%20of%20different%20methodical%20components%20based%20on%20single%20instruction%20multiple%20data%20(SIMD)%20techniques.%20Furthermore%2C%20the%20resulting%20algorithmic%20variations%20are%20compared%20with%20other%20publicly%20available%20algorithms.%20The%20authors%20argue%20that%20existing%20publicly%20available%20stereo%20data%20sets%20are%20not%20very%20suitable%20for%20the%20IV%20domain.%20Therefore%2C%20the%20authors'%20evaluation%20of%20stereo%20algorithms%20is%20based%20on%20novel%20realistically%20looking%20simulated%20data%20as%20well%20as%20real%20data%20from%20complex%20urban%20traffic%20scenes.%20In%20order%20to%20facilitate%20future%20benchmarks%2C%20all%20data%20used%20in%20this%20paper%20is%20made%20publicly%20available.%20The%20results%20from%20this%20study%20reveal%20that%20there%20is%20a%20considerable%20influence%20of%20scene%20conditions%20on%20the%20performance%20of%20all%20tested%20algorithms.%20Approaches%20that%20aim%20for%20(global)%20search%20optimization%20are%20more%20affected%20by%20this%20than%20other%20approaches.%20The%20best%20overall%20performance%20is%20achieved%20by%20the%20proposed%20multiple-window%20algorithm%2C%20which%20uses%20local%20matching%20and%20a%20left-right%20check%20for%20a%20robust%20error%20rejection.%20Timing%20results%20show%20that%20the%20simplest%20of%20the%20proposed%20SIMD%20variants%20are%20more%20than%20twice%20as%20fast%20than%20the%20most%20complex%20one.%20Nevertheless%2C%20the%20latter%20still%20achieves%20real-time%20processing%20speeds%2C%20while%20their%20average%20accuracy%20is%20at%20least%20equal%20to%20that%20of%20publicly%20available%20non-SIMD%20algorithms%22%2C%22container-title%22%3A%22IEEE%20Transactions%20on%20Intelligent%20Transportation%20Systems%22%2C%22DOI%22%3A%2210.1109%2FTITS.2006.869625%22%2C%22ISSN%22%3A%221558-0016%22%2C%22issue%22%3A%221%22%2C%22note%22%3A%22event-title%3A%20IEEE%20Transactions%20on%20Intelligent%20Transportation%20Systems%22%2C%22page%22%3A%2238-50%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Real-time%20dense%20stereo%20for%20intelligent%20vehicles%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F1603551%2F%3Farnumber%3D1603551%22%2C%22volume%22%3A%227%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Mark%22%2C%22given%22%3A%22W.%22%2C%22non-dropping-particle%22%3A%22van%20der%22%7D%2C%7B%22family%22%3A%22Gavrila%22%2C%22given%22%3A%22D.M.%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C17%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222006%22%2C3%5D%5D%7D%7D%7D%5D&quot; data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;Given geometric constraints two &lt;span class=&quot;highlight&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F56EYZP6B%22%2C%22pageLabel%22%3A%2242%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B239.91911688000027%2C365.74307202%2C298.7911090600003%2C374.50019741999995%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22locator%22%3A%2242%22%7D%7D&quot;&gt;“trifocal tensor”&lt;/span&gt; &lt;span class=&quot;citation&quot; data-citation=&quot;%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22locator%22%3A%2242%22%7D%5D%2C%22properties%22%3A%7B%7D%7D&quot;&gt;(&lt;span class=&quot;citation-item&quot;&gt;Comport et al., 2007, p. 42&lt;/span&gt;)&lt;/span&gt; can be computed from the two references images and left/right current images.&lt;/p&gt;
&lt;p&gt;In case of the two ref images and the left current images, the &lt;span class=&quot;highlight&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F56EYZP6B%22%2C%22pageLabel%22%3A%2242%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B365.40863176000005%2C618.2830315399998%2C557.9817047199998%2C627.0401569399999%5D%2C%5B313.19962646000005%2C606.5650214199998%2C548.2701622399991%2C615.3221468199998%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22locator%22%3A%2242%22%7D%7D&quot;&gt;“the warping from the left reference image to the left current image [is done] via a plane in the right reference image”&lt;/span&gt; &lt;span class=&quot;citation&quot; data-citation=&quot;%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FBJACI2PR%22%5D%2C%22locator%22%3A%2242%22%7D%5D%2C%22properties%22%3A%7B%7D%7D&quot;&gt;(&lt;span class=&quot;citation-item&quot;&gt;Comport et al., 2007, p. 42&lt;/span&gt;)&lt;/span&gt;. A mathematical relation describe (depending on the transformation between the ref and current) the warping in term of coordinates between the left ref and left curr images.&lt;/p&gt;
&lt;p&gt;Similar relation can be computed using the two ref images and the right images.&lt;/p&gt;
&lt;p&gt;These relations can be used on the pixels of 3d coordinates seen in both ref images (found using dense correspondence &lt;span class=&quot;citation&quot; data-citation=&quot;%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2FNGJ5PVX7%22%5D%7D%5D%2C%22properties%22%3A%7B%7D%7D&quot;&gt;(&lt;span class=&quot;citation-item&quot;&gt;Mark and Gavrila, 2006&lt;/span&gt;)&lt;/span&gt;) to compare luminescence (images are in grayscale) between ref and current images. This quantity needs to be minimized (describe in Section 4). &lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_13">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=4209067&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_12">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9739395/?arnumber=9739395">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)</dc:title>
                <dc:identifier>DOI 10.1109/ROBIO54168.2021.9739395</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yuqian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Xuetao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Yisha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhuang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_20"/>
        <link:link rdf:resource="#item_21"/>
        <link:link rdf:resource="#item_19"/>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Topological Map</dc:subject>
        <dc:title>2D Topological Map Building by UAVs for Ground Robot Navigation</dc:title>
        <dcterms:abstract>In air-ground cooperation, unmanned aerial vehicles (UAVs) are used to build a priori map of the ground environment from an aerial perspective, which is conducive to improve the navigation ability of ground robots. This paper proposes a 2D topology map building method from the air view, which can be effectively used for global path planning of ground mobile robots. To realize the 3D reconstruction of the ground from the air view, the 3D Euclidean Signed Distance Field (ESDF) is constructed incrementally from the Truncated Signed Distance Field (TSDF). The ESDF map occupies a large storage space, which is inconvenient to transmit to ground robots. To solve this problem, a lighter topology map is constructed on the basis of the ESDF map. Since ground robots can only move in the 2D plane, the 2D topological map is generated at any height from the ground to represent the topological structure of the ground robots working environment. The experimental results on the dataset and simulation environment shows the effectiveness of the proposed method.</dcterms:abstract>
        <dc:date>2021-12</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9739395/?arnumber=9739395</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:47:04</dcterms:dateSubmitted>
        <bib:pages>663-668</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_20">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;- Use of a 3D TSDF to stores the &quot;distance&quot; to the obstacle (is it a weight from 1 (far in front) to 0 (surface of object) to -1 (far behind))&lt;/p&gt;
&lt;p&gt;- Updating a cell to a new values (&quot;Weighting&quot;) is done by doing a weighted average of the previous weight and distance and new weight and distance.&lt;/p&gt;
&lt;p&gt;- Grouping points in sensor data (&quot;Merging&quot;) is done by grouped ray-casting&lt;/p&gt;
&lt;p&gt;- ESDF from TSDF using wave propagation&lt;/p&gt;
&lt;p&gt;- Then ESDF is used to generate the 2D topology map&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_21">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9739395&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:47:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_19">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9739395/?arnumber=9739395</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:47:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2501.18351">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Jianfeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>Hanlin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Jiahui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Shibo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Ke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Xuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>Xian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>You</foaf:surname>
                        <foaf:givenName>Xiong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_24"/>
        <link:link rdf:resource="#item_22"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Satellite imagery</dc:subject>
        <dc:title>Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments</dc:title>
        <dcterms:abstract>Path planning with strong environmental adaptability plays a crucial role in robotic navigation in unstructured outdoor environments, especially in the case of low-quality location and map information. The path planning ability of a robot depends on the identification of the traversability of global and local ground areas. In real-world scenarios, the complexity of outdoor open environments makes it difficult for robots to identify the traversability of ground areas that lack a clearly defined structure. Moreover, most existing methods have rarely analyzed the integration of local and global traversability identifications in unstructured outdoor scenarios. To address this problem, we propose a novel method, Dual-BEV Nav, first introducing Bird’s Eye View (BEV) representations into local planning to generate high-quality traversable paths. Then, these paths are projected onto the global traversability map generated by the global BEV planning model to obtain the optimal waypoints. By integrating the traversability from both local and global BEV, we establish a dual-layer BEV heuristic planning paradigm, enabling long-distance navigation in unstructured outdoor environments. We test our approach through both public dataset evaluations and real-world robot deployments, yielding promising results. Compared to baselines, the DualBEV Nav improved temporal distance prediction accuracy by up to 18.7%. In the real-world deployment, under conditions significantly different from the training set and with notable occlusions in the global BEV, the Dual-BEV Nav successfully achieved a 65-meter-long outdoor navigation. Further analysis demonstrates that the local BEV representation significantly enhances the rationality of the planning, while the global BEV probability map ensures the robustness of the overall planning.</dcterms:abstract>
        <dc:date>2025-01-30</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Dual-BEV Nav</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2501.18351</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:50:55</dcterms:dateSubmitted>
        <dc:description>arXiv:2501.18351 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2501.18351</dc:identifier>
        <prism:number>arXiv:2501.18351</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_24">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;LBPM Local BEV plannning model = local BEV perception encoder + task-driven goal decoder&lt;/p&gt;
&lt;h2&gt;LBPM&lt;/h2&gt;
&lt;h3&gt;Local BEV Perception encoder&lt;/h3&gt;
&lt;p&gt;inputs: (i) context observation $o_{t-P:t-1}$ (ii) current observation $o_t$.&lt;/p&gt;
&lt;p&gt;- BEV transformation on the observation&lt;/p&gt;
&lt;p&gt;- Feature extract based on LSS method. Uses EfficientNet&lt;/p&gt;
&lt;p&gt;- Uses LSS and BEVDet to predict discrete depth distribution for each pixel&lt;/p&gt;
&lt;p&gt;- BEV transformation (If multiple feature -&amp;gt; BEV pooling using BEVFusion)&lt;/p&gt;
&lt;p&gt;_I'm confused about what is new compared to LSS. At least until the computation of the&lt;/p&gt;
&lt;p&gt;BEV features (see Fig. 2), it is every similar. Also (this is one of the difference compared to LSS), I&lt;/p&gt;
&lt;p&gt;don't understand the point of the traversability features, is it to make the system&lt;/p&gt;
&lt;p&gt;keep in mind what was traversable?_&lt;/p&gt;
&lt;h3&gt;Task-driven goal decoder&lt;/h3&gt;
&lt;p&gt;based on the ViKiNG architecture&lt;/p&gt;
&lt;p&gt;inputs: environmental context $o_{t-P}$, current observation $o_t$, goal observation $o_\omega$.&lt;/p&gt;
&lt;h2&gt;GBPM&lt;/h2&gt;
&lt;p&gt;Provide traversability hint and overall direction.&lt;/p&gt;
&lt;p&gt;- Use an overhead map from satellite images&lt;/p&gt;
&lt;p&gt;- segmentation that gradually increase as they approach impassable.&lt;/p&gt;
&lt;p&gt;&amp;gt; GBPM uses trajectory data to learn traversability in the&lt;/p&gt;
&lt;p&gt;&amp;gt; overhead map [...], the more easily accessible areas will&lt;/p&gt;
&lt;p&gt;&amp;gt; be covered by a larger number of trajectories.&lt;/p&gt;
&lt;p&gt;- This step uses the probability predictions as a map (not a thresholds regulated&lt;/p&gt;
&lt;p&gt; &amp;nbsp;output). [U-net](https://github.com/milesial/Pytorch-UNet) is used to do the segmentation.&lt;/p&gt;
&lt;h3&gt;Use GBPM with potential trajectories generated by LBPM&lt;/h3&gt;
&lt;p&gt;&amp;gt; First, the LBPM generates multiple potential traversable paths, providing information including temporal distance, GPS&lt;/p&gt;
&lt;p&gt;&amp;gt; offsets, and waypoints from the current position to the goal. The GBPM encodes the overhead map into a global&lt;/p&gt;
&lt;p&gt;&amp;gt; probability map, projecting traversable paths of LBPM onto this map.&lt;/p&gt;
&lt;p&gt;&amp;gt;&lt;/p&gt;
&lt;p&gt;&amp;gt; $cost = k \times score + (1-k) \times temporal\_distance$&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_22">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2501.18351</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:50:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2004.04697">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manderson</foaf:surname>
                        <foaf:givenName>Travis</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wapnick</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meger</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dudek</foaf:surname>
                        <foaf:givenName>Gregory</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_27"/>
        <dcterms:isReferencedBy rdf:resource="#item_28"/>
        <link:link rdf:resource="#item_25"/>
        <dc:relation rdf:resource="https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1"/>
        <dc:relation rdf:resource="#item_173"/>
        <dc:relation rdf:resource="https://www.mdpi.com/1424-8220/13/1/1247"/>
        <dc:subject>Reinforcement learning</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Local path</dc:subject>
        <dc:subject>Collaboration</dc:subject>
        <dc:title>Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images</dc:title>
        <dcterms:abstract>We present a method for learning to drive on smooth terrain while simultaneously avoiding collisions in challenging off-road and unstructured outdoor environments using only visual inputs. Our approach applies a hybrid model-based and model-free reinforcement learning method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors. Notably, we provide both ﬁrst-person and overhead aerial image inputs to our model. We ﬁnd that the fusion of these complementary inputs improves planning foresight and makes the model robust to visual obstructions. Our results show the ability to generalize to environments with plentiful vegetation, various types of rock, and sandy trails. During evaluation, our policy attained 90% smooth terrain traversal and reduced the proportion of rough terrain driven over by 6.1 times compared to a model using only ﬁrstperson imagery. Video and project details can be found at www.cim.mcgill.ca/mrl/offroad driving/.</dcterms:abstract>
        <dc:date>2020-04-09</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2004.04697</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:56:22</dcterms:dateSubmitted>
        <dc:description>arXiv:2004.04697 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2004.04697</dc:identifier>
        <prism:number>arXiv:2004.04697</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_27">
        <rdf:value>Comment: ICRA 2020. Video and project details can be found at http://www.cim.mcgill.ca/mrl/offroad_driving/</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_28">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this paper, we present a system for learning a navigation policy that **preferentially chooses smooth terrain** [...]. The emphasis of the paper, however, is not road classification perse, but rather to propose an approach for **online adaptive self-supervised learning** for off-road driving in rough terrain and to explore the synthesis of aerial and first-person (ground) sensing in this context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
From BEV, FPS images, use CNN to predict the rougher terrain for $H$ steps (supplying an action for every step)
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;use of Value Prediction Networks, a hybrid model-based and model-free reinforcement learning architecture.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
It is model-based as it implicitly learns a dynamics model for abstract states optimized for predicting future rewards and value functions.
&lt;/li&gt;
&lt;li&gt;
It is also model-free as it maps these encoded abstract states to rewards and value functions using direct experience with the environment prior to the planning phase.
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
In training terrain roughness is estimated using IMU and obstacles using short-range LiDAR.
&lt;/li&gt;
&lt;li&gt;
Reward based on the difference between the prediction and the actual roughness
&lt;/li&gt;
&lt;li&gt;
It is classification as type of terrain is associated with different number and the model tries to predict the correct one
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_25">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2004.04697</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:56:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2409.18253">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fortin</foaf:surname>
                        <foaf:givenName>Jean-Michel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gamache</foaf:surname>
                        <foaf:givenName>Olivier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fecteau</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Daum</foaf:surname>
                        <foaf:givenName>Effie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Larrivée-Hardy</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pomerleau</foaf:surname>
                        <foaf:givenName>François</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giguère</foaf:surname>
                        <foaf:givenName>Philippe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_30"/>
        <dcterms:isReferencedBy rdf:resource="#item_41"/>
        <link:link rdf:resource="#item_31"/>
        <link:link rdf:resource="#item_32"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Dataset</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>ResNet18</dc:subject>
        <dc:title>UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation</dc:title>
        <dcterms:abstract>Terrain awareness is an essential milestone to enable truly autonomous off-road navigation. Accurately predicting terrain characteristics allows optimizing a vehicle's path against potential hazards. Recent methods use deep neural networks to predict traversability-related terrain properties in a self-supervised manner, relying on proprioception as a training signal. However, onboard cameras are inherently limited by their point-of-view relative to the ground, suffering from occlusions and vanishing pixel density with distance. This paper introduces a novel approach for self-supervised terrain characterization using an aerial perspective from a hovering drone. We capture terrain-aligned images while sampling the environment with a ground vehicle, effectively training a simple predictor for vibrations, bumpiness, and energy consumption. Our dataset includes 2.8 km of off-road data collected in forest environment, comprising 13 484 ground-based images and 12 935 aerial images. Our findings show that drone imagery improves terrain property prediction by 21.37 % on the whole dataset and 37.35 % in high vegetation, compared to ground robot images. We conduct ablation studies to identify the offline causes of these performance improvements. We also demonstrate the real-world applicability of our approach by scouting an unseen area with a drone, planning and executing an optimized path on the ground.</dcterms:abstract>
        <dc:date>2024-09-26</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2409.18253</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 16:02:01</dcterms:dateSubmitted>
        <dc:description>arXiv:2409.18253 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2409.18253</dc:identifier>
        <prism:number>arXiv:2409.18253</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_30">
       <rdf:value>Comment: 7 pages, 5 figures, submitted to ICRA 2025</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_41">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
good documentation of related work
&lt;/li&gt;
&lt;li&gt;
datasets created could be very useful
&lt;/li&gt;
&lt;li&gt;
makes me think of the work done by Tom.
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;use of ResNet18 and a &quot;homemade&quot; MLP to predict $M_z$, $M_\omega$ and $M_p$&lt;/p&gt;
&lt;p&gt;respectively vibration metric, bumpiness and electrical energy consumption.&lt;/p&gt;
&lt;p&gt;BEV from UAV gives better prediction than FPS from UGV.&lt;/p&gt;
&lt;p&gt;Using the BEV from UAV, can obtain maps (similar to occupancy grid) (see Fig5)&lt;/p&gt;
&lt;p&gt;than can be used to choose &quot;better&quot; path.&lt;/p&gt;
&lt;p&gt;-&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_31">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2409.18253v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 16:02:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_32">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2409.18253</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 16:02:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2404.18750">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mortimer</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maehlisch</foaf:surname>
                        <foaf:givenName>Mirko</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_37"/>
        <dcterms:isReferencedBy rdf:resource="#item_107"/>
        <link:link rdf:resource="#item_38"/>
        <link:link rdf:resource="#item_39"/>
        <dc:subject>Dataset</dc:subject>
        <dc:title>Survey on Datasets for Perception in Unstructured Outdoor Environments</dc:title>
        <dcterms:abstract>Perception is an essential component of pipelines in field robotics. In this survey, we quantitatively compare publicly available datasets available in unstructured outdoor environments. We focus on datasets for common perception tasks in field robotics. Our survey categorizes and compares available research datasets. This survey also reports on relevant dataset characteristics to help practitioners determine which dataset fits best for their own application. We believe more consideration should be taken in choosing compatible annotation policies across the datasets in unstructured outdoor environments.</dcterms:abstract>
        <dc:date>2024-04-29</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2404.18750</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 08:41:36</dcterms:dateSubmitted>
        <dc:description>arXiv:2404.18750 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2404.18750</dc:identifier>
        <prism:number>arXiv:2404.18750</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_37">
        <rdf:value>Comment: Accepted to the IEEE ICRA Workshop on Field Robotics 2024</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_107">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;Minimize using M-estimators the sum of a given function of the error (the difference between the the two feature vectors)&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_38">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2404.18750v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 08:41:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_39">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2404.18750</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 08:41:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2008.4651086</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rufus Blas</foaf:surname>
                        <foaf:givenName>Morten</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agrawal</foaf:surname>
                        <foaf:givenName>Motilal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sundaresan</foaf:surname>
                        <foaf:givenName>Aravind</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Konolige</foaf:surname>
                        <foaf:givenName>Kurt</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_47"/>
        <link:link rdf:resource="#item_46"/>
        <link:link rdf:resource="#item_45"/>
        <dc:relation rdf:resource="urn:isbn:978-1-4244-3803-7"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Trail classification</dc:subject>
        <dc:title>Fast color/texture segmentation for outdoor robots</dc:title>
        <dcterms:abstract>We present a fast integrated approach for online segmentation of images for outdoor robots. A compact color and texture descriptor has been developed to describe local color and texture variations in an image. This descriptor is then used in a two-stage fast clustering framework using K-means to perform online segmentation of natural images. We present results of applying our descriptor for segmenting a synthetic image and compare it against other state-of-the-art descriptors. We also apply our segmentation algorithm to the task of detecting natural paths in outdoor images. The whole system has been demonstrated to work online alongside localization, 3D obstacle detection, and planning.</dcterms:abstract>
        <dc:date>2008-09</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 09:59:36</dcterms:dateSubmitted>
        <dc:description>ISSN: 2153-0866</dc:description>
        <bib:pages>4078-4085</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_47">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
create a vector of LAB, and texture around the pixel
&lt;/li&gt;
&lt;li&gt;
k-means to find 16 centroids “textons”
&lt;/li&gt;
&lt;li&gt;
for each pixels do a histogram of the classification around the pixel
&lt;/li&gt;
&lt;li&gt;
k-means on the histogram
&lt;/li&gt;
&lt;li&gt;
EMD (using the distance between the textons) to merge clusters that are “too” close
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Seb-sti1/mastersthesis/blob/master/scripts/fast_colortexture_seg_outdoor_robots.py&quot; rel=&quot;noopener noreferrer nofollow&quot;&gt;https://github.com/Seb-sti1/mastersthesis/blob/master/scripts/fast_colortexture_seg_outdoor_robots.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/tbjszhu/&quot; rel=&quot;noopener noreferrer nofollow&quot;&gt;https://github.com/tbjszhu/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_46">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=4651086&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 09:59:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_45">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 09:59:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4244-3803-7">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4244-3803-7</dc:identifier>
                <dc:title>2009 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2009.5354059</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>St. Louis, MO, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rasmussen</foaf:surname>
                        <foaf:givenName>Christopher</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kocamaz</foaf:surname>
                        <foaf:givenName>Mehmet</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_132"/>
        <link:link rdf:resource="#item_48"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Trail classification</dc:subject>
        <dc:subject>Local path</dc:subject>
        <dc:subject>Lidar</dc:subject>
        <dc:title>Appearance contrast for fast, robust trail-following</dc:title>
        <dcterms:abstract>We describe a framework for ﬁnding and tracking “trails” for autonomous outdoor robot navigation. Through a combination of visual cues and ladar-derived structural information, the algorithm is able to follow paths which pass through multiple zones of terrain smoothness, border vegetation, tread material, and illumination conditions. Our shape-based visual trail tracker assumes that the approaching trail region is approximately triangular under perspective. It generates region hypotheses from a learned distribution of expected trail width and curvature variation, and scores them using a robust measure of color and brightness contrast with ﬂanking regions. The structural component analogously rewards hypotheses which correspond to empty or low-density regions in a groundstrike-ﬁltered ladar obstacle map. Our system’s performance is analyzed on several long sequences with diverse appearance and structural characteristics. Ground-truth segmentations are used to quantify performance where available, and several alternative algorithms are compared on the same data.</dcterms:abstract>
        <dc:date>10/2009</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/5354059/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 10:18:03</dcterms:dateSubmitted>
        <bib:pages>3505-3512</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2009)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_132">
        <rdf:value>&lt;div data-citation-items=&quot;%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F2IXKKLES%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F2IXKKLES%22%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22We%20present%20a%20fast%20integrated%20approach%20for%20online%20segmentation%20of%20images%20for%20outdoor%20robots.%20A%20compact%20color%20and%20texture%20descriptor%20has%20been%20developed%20to%20describe%20local%20color%20and%20texture%20variations%20in%20an%20image.%20This%20descriptor%20is%20then%20used%20in%20a%20two-stage%20fast%20clustering%20framework%20using%20K-means%20to%20perform%20online%20segmentation%20of%20natural%20images.%20We%20present%20results%20of%20applying%20our%20descriptor%20for%20segmenting%20a%20synthetic%20image%20and%20compare%20it%20against%20other%20state-of-the-art%20descriptors.%20We%20also%20apply%20our%20segmentation%20algorithm%20to%20the%20task%20of%20detecting%20natural%20paths%20in%20outdoor%20images.%20The%20whole%20system%20has%20been%20demonstrated%20to%20work%20online%20alongside%20localization%2C%203D%20obstacle%20detection%2C%20and%20planning.%22%2C%22container-title%22%3A%222008%20IEEE%2FRSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%22%2C%22DOI%22%3A%2210.1109%2FIROS.2008.4651086%22%2C%22event-title%22%3A%222008%20IEEE%2FRSJ%20International%20Conference%20on%20Intelligent%20Robots%20and%20Systems%22%2C%22note%22%3A%22ISSN%3A%202153-0866%22%2C%22page%22%3A%224078-4085%22%2C%22source%22%3A%22IEEE%20Xplore%22%2C%22title%22%3A%22Fast%20color%2Ftexture%20segmentation%20for%20outdoor%20robots%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F4651086%2F%3Farnumber%3D4651086%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Rufus%20Blas%22%2C%22given%22%3A%22Morten%22%7D%2C%7B%22family%22%3A%22Agrawal%22%2C%22given%22%3A%22Motilal%22%7D%2C%7B%22family%22%3A%22Sundaresan%22%2C%22given%22%3A%22Aravind%22%7D%2C%7B%22family%22%3A%22Konolige%22%2C%22given%22%3A%22Kurt%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222025%22%2C2%2C17%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222008%22%2C9%5D%5D%7D%7D%7D%5D&quot; data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
CIE-Lab used
&lt;/li&gt;
&lt;li&gt;
textons generated from the input image using &lt;span class=&quot;citation&quot; data-citation=&quot;%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FdndPD7NB%2Fitems%2F2IXKKLES%22%5D%7D%5D%2C%22properties%22%3A%7B%7D%7D&quot;&gt;(&lt;span class=&quot;citation-item&quot;&gt;Rufus Blas et al., 2008&lt;/span&gt;)&lt;/span&gt; method
&lt;/li&gt;
&lt;li&gt;
Then triangles are scored using a likelihood function
&lt;/li&gt;
&lt;li&gt;
Add lidar info to enhance results
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_48">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_IROS_2009/papers/1405.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 10:17:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/988771/?arnumber=988771">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)</dc:title>
                <dc:identifier>DOI 10.1109/SMBV.2001.988771</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scharstein</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szeliski</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zabih</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_67"/>
        <link:link rdf:resource="#item_66"/>
        <dc:subject>To read</dc:subject>
        <dc:title>A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</dc:title>
        <dcterms:abstract>Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can be easily extended to include new algorithms. We have also produced several new multiframe stereo data sets with ground truth, and are making both the code and data sets available on the Web.</dcterms:abstract>
        <dc:date>2001-12</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/988771/?arnumber=988771</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 13:33:33</dcterms:dateSubmitted>
        <bib:pages>131-140</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_67">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=988771&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 13:33:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_66">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/988771/?arnumber=988771</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 13:33:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1558-0016"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van der Mark</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gavrila</foaf:surname>
                        <foaf:givenName>D.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_95"/>
        <link:link rdf:resource="#item_94"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067"/>
        <dc:subject>To read</dc:subject>
        <dc:title>Real-time dense stereo for intelligent vehicles</dc:title>
        <dcterms:abstract>Stereo vision is an attractive passive sensing technique for obtaining three-dimensional (3-D) measurements. Recent hardware advances have given rise to a new class of real-time dense disparity estimation algorithms. This paper examines their suitability for intelligent vehicle (IV) applications. In order to gain a better understanding of the performance and the computational-cost tradeoff, the authors created a framework of real-time implementations. This consists of different methodical components based on single instruction multiple data (SIMD) techniques. Furthermore, the resulting algorithmic variations are compared with other publicly available algorithms. The authors argue that existing publicly available stereo data sets are not very suitable for the IV domain. Therefore, the authors' evaluation of stereo algorithms is based on novel realistically looking simulated data as well as real data from complex urban traffic scenes. In order to facilitate future benchmarks, all data used in this paper is made publicly available. The results from this study reveal that there is a considerable influence of scene conditions on the performance of all tested algorithms. Approaches that aim for (global) search optimization are more affected by this than other approaches. The best overall performance is achieved by the proposed multiple-window algorithm, which uses local matching and a left-right check for a robust error rejection. Timing results show that the simplest of the proposed SIMD variants are more than twice as fast than the most complex one. Nevertheless, the latter still achieves real-time processing speeds, while their average accuracy is at least equal to that of publicly available non-SIMD algorithms</dcterms:abstract>
        <dc:date>2006-03</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 16:14:39</dcterms:dateSubmitted>
        <dc:description>Conference Name: IEEE Transactions on Intelligent Transportation Systems</dc:description>
        <bib:pages>38-50</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1558-0016">
        <prism:volume>7</prism:volume>
        <dc:title>IEEE Transactions on Intelligent Transportation Systems</dc:title>
        <dc:identifier>DOI 10.1109/TITS.2006.869625</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1558-0016</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_95">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=1603551&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 16:14:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_94">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 16:14:48</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="http://arxiv.org/abs/1806.02487">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
           <bib:Book><prism:volume>11</prism:volume></bib:Book>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Luqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Daqian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>Fei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Fengyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Jixin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Mengxiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Shaojie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_112"/>
        <link:link rdf:resource="#item_110"/>
        <dc:subject>Exploration</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Collaboration</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>A Collaborative Aerial-Ground Robotic System for Fast Exploration</dc:title>
        <dcterms:abstract>Autonomous exploration of unknown environments has been widely applied in inspection, surveillance, and search and rescue. In exploration task, the basic requirement for robots is to detect the unknown space as fast as possible. In this paper, we propose an autonomous collaborative system consists of an aerial robot and a ground vehicle to explore in unknown environments. We combine the frontier based method and the harmonic field to generate a path. Then, For the ground robot, a minimum jerk piecewise Bezier curve which can guarantee safety and dynamical feasibility is generated amid obstacles. For the aerial robot, a motion primitive method is adopted for local path planning. We implement the proposed framework on an autonomous collaborative aerial-ground system. Extensive field experiments as well as simulations are presented to validate the method and demonstrate its higher efficiency against each single vehicle.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1806.02487</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 10:34:14</dcterms:dateSubmitted>
        <dc:description>DOI: 10.1007/978-3-030-33950-0_6
arXiv:1806.02487 [cs]</dc:description>
        <bib:pages>59-71</bib:pages>
    </bib:BookSection>
    <bib:Memo rdf:about="#item_112">
       <rdf:value>Comment: 12 pages, 16 figures</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_110">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://typeset.io/pdf/a-collaborative-aerial-ground-robotic-system-for-fast-4l6aejojb1.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 10:34:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/6224988/?arnumber=6224988">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2012 IEEE International Conference on Robotics and Automation</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2012.6224988</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khan</foaf:surname>
                        <foaf:givenName>Yasir Niaz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Masselli</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zell</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_119"/>
        <link:link rdf:resource="#item_117"/>
        <link:link rdf:resource="#item_115"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>TSURF</dc:subject>
        <dc:subject>LBP</dc:subject>
        <dc:subject>LTP</dc:subject>
        <dc:title>Visual terrain classification by flying robots</dc:title>
        <dcterms:abstract>In this paper we investigate the effectiveness of SURF features for visual terrain classification for outdoor flying robots. A quadrocopter fitted with a single camera is flown over different terrains to take images of the ground below. Each image is divided into a grid and SURF features are calculated at grid intersections. A classifier is then used to learn to differentiate between different terrain types. Classification results of the SURF descriptor are compared with results from other texture descriptors like Local Binary Patterns and Local Ternary Patterns. Six different terrain types are considered in this approach. Random forests are used for classification on each descriptor. It is shown that SURF features perform better than other descriptors at higher resolutions.</dcterms:abstract>
        <dc:date>2012-05</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/6224988/?arnumber=6224988</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:17:57</dcterms:dateSubmitted>
        <dc:description>ISSN: 1050-4729</dc:description>
        <bib:pages>498-503</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2012 IEEE International Conference on Robotics and Automation</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_119">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;Comparison of SURF descriptor, LBP, LTP for visual outdoor terrain classification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
Random forest gives the best results
&lt;/li&gt;
&lt;li&gt;
TSURF good with small grid size and lower training time than LBP, LTP
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_117">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=6224988&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_115">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/6224988/?arnumber=6224988</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2006.281686</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Silver</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sofman</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vandapel</foaf:surname>
                        <foaf:givenName>Nicolas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bagnell</foaf:surname>
                        <foaf:givenName>J. Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stentz</foaf:surname>
                        <foaf:givenName>Anthony</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_122"/>
        <link:link rdf:resource="#item_118"/>
        <link:link rdf:resource="#item_116"/>
        <dc:relation rdf:resource="https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Satellite imagery</dc:subject>
        <dc:subject>Lidar</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>Experimental Analysis of Overhead Data Processing To Support Long Range Navigation</dc:title>
        <dcterms:abstract>Long range navigation by unmanned ground vehicles continues to challenge the robotics community. Efficient navigation requires not only intelligent on-board perception and planning systems, but also the effective use of prior knowledge of the vehicle's environment. This paper describes a system for supporting unmanned ground vehicle navigation through the use of heterogeneous overhead data. Semantic information is obtained through supervised classification, and vehicle mobility is predicted from available geometric data. This approach is demonstrated and validated through over 50 kilometers of autonomous traversal through complex natural environments</dcterms:abstract>
        <dc:date>2006-10</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:13</dcterms:dateSubmitted>
        <dc:description>ISSN: 2153-0866</dc:description>
        <bib:pages>2443-2450</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_122">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;p&gt;usage of LiDAR and imagery on (only) terrestrial robot and prior data (from variety of sources) to achieve robust navigation.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_118">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=4058754&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_116">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>DOI 10.1184/R1/6561173.v1</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sofman</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bagnell</foaf:surname>
                        <foaf:givenName>J. Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stentz</foaf:surname>
                        <foaf:givenName>Anthony</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vandapel</foaf:surname>
                        <foaf:givenName>Nicolas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_121"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754"/>
        <dc:relation rdf:resource="http://arxiv.org/abs/2004.04697"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>Satellite imagery</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>Terrain Classification from Aerial Data to Support Ground Vehicle Navigation</dc:title>
        <dcterms:abstract>Sensory perception for unmanned ground vehicle navigation has received great attention from the robotics community. However, sensors mounted on the vehicle are regularly viewpoint impaired. A vehicle navigating at high speeds in off- road environments may be unable to react to negative obstacles such as large holes and cliffs. One approach to address this problem is to complement the sensing capabilities of an unmanned ground vehicle with overhead data gathered from an aerial source. This paper presents techniques to achieve accurate terrain classiﬁcation by utilizing high-density, colorized, three- dimensional laser data. We describe methods to extract relevant features from this sensor data in such a way that a learning algorithm can successfully train on a small set of labeled data in order to classify a much larger map and show experimental results. Additionally, we introduce a technique to signiﬁcantly reduce classiﬁcation errors through the use of context. Finally, we show how this algorithm can be customized for the intended vehicle’s capabilities in order to create more accurate a priori maps that can then be used for path planning.</dcterms:abstract>
        <dc:date>2006/01/01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>kilthub.cmu.edu</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:20:58</dcterms:dateSubmitted>
        <dc:description>Publisher: Carnegie Mellon University</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_121">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1/files/12043478.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:20:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-0-7695-2372-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 978-0-7695-2372-9</dc:identifier>
                <dc:identifier>DOI 10.1109/CVPR.2005.52</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Guo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mao</foaf:surname>
                        <foaf:givenName>Zhihong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_124"/>
        <link:link rdf:resource="#item_125"/>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Man-made segmentation</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>A two-stage level set evolution scheme for man-made objects detection in aerial images</dc:title>
        <dcterms:abstract>A novel two-stage level set evolution method for detecting man-made objects in aerial images is described. The method is based on a modified Mumford-Shah model and it uses a two-stage curve evolution strategy to get a preferable detection. It applies fractal error metric, developed by Cooper, et al. (1994) at the first curve evolution stage and adds additional constraint texture edge descriptor that is defined by using DCT (discrete cosine transform) coefficients on the image at the next stage. Man-made objects and natural areas are optimally differentiated by evolving the partial differential equation. The method artfully avoids selecting a threshold to separate the fractal error image, while an improper threshold often results in great segmentation errors. Experiments of the segmentation show that the proposed method is efficient.</dcterms:abstract>
        <dc:date>2005-07-20</dc:date>
        <z:libraryCatalog>ResearchGate</z:libraryCatalog>
        <bib:pages>474-479 vol. 1</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_124">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Xin-Yang-56/publication/4156206_A_two-stage_level_set_evolution_scheme_for_man-made_objects_detection_in_aerial_images/links/54bc76c20cf29e0cb04bf99b/A-two-stage-level-set-evolution-scheme-for-man-made-objects-detection-in-aerial-images.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:27:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_125">
        <z:itemType>attachment</z:itemType>
        <dc:title>ResearchGate Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/publication/4156206_A_two-stage_level_set_evolution_scheme_for_man-made_objects_detection_in_aerial_images</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:27:18</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/7358076">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:title>IEEE Robotics and Automation Letters</dc:title>
                <dc:identifier>DOI 10.1109/LRA.2015.2509024</dc:identifier>
                <prism:number>2</prism:number>
                <dc:identifier>ISSN 2377-3766</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giusti</foaf:surname>
                        <foaf:givenName>Alessandro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guzzi</foaf:surname>
                        <foaf:givenName>Jérôme</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cireşan</foaf:surname>
                        <foaf:givenName>Dan C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Fang-Lin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rodríguez</foaf:surname>
                        <foaf:givenName>Juan P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fontana</foaf:surname>
                        <foaf:givenName>Flavio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Faessler</foaf:surname>
                        <foaf:givenName>Matthias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Forster</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schmidhuber</foaf:surname>
                        <foaf:givenName>Jürgen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Caro</foaf:surname>
                        <foaf:givenName>Gianni Di</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gambardella</foaf:surname>
                        <foaf:givenName>Luca M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_131"/>
        <link:link rdf:resource="#item_129"/>
        <link:link rdf:resource="#item_130"/>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Dataset</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Trail classification</dc:subject>
        <dc:title>A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots</dc:title>
        <dcterms:abstract>We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the offline direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.</dcterms:abstract>
        <dc:date>2016-07</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/7358076</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:29:32</dcterms:dateSubmitted>
        <dc:description>Conference Name: IEEE Robotics and Automation Letters</dc:description>
        <bib:pages>661-667</bib:pages>
    </bib:Article>
    <bib:Memo rdf:about="#item_131">
        <rdf:value>&lt;div data-schema-version=&quot;9&quot;&gt;&lt;h1&gt;My notes&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
Clever technic to generate dataset: human with 3 cameras oriented left, center, right walks on a forest trails making&amp;nbsp;sure to face the direction of the path. It generates, respectively, images with trails on the right side, center and left side
&lt;/li&gt;
&lt;li&gt;
The model predict if the trail is on the right side, center or left side
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;15+ GB of training testing dataset available (images left, center and right of forest trails)&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_129">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=7358076&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:29:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_130">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/7358076</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:29:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21423">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1556-4967"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Santana</foaf:surname>
                        <foaf:givenName>Pedro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Correia</foaf:surname>
                        <foaf:givenName>Luís</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mendonça</foaf:surname>
                        <foaf:givenName>Ricardo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alves</foaf:surname>
                        <foaf:givenName>Nelson</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barata</foaf:surname>
                        <foaf:givenName>José</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_134"/>
        <link:link rdf:resource="#item_135"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Trail classification</dc:subject>
        <dc:subject>Saliency</dc:subject>
        <dc:subject>Not read</dc:subject>
        <dc:title>Tracking natural trails with swarm-based visual saliency</dc:title>
        <dcterms:abstract>This paper proposes a model for trail detection and tracking that builds upon the observation that trails are salient structures in the robot's visual field. Due to the complexity of natural environments, the straightforward application of bottom-up visual saliency models is not sufficiently robust to predict the location of trails. As for other detection tasks, robustness can be increased by modulating the saliency computation based on a priori knowledge about which pixel-wise visual features are most representative of the object being sought. This paper proposes the use of the object's overall layout as the primary cue instead, as it is more stable and predictable in natural trails. Bearing in mind computational parsimony and detection robustness, this knowledge is specified in terms of perception-action rules, which control the behavior of simple agents performing as a swarm to compute the saliency map of the input image. For the purpose of tracking, multiframe evidence about the trail location is obtained with a motion-compensated dynamic neural field. In addition, to reduce ambiguity between the trail and trail-like distractors, a simple appearance model is learned online and used to influence the agents' activity. Experimental results on a large data set reveal the ability of the model to produce a success rate on the order of 97% at 20 Hz. The model is shown to be robust in situations where previous models would fail, such as when the trail does not emerge from the lower part of the image or when it is considerably interrupted. © 2012 Wiley Periodicals, Inc.</dcterms:abstract>
        <dc:date>2013</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Wiley Online Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21423</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:38:29</dcterms:dateSubmitted>
        <dc:rights>© 2012 Wiley Periodicals, Inc.</dc:rights>
        <dc:description>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21423</dc:description>
        <bib:pages>64-86</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1556-4967">
        <prism:volume>30</prism:volume>
        <dc:title>Journal of Field Robotics</dc:title>
        <dc:identifier>DOI 10.1002/rob.21423</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1556-4967</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_134">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rob.21423</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:38:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_135">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/10.1002/rob.21423</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:38:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-6654-7927-1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-6654-7927-1</dc:identifier>
                <dc:title>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</dc:title>
                <dc:identifier>DOI 10.1109/IROS47612.2022.9981258</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Kyoto, Japan</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ross</foaf:surname>
                        <foaf:givenName>James</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mendez</foaf:surname>
                        <foaf:givenName>Oscar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saha</foaf:surname>
                        <foaf:givenName>Avishkar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Johnson</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bowden</foaf:surname>
                        <foaf:givenName>Richard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_136"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Not read</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:subject>BEV</dc:subject>
        <dc:subject>SLAM</dc:subject>
        <dc:title>BEV-SLAM: Building a Globally-Consistent World Map Using Monocular Vision</dc:title>
        <dcterms:abstract>The ability to produce large-scale maps for navigation, path planning and other tasks is a crucial step for autonomous agents, but has always been challenging. In this work, we introduce BEV-SLAM, a novel type of graph-based SLAM that aligns semantically-segmented Bird’s Eye View (BEV) predictions from monocular cameras. We introduce a novel form of occlusion reasoning into BEV estimation and demonstrate its importance to aid spatial aggregation of BEV predictions. The result is a versatile SLAM system that can operate across arbitrary multi-camera configurations and can be seamlessly integrated with other sensors. We show that the use of multiple cameras significantly increases performance, and achieves lower relative error than high-performance GPS.</dcterms:abstract>
        <dc:date>2022-10-23</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>BEV-SLAM</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9981258/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:40:30</dcterms:dateSubmitted>
        <dc:rights>https://doi.org/10.15223/policy-029</dc:rights>
        <bib:pages>3830-3836</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_136">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://personalpages.surrey.ac.uk/r.bowden/publications/2022/Ross_IROS2022pp.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:40:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1007/s40747-022-00831-5">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2198-6053"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shang</foaf:surname>
                        <foaf:givenName>Zhexiong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Zhigang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_139"/>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Not read</dc:subject>
        <dc:title>Topology-based UAV path planning for multi-view stereo 3D reconstruction of complex structures</dc:title>
        <dcterms:abstract>This paper introduces a new UAV path planning method for creating high-quality 3D reconstruction models of large and complex structures. The core of the new method is incorporating the topology information of the surveyed 3D structure to decompose the multi-view stereo path planning into a collection of overlapped view optimization problems that can be processed in parallel. Different from the existing state-of-the-arts that recursively select the vantage camera views, the new method iteratively resamples all nearby cameras (i.e., positions/orientations) together and achieves a substantial reduction in computation cost while improving reconstruction quality. The new approach also provides a higher-level automation function that facilitates field implementations by eliminating the need for redundant camera initialization as in existing studies. Validations are provided by measuring the variance between the reconstructions to the ground truth models. Results from three synthetic case studies and one real-world application are presented to demonstrate the improved performance. The new method is expected to be instrumental in expanding the adoption of UAV-based multi-view stereo 3D reconstruction of large and complex structures.</dcterms:abstract>
        <dc:date>2023-02-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/s40747-022-00831-5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:41:23</dcterms:dateSubmitted>
        <bib:pages>909-926</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2198-6053">
        <prism:volume>9</prism:volume>
        <dc:title>Complex &amp; Intelligent Systems</dc:title>
        <dc:identifier>DOI 10.1007/s40747-022-00831-5</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Complex Intell. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 2198-6053</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_139">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs40747-022-00831-5.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:41:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1007/s10514-017-9638-9">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1573-7527"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Huei-Yung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yao</foaf:surname>
                        <foaf:givenName>Chia-Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Kai-Sheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tran</foaf:surname>
                        <foaf:givenName>Van Luan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_141"/>
        <dc:subject>To read</dc:subject>
        <dc:title>Topological map construction and scene recognition for vehicle localization</dc:title>
        <dcterms:abstract>This paper presents a vehicle localization method to assist vehicle navigation based on topological map construction and scene recognition. A topological map is constructed using omni-directional image sequences, and the node information of the topological map is used for place recognition and derivation of vehicle location. In topological map construction and scene change detection, we utilize the Extended-HCT method for semantic description and feature extraction. Content-based and feature-based image retrieval approaches are adopted for place recognition and vehicle localization on the real scene image dataset. The proposed technique is able to construct a real-time image retrieval system for navigation assistance and validate the correctness of the route. Experiments are carried out in both the indoor and outdoor environments using real world images.</dcterms:abstract>
        <dc:date>2018-01-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/s10514-017-9638-9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:25:28</dcterms:dateSubmitted>
        <bib:pages>65-81</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1573-7527">
        <prism:volume>42</prism:volume>
        <dc:title>Autonomous Robots</dc:title>
        <dc:identifier>DOI 10.1007/s10514-017-9638-9</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Auton Robot</dcterms:alternative>
        <dc:identifier>ISSN 1573-7527</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_141">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs10514-017-9638-9.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:25:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/10550628/?arnumber=10550628">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2024 International Conference on 3D Vision (3DV)</dc:title>
                <dc:identifier>DOI 10.1109/3DV62453.2024.00017</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sautier</foaf:surname>
                        <foaf:givenName>Corentin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Puy</foaf:surname>
                        <foaf:givenName>Gilles</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Boulch</foaf:surname>
                        <foaf:givenName>Alexandre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marlet</foaf:surname>
                        <foaf:givenName>Renaud</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lepetit</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_144"/>
        <link:link rdf:resource="#item_143"/>
        <dc:subject>Not read</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds</dc:title>
        <dcterms:abstract>We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast [40], which uses a contrast at the level of points, to the state-of-the-art TARL [30], which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of implementation, it is surpassed by the latter, which however requires a costly pre-processing. In BEVContrast, we define our contrast at the level of 2D cells in the Bird’s Eye View plane. Resulting cell-level representations offer a good trade-off between the point-level representations exploited in PointContrast and segment-level representations exploited in TARL: we retain the simplicity of PointContrast (cell representations are cheap to compute) while surpassing the performance of TARL in downstream semantic segmentation. The code is available at github.com/valeoai/BEVContrast</dcterms:abstract>
        <dc:date>2024-03</dc:date>
        <z:shortTitle>BEVContrast</z:shortTitle>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10550628/?arnumber=10550628</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:27:02</dcterms:dateSubmitted>
        <dc:description>ISSN: 2475-7888</dc:description>
        <bib:pages>559-568</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2024 International Conference on 3D Vision (3DV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_144">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10550628&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:27:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_143">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10550628/?arnumber=10550628</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:27:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_149">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>60</prism:volume>
                <dc:title>Robotics and Autonomous Systems</dc:title>
                <dc:identifier>DOI 10.1016/j.robot.2012.09.004</dc:identifier>
                <dcterms:alternative>Robotics and Autonomous Systems</dcterms:alternative>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scherer</foaf:surname>
                        <foaf:givenName>Sebastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chamberlain</foaf:surname>
                        <foaf:givenName>Lyle</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>Sanjiv</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_150"/>
        <link:link rdf:resource="#item_151"/>
        <dc:subject>Not read</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>Autonomous Landing at Unprepared Sites by a Full-Scale Helicopter</dc:title>
        <dcterms:abstract>Helicopters are valuable since they can land at unprepared sites; however, current unmanned helicopters are unable to select or validate landing zones (LZs) and approach paths. For operation in unknown terrain it is necessary to assess the safety of a LZ. In this paper, we describe a lidar-based perception system that enables a full-scale autonomous helicopter to identify and land in previously unmapped terrain with no human input.We describe the problem, real-time algorithms, perception hardware, and results. Our approach has extended the state of the art in terrain assessment by incorporating not only plane fitting, but by also considering factors such as terrain/skid interaction, rotor and tail clearance, wind direction, clear approach/abort paths, and ground paths.In results from urban and natural environments we were able to successfully classify LZs from point cloud maps. We also present results from 8 successful landing experiments with varying ground clutter and approach directions. The helicopter selected its own landing site, approaches, and then proceeds to land. To our knowledge, these experiments were the first demonstration of a full-scale autonomous helicopter that selected its own landing zones and landed.</dcterms:abstract>
        <dc:date>2012-12-01</dc:date>
        <z:libraryCatalog>ResearchGate</z:libraryCatalog>
        <bib:pages>1545–1562</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_150">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Sanjiv-Singh-2/publication/257343605_Autonomous_Landing_at_Unprepared_Sites_by_a_Full-Scale_Helicopter/links/5b88832f4585151fd13dc576/Autonomous-Landing-at-Unprepared-Sites-by-a-Full-Scale-Helicopter.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:28:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_151">
        <z:itemType>attachment</z:itemType>
        <dc:title>ResearchGate Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/publication/257343605_Autonomous_Landing_at_Unprepared_Sites_by_a_Full-Scale_Helicopter</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:28:31</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
    </z:Attachment>
    <bib:Article rdf:about="https://ora.ox.ac.uk/objects/uuid:0758dad0-6b33-40d1-bade-0cc2eb6f989a">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1050-4729"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vineet</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Miksik</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lidegaard</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nießner</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Golodetz</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Prisacariu</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kähler</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murray</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Izadi</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pérez</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torr</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_156"/>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:subject>move_base_flex</dc:subject>
        <dc:subject>Dense</dc:subject>
        <dc:title>Incremental dense semantic stereo fusion for large-scale semantic scene reconstruction.</dc:title>
        <dcterms:abstract>Our abilities in scene understanding, which allow us to perceive the 3D structure of our surroundings and intuitively recognise the objects we see, are things that we largely take for granted, but for robots, the task of understanding large scenes quickly remains extremely challenging. Recently, scene understanding approaches based on 3D reconstruction and semantic segmentation have become popular, but existing methods either do not scale, fail outdoors, provide only sparse reconstructions or are rather slow. In this paper, we build on a recent hash-based technique for large-scale fusion and an efficient mean-field inference algorithm for densely-connected CRFs to present what to our knowledge is the first system that can perform dense, large-scale, outdoor semantic reconstruction of a scene in (near) real time. We also present a 'semantic fusion' approach that allows us to handle dynamic objects more effectively than previous approaches. We demonstrate the effectiveness of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense reconstruction and labelling of a number of scenes.</dcterms:abstract>
        <dc:date>2015</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ora.ox.ac.uk</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ora.ox.ac.uk/objects/uuid:0758dad0-6b33-40d1-bade-0cc2eb6f989a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:29:52</dcterms:dateSubmitted>
        <dc:description>ISBN: 9781479969234
Publisher: Institute of Electrical and Electronics Engineers</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1050-4729">
        <prism:volume>2015-June</prism:volume>
        <dc:title>ICRA 2015: IEEE International Conference on Robotics and Automation</dc:title>
        <prism:number>June</prism:number>
        <dc:identifier>ISSN 1050-4729</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_156">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ora.ox.ac.uk/objects/uuid:0758dad0-6b33-40d1-bade-0cc2eb6f989a/files/m7b65fa0d7751c7be336728e6cd6e6c08</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:29:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4799-3685-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4799-3685-4</dc:identifier>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2014.6907233</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Hong Kong, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pizzoli</foaf:surname>
                        <foaf:givenName>Matia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Forster</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_157"/>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:subject>move_base_flex</dc:subject>
        <dc:subject>Dense</dc:subject>
        <dc:title>REMODE: Probabilistic, monocular dense reconstruction in real time</dc:title>
        <dcterms:abstract>In this paper, we solve the problem of estimating dense and accurate depth maps from a single moving camera. A probabilistic depth measurement is carried out in real time on a per-pixel basis and the computed uncertainty is used to reject erroneous estimations and provide live feedback on the reconstruction progress. Our contribution is a novel approach to depth map computation that combines Bayesian estimation and recent development on convex optimization for image processing. We demonstrate that our method outperforms stateof-the-art techniques in terms of accuracy, while exhibiting high efﬁciency in memory usage and computing power. We call our approach REMODE (REgularized MOnocular Depth Estimation) and the CUDA-based implementation runs at 30Hz on a laptop computer.</dcterms:abstract>
        <dc:date>5/2014</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>REMODE</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/6907233/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:31:06</dcterms:dateSubmitted>
        <bib:pages>2609-2616</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_157">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://rpg.ifi.uzh.ch/docs/ICRA14_Pizzoli.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:31:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2008.05711">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Philion</foaf:surname>
                        <foaf:givenName>Jonah</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fidler</foaf:surname>
                        <foaf:givenName>Sanja</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_161"/>
        <link:link rdf:resource="#item_159"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>To read</dc:subject>
        <dc:subject>BEV</dc:subject>
        <dc:title>Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</dc:title>
        <dcterms:abstract>The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eyeview grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’seye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eyeview cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot.</dcterms:abstract>
        <dc:date>2020-08-13</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Lift, Splat, Shoot</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2008.05711</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:32:23</dcterms:dateSubmitted>
        <dc:description>arXiv:2008.05711 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2008.05711</dc:identifier>
        <prism:number>arXiv:2008.05711</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_161">
       <rdf:value>Comment: ECCV 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_159">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2008.05711</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:32:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2202.11271">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Robotics: Science and Systems XVIII</dc:title>
                <dc:identifier>DOI 10.15607/RSS.2022.XVIII.019</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shah</foaf:surname>
                        <foaf:givenName>Dhruv</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_164"/>
        <link:link rdf:resource="#item_162"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Local path</dc:subject>
        <dc:subject>Global path</dc:subject>
        <dc:subject>To read</dc:subject>
        <dc:subject>Vision based navigation</dc:subject>
        <dc:title>ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints</dc:title>
        <dcterms:abstract>Robotic navigation has been approached as a problem of 3D reconstruction and planning, as well as an end-to-end learning problem. However, long-range navigation requires both planning and reasoning about local traversability, as well as being able to utilize general knowledge about global geography, in the form of a roadmap, GPS, or other side information providing important cues. In this work, we propose an approach that integrates learning and planning, and can utilize side information such as schematic roadmaps, satellite maps and GPS coordinates as a planning heuristic, without relying on them being accurate. Our method, ViKiNG, incorporates a local traversability model, which looks at the robot's current camera observation and a potential subgoal to infer how easily that subgoal can be reached, as well as a heuristic model, which looks at overhead maps for hints and attempts to evaluate the appropriateness of these subgoals in order to reach the goal. These models are used by a heuristic planner to identify the best waypoint in order to reach the final destination. Our method performs no explicit geometric reconstruction, utilizing only a topological representation of the environment. Despite having never seen trajectories longer than 80 meters in its training dataset, ViKiNG can leverage its image-based learned controller and goal-directed heuristic to navigate to goals up to 3 kilometers away in previously unseen environments, and exhibit complex behaviors such as probing potential paths and backtracking when they are found to be non-viable. ViKiNG is also robust to unreliable maps and GPS, since the low-level controller ultimately makes decisions based on egocentric image observations, using maps only as planning heuristics. For videos of our experiments, please check out our project page https://sites.google.com/view/viking-release.</dcterms:abstract>
        <dc:date>2022-06-27</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>ViKiNG</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2202.11271</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:33:44</dcterms:dateSubmitted>
        <dc:description>arXiv:2202.11271 [cs]</dc:description>
    </rdf:Description>
    <bib:Memo rdf:about="#item_164">
        <rdf:value>Comment: Best Systems Paper Finalist at XVII Robotics: Science and Systems (RSS 2022), New York City, USA. Project page https://sites.google.com/view/viking-release</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_162">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2202.11271</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:33:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.mdpi.com/2504-446X/6/7/183">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2504-446X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Yao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Rendong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Binbing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Youchun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_166"/>
        <dc:subject>To read</dc:subject>
        <dc:subject>BEV</dc:subject>
        <dc:subject>Topological Map</dc:subject>
        <dc:title>Compact and Efficient Topological Mapping for Large-Scale Environment with Pruned Voronoi Diagram</dc:title>
        <dcterms:abstract>Topological maps generated in complex and irregular unknown environments are meaningful for autonomous robots’ navigation. To obtain the skeleton of the environment without obstacle polygon extraction and clustering, we propose a method to obtain high-quality topological maps using only pure Voronoi diagrams in three steps. Supported by Voronoi vertex’s property of the largest empty circle, the method updates the global topological map incrementally in both dynamic and static environments online. The incremental method can be adapted to any fundamental Voronoi diagram generator. We maintain the entire space by two graphs, the pruned Voronoi graph for incremental updates and the reduced approximated generalized Voronoi graph for routing planning requests. We present an extensive benchmark and real-world experiment, and our method completes the environment representation in both indoor and outdoor areas. The proposed method generates a compact topological map in both small- and large-scale scenarios, which is defined as the total length and vertices of topological maps. Additionally, our method has been shortened by several orders of magnitude in terms of the total length and consumes less than 30% of the average time cost compared to state-of-the-art methods.</dcterms:abstract>
        <dc:date>2022/7</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>www.mdpi.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.mdpi.com/2504-446X/6/7/183</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:37:21</dcterms:dateSubmitted>
        <dc:rights>http://creativecommons.org/licenses/by/3.0/</dc:rights>
        <dc:description>Number: 7
Publisher: Multidisciplinary Digital Publishing Institute</dc:description>
        <bib:pages>183</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2504-446X">
        <prism:volume>6</prism:volume>
        <dc:title>Drones</dc:title>
        <dc:identifier>DOI 10.3390/drones6070183</dc:identifier>
        <prism:number>7</prism:number>
        <dc:identifier>ISSN 2504-446X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_166">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.mdpi.com/2504-446X/6/7/183/pdf?version=1658456565</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:37:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0140366419307868">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0140-3664"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>Tao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Almeida</foaf:surname>
                        <foaf:givenName>Jefferson S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>da Silva</foaf:surname>
                        <foaf:givenName>Suane Pires P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Filho</foaf:surname>
                        <foaf:givenName>Paulo Honório</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Oliveira Rodrigues</foaf:surname>
                        <foaf:givenName>Antonio W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Albuquerque</foaf:surname>
                        <foaf:givenName>Victor Hugo C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rebouças Filho</foaf:surname>
                        <foaf:givenName>Pedro P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_171"/>
        <link:link rdf:resource="#item_170"/>
        <dc:subject>To read</dc:subject>
        <dc:subject>Topological Map</dc:subject>
        <dc:title>An effective approach to unmanned aerial vehicle navigation using visual topological map in outdoor and indoor environments</dc:title>
        <dcterms:abstract>Unmanned Aerial Vehicles are constantly being using in professional activities that require higher precision in navigating and positioning the aircraft during operation. Advanced location technologies such as Global Navigation Satellite System and Real-Time Kinematic are widely used, however, they depend on an area with transmission coverage. In this approach, this article presents a visual navigation methodology based on topological maps. We compared the performance of consolidated classifiers such as Bayesian classifier, k-nearest neighbor, Multilayer Perceptron, Optimal Path Forest and Support Vector Machines (SVM). They are evaluated with attributes returned by last generation resource extractors such as Fourier, Gray Level Co-Occurrence and Local Binary Patterns (LBP). After analyzing the results we found that the combination of LBP and SVM obtained the best values in the evaluation metrics considered, among them, 99.99% Specificity and 99.98% Precision in the navigation process. SVM reached 5.49787 s in combination with LBP completes the training in 5.49787 s. Concerning the testing time, SVM achieving 80.91 ms in association with LBP.</dcterms:abstract>
        <dc:date>2020-01-15</dc:date>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0140366419307868</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:40:39</dcterms:dateSubmitted>
        <bib:pages>696-702</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0140-3664">
        <prism:volume>150</prism:volume>
        <dc:title>Computer Communications</dc:title>
        <dc:identifier>DOI 10.1016/j.comcom.2019.12.026</dc:identifier>
        <dcterms:alternative>Computer Communications</dcterms:alternative>
        <dc:identifier>ISSN 0140-3664</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_171">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_170">
        <z:itemType>attachment</z:itemType>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0140366419307868</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:40:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_173">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stentz</foaf:surname>
                        <foaf:givenName>Anthony</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_172"/>
        <dc:relation rdf:resource="http://arxiv.org/abs/2004.04697"/>
        <dc:subject>To read</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>The Focussed D* Algorithm for Real-Time Replanning</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_172">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://robby.caltech.edu/~jwb/courses/ME132/handouts/Dstar_ijcai95.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:43:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.mdpi.com/1424-8220/13/1/1247">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1424-8220"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Garzón</foaf:surname>
                        <foaf:givenName>Mario</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Valente</foaf:surname>
                        <foaf:givenName>João</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zapata</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barrientos</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_175"/>
        <dc:relation rdf:resource="http://arxiv.org/abs/2004.04697"/>
        <dc:subject>To read</dc:subject>
        <dc:title>An Aerial-Ground Robotic System for Navigation and Obstacle Mapping in Large Outdoor Areas</dc:title>
        <dcterms:abstract>There are many outdoor robotic applications where a robot must reach a goal position or explore an area without previous knowledge of the environment around it. Additionally, other applications (like path planning) require the use of known maps or previous information of the environment. This work presents a system composed by a terrestrial and an aerial robot that cooperate and share sensor information in order to address those requirements. The ground robot is able to navigate in an unknown large environment aided by visual feedback from a camera on board the aerial robot. At the same time, the obstacles are mapped in real-time by putting together the information from the camera and the positioning system of the ground robot. A set of experiments were carried out with the purpose of verifying the system applicability. The experiments were performed in a simulation environment and outdoor with a medium-sized ground robot and a mini quad-rotor. The proposed robotic system shows outstanding results in simultaneous navigation and mapping applications in large outdoor environments.</dcterms:abstract>
        <dc:date>2013/1</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>www.mdpi.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.mdpi.com/1424-8220/13/1/1247</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:43:56</dcterms:dateSubmitted>
        <dc:rights>http://creativecommons.org/licenses/by/3.0/</dc:rights>
        <dc:description>Number: 1
Publisher: Multidisciplinary Digital Publishing Institute</dc:description>
        <bib:pages>1247-1267</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1424-8220">
        <prism:volume>13</prism:volume>
        <dc:title>Sensors</dc:title>
        <dc:identifier>DOI 10.3390/s130101247</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1424-8220</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_175">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.mdpi.com/1424-8220/13/1/1247/pdf?version=1403322339</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:43:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
</rdf:RDF>
