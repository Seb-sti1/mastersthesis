<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/7812671/?arnumber=7812671">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2377-3766"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Delmerico</foaf:surname>
                        <foaf:givenName>Jeffrey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mueggler</foaf:surname>
                        <foaf:givenName>Elias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nitsch</foaf:surname>
                        <foaf:givenName>Julia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_4"/>
        <link:link rdf:resource="#item_3"/>
        <dc:subject>Exploration</dc:subject>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Collaboration</dc:subject>
        <dc:subject>Global path</dc:subject>
        <dc:title>Active Autonomous Aerial Exploration for Ground Robot Path Planning</dc:title>
        <dcterms:abstract>We address the problem of planning a path for a ground robot through unknown terrain, using observations from a flying robot. In search and rescue missions, which are our target scenarios, the time from arrival at the disaster site to the delivery of aid is critically important. Previous works required exhaustive exploration before path planning, which is time-consuming but eventually leads to an optimal path for the ground robot. Instead, we propose active exploration of the environment, where the flying robot chooses regions to map in a way that optimizes the overall response time of the system, which is the combined time for the air and ground robots to execute their missions. In our approach, we estimate terrain classes throughout our terrain map, and we also add elevation information in areas where the active exploration algorithm has chosen to perform 3-D reconstruction. This terrain information is used to estimate feasible and efficient paths for the ground robot. By exploring the environment actively, we achieve superior response times compared to both exhaustive and greedy exploration strategies. We demonstrate the performance and capabilities of the proposed system in simulated and real-world outdoor experiments. To the best of our knowledge, this is the first work to address ground robot path planning using active aerial exploration.</dcterms:abstract>
        <dc:date>2017-04</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/7812671/?arnumber=7812671</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 08:30:10</dcterms:dateSubmitted>
        <dc:description>Conference Name: IEEE Robotics and Automation Letters</dc:description>
        <bib:pages>664-671</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2377-3766">
        <prism:volume>2</prism:volume>
        <dc:title>IEEE Robotics and Automation Letters</dc:title>
        <dc:identifier>DOI 10.1109/LRA.2017.2651163</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 2377-3766</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_4">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=7812671&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 08:30:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_3">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/7812671/?arnumber=7812671</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 08:30:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://ieeexplore.ieee.org/document/1618537/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1552-3098"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Comport</foaf:surname>
                        <foaf:givenName>A.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marchand</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chaumette</foaf:surname>
                        <foaf:givenName>F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_9"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067"/>
        <dc:subject>M-estimator</dc:subject>
        <dc:subject>Robust matching</dc:subject>
        <dc:subject>Key points</dc:subject>
        <dc:title>Statistically robust 2-D visual servoing</dc:title>
        <dcterms:abstract>A fundamental step towards broadening the use of real world image-based visual servoing is to deal with the important issue of reliability and robustness. In order to address this issue, a closed loop control law is proposed that simultaneously accomplishes a visual servoing task and is robust to a general class of image processing errors. This is achieved with the application of widely accepted statistical techniques such as robust M-estimation and LMedS. Experimental results are presented which demonstrate visual servoing tasks that resist severe outlier contamination.</dcterms:abstract>
        <dc:date>04/2006</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/1618537/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:06</dcterms:dateSubmitted>
        <dc:rights>https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html</dc:rights>
        <bib:pages>415-420</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1552-3098">
        <prism:volume>22</prism:volume>
        <dc:title>IEEE Transactions on Robotics</dc:title>
        <dc:identifier>DOI 10.1109/TRO.2006.870666</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>IEEE Trans. Robot.</dcterms:alternative>
        <dc:identifier>ISSN 1552-3098</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_9">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://inria.hal.science/inria-00350284/document</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings 2007 IEEE International Conference on Robotics and Automation</dc:title>
                <dc:identifier>DOI 10.1109/ROBOT.2007.363762</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Comport</foaf:surname>
                        <foaf:givenName>A.I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Malis</foaf:surname>
                        <foaf:givenName>E.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rives</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_13"/>
        <link:link rdf:resource="#item_12"/>
        <dc:relation rdf:resource="http://ieeexplore.ieee.org/document/1618537/"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551"/>
        <dc:subject>Dense correspondences</dc:subject>
        <dc:subject>Quadrifocal warping</dc:subject>
        <dc:title>Accurate Quadrifocal Tracking for Robust 3D Visual Odometry</dc:title>
        <dcterms:abstract>This paper describes a new image-based approach to tracking the 6dof trajectory of a stereo camera pair using a corresponding reference image pairs instead of explicit 3D feature reconstruction of the scene. A dense minimisation approach is employed which directly uses all grey-scale information available within the stereo pair (or stereo region) leading to very robust and precise results. Metric 3D structure constraints are imposed by consistently warping corresponding stereo images to generate novel viewpoints at each stereo acquisition. An iterative non-linear trajectory estimation approach is formulated based on a quadrifocal relationship between the image intensities within adjacent views of the stereo pair. A robust M-estimation technique is used to reject outliers corresponding to moving objects within the scene or other outliers such as occlusions and illumination changes. The technique is applied to recovering the trajectory of a moving vehicle in long and difficult sequences of images.</dcterms:abstract>
        <dc:date>2007-04</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:47</dcterms:dateSubmitted>
        <dc:description>ISSN: 1050-4729</dc:description>
        <bib:pages>40-45</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings 2007 IEEE International Conference on Robotics and Automation</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_13">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=4209067&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_12">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:40:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9739395/?arnumber=9739395">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)</dc:title>
                <dc:identifier>DOI 10.1109/ROBIO54168.2021.9739395</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Yuqian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Xuetao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Yisha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhuang</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_21"/>
        <link:link rdf:resource="#item_19"/>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Topological Map</dc:subject>
        <dc:title>2D Topological Map Building by UAVs for Ground Robot Navigation</dc:title>
        <dcterms:abstract>In air-ground cooperation, unmanned aerial vehicles (UAVs) are used to build a priori map of the ground environment from an aerial perspective, which is conducive to improve the navigation ability of ground robots. This paper proposes a 2D topology map building method from the air view, which can be effectively used for global path planning of ground mobile robots. To realize the 3D reconstruction of the ground from the air view, the 3D Euclidean Signed Distance Field (ESDF) is constructed incrementally from the Truncated Signed Distance Field (TSDF). The ESDF map occupies a large storage space, which is inconvenient to transmit to ground robots. To solve this problem, a lighter topology map is constructed on the basis of the ESDF map. Since ground robots can only move in the 2D plane, the 2D topological map is generated at any height from the ground to represent the topological structure of the ground robots working environment. The experimental results on the dataset and simulation environment shows the effectiveness of the proposed method.</dcterms:abstract>
        <dc:date>2021-12</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9739395/?arnumber=9739395</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:47:04</dcterms:dateSubmitted>
        <bib:pages>663-668</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_21">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9739395&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:47:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_19">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9739395/?arnumber=9739395</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:47:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2501.18351">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Jianfeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dong</foaf:surname>
                        <foaf:givenName>Hanlin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Jiahui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Shibo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Ke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tang</foaf:surname>
                        <foaf:givenName>Xuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wei</foaf:surname>
                        <foaf:givenName>Xian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>You</foaf:surname>
                        <foaf:givenName>Xiong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_22"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Satellite imagery</dc:subject>
        <dc:title>Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments</dc:title>
        <dcterms:abstract>Path planning with strong environmental adaptability plays a crucial role in robotic navigation in unstructured outdoor environments, especially in the case of low-quality location and map information. The path planning ability of a robot depends on the identification of the traversability of global and local ground areas. In real-world scenarios, the complexity of outdoor open environments makes it difficult for robots to identify the traversability of ground areas that lack a clearly defined structure. Moreover, most existing methods have rarely analyzed the integration of local and global traversability identifications in unstructured outdoor scenarios. To address this problem, we propose a novel method, Dual-BEV Nav, first introducing Bird’s Eye View (BEV) representations into local planning to generate high-quality traversable paths. Then, these paths are projected onto the global traversability map generated by the global BEV planning model to obtain the optimal waypoints. By integrating the traversability from both local and global BEV, we establish a dual-layer BEV heuristic planning paradigm, enabling long-distance navigation in unstructured outdoor environments. We test our approach through both public dataset evaluations and real-world robot deployments, yielding promising results. Compared to baselines, the DualBEV Nav improved temporal distance prediction accuracy by up to 18.7%. In the real-world deployment, under conditions significantly different from the training set and with notable occlusions in the global BEV, the Dual-BEV Nav successfully achieved a 65-meter-long outdoor navigation. Further analysis demonstrates that the local BEV representation significantly enhances the rationality of the planning, while the global BEV probability map ensures the robustness of the overall planning.</dcterms:abstract>
        <dc:date>2025-01-30</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Dual-BEV Nav</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2501.18351</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:50:55</dcterms:dateSubmitted>
        <dc:description>arXiv:2501.18351 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2501.18351</dc:identifier>
        <prism:number>arXiv:2501.18351</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_22">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2501.18351</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:50:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2004.04697">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manderson</foaf:surname>
                        <foaf:givenName>Travis</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wapnick</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meger</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dudek</foaf:surname>
                        <foaf:givenName>Gregory</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_25"/>
        <dc:relation rdf:resource="https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1"/>
        <dc:relation rdf:resource="#item_173"/>
        <dc:relation rdf:resource="https://www.mdpi.com/1424-8220/13/1/1247"/>
        <dc:subject>Reinforcement learning</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Local path</dc:subject>
        <dc:subject>Collaboration</dc:subject>
        <dc:title>Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images</dc:title>
        <dcterms:abstract>We present a method for learning to drive on smooth terrain while simultaneously avoiding collisions in challenging off-road and unstructured outdoor environments using only visual inputs. Our approach applies a hybrid model-based and model-free reinforcement learning method that is entirely self-supervised in labeling terrain roughness and collisions using on-board sensors. Notably, we provide both ﬁrst-person and overhead aerial image inputs to our model. We ﬁnd that the fusion of these complementary inputs improves planning foresight and makes the model robust to visual obstructions. Our results show the ability to generalize to environments with plentiful vegetation, various types of rock, and sandy trails. During evaluation, our policy attained 90% smooth terrain traversal and reduced the proportion of rough terrain driven over by 6.1 times compared to a model using only ﬁrstperson imagery. Video and project details can be found at www.cim.mcgill.ca/mrl/offroad driving/.</dcterms:abstract>
        <dc:date>2020-04-09</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2004.04697</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:56:22</dcterms:dateSubmitted>
        <dc:description>arXiv:2004.04697 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2004.04697</dc:identifier>
        <prism:number>arXiv:2004.04697</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_25">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2004.04697</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 14:56:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2409.18253">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fortin</foaf:surname>
                        <foaf:givenName>Jean-Michel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gamache</foaf:surname>
                        <foaf:givenName>Olivier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fecteau</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Daum</foaf:surname>
                        <foaf:givenName>Effie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Larrivée-Hardy</foaf:surname>
                        <foaf:givenName>William</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pomerleau</foaf:surname>
                        <foaf:givenName>François</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giguère</foaf:surname>
                        <foaf:givenName>Philippe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_31"/>
        <link:link rdf:resource="#item_32"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Dataset</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>ResNet18</dc:subject>
        <dc:title>UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation</dc:title>
        <dcterms:abstract>Terrain awareness is an essential milestone to enable truly autonomous off-road navigation. Accurately predicting terrain characteristics allows optimizing a vehicle's path against potential hazards. Recent methods use deep neural networks to predict traversability-related terrain properties in a self-supervised manner, relying on proprioception as a training signal. However, onboard cameras are inherently limited by their point-of-view relative to the ground, suffering from occlusions and vanishing pixel density with distance. This paper introduces a novel approach for self-supervised terrain characterization using an aerial perspective from a hovering drone. We capture terrain-aligned images while sampling the environment with a ground vehicle, effectively training a simple predictor for vibrations, bumpiness, and energy consumption. Our dataset includes 2.8 km of off-road data collected in forest environment, comprising 13 484 ground-based images and 12 935 aerial images. Our findings show that drone imagery improves terrain property prediction by 21.37 % on the whole dataset and 37.35 % in high vegetation, compared to ground robot images. We conduct ablation studies to identify the main causes of these performance improvements. We also demonstrate the real-world applicability of our approach by scouting an unseen area with a drone, planning and executing an optimized path on the ground.</dcterms:abstract>
        <dc:date>2024-09-26</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2409.18253</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 16:02:01</dcterms:dateSubmitted>
        <dc:description>arXiv:2409.18253 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2409.18253</dc:identifier>
        <prism:number>arXiv:2409.18253</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_31">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2409.18253v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 16:02:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_32">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2409.18253</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-14 16:02:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2404.18750">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mortimer</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maehlisch</foaf:surname>
                        <foaf:givenName>Mirko</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_38"/>
        <link:link rdf:resource="#item_39"/>
        <dc:subject>Dataset</dc:subject>
        <dc:title>Survey on Datasets for Perception in Unstructured Outdoor Environments</dc:title>
        <dcterms:abstract>Perception is an essential component of pipelines in field robotics. In this survey, we quantitatively compare publicly available datasets available in unstructured outdoor environments. We focus on datasets for common perception tasks in field robotics. Our survey categorizes and compares available research datasets. This survey also reports on relevant dataset characteristics to help practitioners determine which dataset fits best for their own application. We believe more consideration should be taken in choosing compatible annotation policies across the datasets in unstructured outdoor environments.</dcterms:abstract>
        <dc:date>2024-04-29</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2404.18750</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 08:41:36</dcterms:dateSubmitted>
        <dc:description>arXiv:2404.18750 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2404.18750</dc:identifier>
        <prism:number>arXiv:2404.18750</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_38">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2404.18750v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 08:41:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_39">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2404.18750</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 08:41:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2008.4651086</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rufus Blas</foaf:surname>
                        <foaf:givenName>Morten</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Agrawal</foaf:surname>
                        <foaf:givenName>Motilal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sundaresan</foaf:surname>
                        <foaf:givenName>Aravind</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Konolige</foaf:surname>
                        <foaf:givenName>Kurt</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_46"/>
        <link:link rdf:resource="#item_45"/>
        <dc:relation rdf:resource="urn:isbn:978-1-4244-3803-7"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Trail classification</dc:subject>
        <dc:title>Fast color/texture segmentation for outdoor robots</dc:title>
        <dcterms:abstract>We present a fast integrated approach for online segmentation of images for outdoor robots. A compact color and texture descriptor has been developed to describe local color and texture variations in an image. This descriptor is then used in a two-stage fast clustering framework using K-means to perform online segmentation of natural images. We present results of applying our descriptor for segmenting a synthetic image and compare it against other state-of-the-art descriptors. We also apply our segmentation algorithm to the task of detecting natural paths in outdoor images. The whole system has been demonstrated to work online alongside localization, 3D obstacle detection, and planning.</dcterms:abstract>
        <dc:date>2008-09</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 09:59:36</dcterms:dateSubmitted>
        <dc:description>ISSN: 2153-0866</dc:description>
        <bib:pages>4078-4085</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2008 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_46">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=4651086&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 09:59:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_45">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 09:59:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4244-3803-7">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4244-3803-7</dc:identifier>
                <dc:title>2009 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2009.5354059</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>St. Louis, MO, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rasmussen</foaf:surname>
                        <foaf:givenName>Christopher</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Yan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kocamaz</foaf:surname>
                        <foaf:givenName>Mehmet</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_48"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/4651086/?arnumber=4651086"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Trail classification</dc:subject>
        <dc:subject>Local path</dc:subject>
        <dc:subject>Lidar</dc:subject>
        <dc:title>Appearance contrast for fast, robust trail-following</dc:title>
        <dcterms:abstract>We describe a framework for ﬁnding and tracking “trails” for autonomous outdoor robot navigation. Through a combination of visual cues and ladar-derived structural information, the algorithm is able to follow paths which pass through multiple zones of terrain smoothness, border vegetation, tread material, and illumination conditions. Our shape-based visual trail tracker assumes that the approaching trail region is approximately triangular under perspective. It generates region hypotheses from a learned distribution of expected trail width and curvature variation, and scores them using a robust measure of color and brightness contrast with ﬂanking regions. The structural component analogously rewards hypotheses which correspond to empty or low-density regions in a groundstrike-ﬁltered ladar obstacle map. Our system’s performance is analyzed on several long sequences with diverse appearance and structural characteristics. Ground-truth segmentations are used to quantify performance where available, and several alternative algorithms are compared on the same data.</dcterms:abstract>
        <dc:date>10/2009</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/5354059/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 10:18:03</dcterms:dateSubmitted>
        <bib:pages>3505-3512</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2009)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_48">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_IROS_2009/papers/1405.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 10:17:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/988771/?arnumber=988771">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)</dc:title>
                <dc:identifier>DOI 10.1109/SMBV.2001.988771</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scharstein</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szeliski</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zabih</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_67"/>
        <link:link rdf:resource="#item_66"/>
        <dc:subject>To read</dc:subject>
        <dc:title>A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</dc:title>
        <dcterms:abstract>Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can be easily extended to include new algorithms. We have also produced several new multiframe stereo data sets with ground truth, and are making both the code and data sets available on the Web.</dcterms:abstract>
        <dc:date>2001-12</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/988771/?arnumber=988771</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 13:33:33</dcterms:dateSubmitted>
        <bib:pages>131-140</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_67">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=988771&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 13:33:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_66">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/988771/?arnumber=988771</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 13:33:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1558-0016"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>van der Mark</foaf:surname>
                        <foaf:givenName>W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gavrila</foaf:surname>
                        <foaf:givenName>D.M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_95"/>
        <link:link rdf:resource="#item_94"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/4209067/?arnumber=4209067"/>
        <dc:subject>To read</dc:subject>
        <dc:title>Real-time dense stereo for intelligent vehicles</dc:title>
        <dcterms:abstract>Stereo vision is an attractive passive sensing technique for obtaining three-dimensional (3-D) measurements. Recent hardware advances have given rise to a new class of real-time dense disparity estimation algorithms. This paper examines their suitability for intelligent vehicle (IV) applications. In order to gain a better understanding of the performance and the computational-cost tradeoff, the authors created a framework of real-time implementations. This consists of different methodical components based on single instruction multiple data (SIMD) techniques. Furthermore, the resulting algorithmic variations are compared with other publicly available algorithms. The authors argue that existing publicly available stereo data sets are not very suitable for the IV domain. Therefore, the authors' evaluation of stereo algorithms is based on novel realistically looking simulated data as well as real data from complex urban traffic scenes. In order to facilitate future benchmarks, all data used in this paper is made publicly available. The results from this study reveal that there is a considerable influence of scene conditions on the performance of all tested algorithms. Approaches that aim for (global) search optimization are more affected by this than other approaches. The best overall performance is achieved by the proposed multiple-window algorithm, which uses local matching and a left-right check for a robust error rejection. Timing results show that the simplest of the proposed SIMD variants are more than twice as fast than the most complex one. Nevertheless, the latter still achieves real-time processing speeds, while their average accuracy is at least equal to that of publicly available non-SIMD algorithms</dcterms:abstract>
        <dc:date>2006-03</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 16:14:39</dcterms:dateSubmitted>
        <dc:description>Conference Name: IEEE Transactions on Intelligent Transportation Systems</dc:description>
        <bib:pages>38-50</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1558-0016">
        <prism:volume>7</prism:volume>
        <dc:title>IEEE Transactions on Intelligent Transportation Systems</dc:title>
        <dc:identifier>DOI 10.1109/TITS.2006.869625</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1558-0016</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_95">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=1603551&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 16:14:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_94">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/1603551/?arnumber=1603551</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-17 16:14:48</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="http://arxiv.org/abs/1806.02487">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
           <bib:Book><prism:volume>11</prism:volume></bib:Book>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Luqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Daqian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>Fei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Fengyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guo</foaf:surname>
                        <foaf:givenName>Jixin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Mengxiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Shaojie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_110"/>
        <dc:subject>Exploration</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Collaboration</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>A Collaborative Aerial-Ground Robotic System for Fast Exploration</dc:title>
        <dcterms:abstract>Autonomous exploration of unknown environments has been widely applied in inspection, surveillance, and search and rescue. In exploration task, the basic requirement for robots is to detect the unknown space as fast as possible. In this paper, we propose an autonomous collaborative system consists of an aerial robot and a ground vehicle to explore in unknown environments. We combine the frontier based method and the harmonic field to generate a path. Then, For the ground robot, a minimum jerk piecewise Bezier curve which can guarantee safety and dynamical feasibility is generated amid obstacles. For the aerial robot, a motion primitive method is adopted for local path planning. We implement the proposed framework on an autonomous collaborative aerial-ground system. Extensive field experiments as well as simulations are presented to validate the method and demonstrate its higher efficiency against each single vehicle.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1806.02487</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 10:34:14</dcterms:dateSubmitted>
        <dc:description>DOI: 10.1007/978-3-030-33950-0_6
arXiv:1806.02487 [cs]</dc:description>
        <bib:pages>59-71</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_110">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://typeset.io/pdf/a-collaborative-aerial-ground-robotic-system-for-fast-4l6aejojb1.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 10:34:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/6224988/?arnumber=6224988">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2012 IEEE International Conference on Robotics and Automation</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2012.6224988</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khan</foaf:surname>
                        <foaf:givenName>Yasir Niaz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Masselli</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zell</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_117"/>
        <link:link rdf:resource="#item_115"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>TSURF</dc:subject>
        <dc:subject>LBP</dc:subject>
        <dc:subject>LTP</dc:subject>
        <dc:title>Visual terrain classification by flying robots</dc:title>
        <dcterms:abstract>In this paper we investigate the effectiveness of SURF features for visual terrain classification for outdoor flying robots. A quadrocopter fitted with a single camera is flown over different terrains to take images of the ground below. Each image is divided into a grid and SURF features are calculated at grid intersections. A classifier is then used to learn to differentiate between different terrain types. Classification results of the SURF descriptor are compared with results from other texture descriptors like Local Binary Patterns and Local Ternary Patterns. Six different terrain types are considered in this approach. Random forests are used for classification on each descriptor. It is shown that SURF features perform better than other descriptors at higher resolutions.</dcterms:abstract>
        <dc:date>2012-05</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/6224988/?arnumber=6224988</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:17:57</dcterms:dateSubmitted>
        <dc:description>ISSN: 1050-4729</dc:description>
        <bib:pages>498-503</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2012 IEEE International Conference on Robotics and Automation</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_117">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=6224988&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_115">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/6224988/?arnumber=6224988</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:16</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
                <dc:identifier>DOI 10.1109/IROS.2006.281686</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Silver</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sofman</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vandapel</foaf:surname>
                        <foaf:givenName>Nicolas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bagnell</foaf:surname>
                        <foaf:givenName>J. Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stentz</foaf:surname>
                        <foaf:givenName>Anthony</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_118"/>
        <link:link rdf:resource="#item_116"/>
        <dc:relation rdf:resource="https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Satellite imagery</dc:subject>
        <dc:subject>Lidar</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>Experimental Analysis of Overhead Data Processing To Support Long Range Navigation</dc:title>
        <dcterms:abstract>Long range navigation by unmanned ground vehicles continues to challenge the robotics community. Efficient navigation requires not only intelligent on-board perception and planning systems, but also the effective use of prior knowledge of the vehicle's environment. This paper describes a system for supporting unmanned ground vehicle navigation through the use of heterogeneous overhead data. Semantic information is obtained through supervised classification, and vehicle mobility is predicted from available geometric data. This approach is demonstrated and validated through over 50 kilometers of autonomous traversal through complex natural environments</dcterms:abstract>
        <dc:date>2006-10</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:13</dcterms:dateSubmitted>
        <dc:description>ISSN: 2153-0866</dc:description>
        <bib:pages>2443-2450</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2006 IEEE/RSJ International Conference on Intelligent Robots and Systems</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_118">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=4058754&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:26</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_116">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:18:21</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>DOI 10.1184/R1/6561173.v1</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sofman</foaf:surname>
                        <foaf:givenName>Boris</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bagnell</foaf:surname>
                        <foaf:givenName>J. Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stentz</foaf:surname>
                        <foaf:givenName>Anthony</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vandapel</foaf:surname>
                        <foaf:givenName>Nicolas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_121"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/4058754/?arnumber=4058754"/>
        <dc:relation rdf:resource="http://arxiv.org/abs/2004.04697"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>Satellite imagery</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>Terrain Classification from Aerial Data to Support Ground Vehicle Navigation</dc:title>
        <dcterms:abstract>Sensory perception for unmanned ground vehicle navigation has received great attention from the robotics community. However, sensors mounted on the vehicle are regularly viewpoint impaired. A vehicle navigating at high speeds in off- road environments may be unable to react to negative obstacles such as large holes and cliffs. One approach to address this problem is to complement the sensing capabilities of an unmanned ground vehicle with overhead data gathered from an aerial source. This paper presents techniques to achieve accurate terrain classiﬁcation by utilizing high-density, colorized, three- dimensional laser data. We describe methods to extract relevant features from this sensor data in such a way that a learning algorithm can successfully train on a small set of labeled data in order to classify a much larger map and show experimental results. Additionally, we introduce a technique to signiﬁcantly reduce classiﬁcation errors through the use of context. Finally, we show how this algorithm can be customized for the intended vehicle’s capabilities in order to create more accurate a priori maps that can then be used for path planning.</dcterms:abstract>
        <dc:date>2006/01/01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>kilthub.cmu.edu</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:20:58</dcterms:dateSubmitted>
        <dc:description>Publisher: Carnegie Mellon University</dc:description>
    </bib:Article>
    <z:Attachment rdf:about="#item_121">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173/1/files/12043478.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:20:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-0-7695-2372-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 978-0-7695-2372-9</dc:identifier>
                <dc:identifier>DOI 10.1109/CVPR.2005.52</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Guo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Xin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mao</foaf:surname>
                        <foaf:givenName>Zhihong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_124"/>
        <link:link rdf:resource="#item_125"/>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Man-made segmentation</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>A two-stage level set evolution scheme for man-made objects detection in aerial images</dc:title>
        <dcterms:abstract>A novel two-stage level set evolution method for detecting man-made objects in aerial images is described. The method is based on a modified Mumford-Shah model and it uses a two-stage curve evolution strategy to get a preferable detection. It applies fractal error metric, developed by Cooper, et al. (1994) at the first curve evolution stage and adds additional constraint texture edge descriptor that is defined by using DCT (discrete cosine transform) coefficients on the image at the next stage. Man-made objects and natural areas are optimally differentiated by evolving the partial differential equation. The method artfully avoids selecting a threshold to separate the fractal error image, while an improper threshold often results in great segmentation errors. Experiments of the segmentation show that the proposed method is efficient.</dcterms:abstract>
        <dc:date>2005-07-20</dc:date>
        <z:libraryCatalog>ResearchGate</z:libraryCatalog>
        <bib:pages>474-479 vol. 1</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_124">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Xin-Yang-56/publication/4156206_A_two-stage_level_set_evolution_scheme_for_man-made_objects_detection_in_aerial_images/links/54bc76c20cf29e0cb04bf99b/A-two-stage-level-set-evolution-scheme-for-man-made-objects-detection-in-aerial-images.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:27:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_125">
        <z:itemType>attachment</z:itemType>
        <dc:title>ResearchGate Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/publication/4156206_A_two-stage_level_set_evolution_scheme_for_man-made_objects_detection_in_aerial_images</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:27:18</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/7358076">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>1</prism:volume>
                <dc:title>IEEE Robotics and Automation Letters</dc:title>
                <dc:identifier>DOI 10.1109/LRA.2015.2509024</dc:identifier>
                <prism:number>2</prism:number>
                <dc:identifier>ISSN 2377-3766</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giusti</foaf:surname>
                        <foaf:givenName>Alessandro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Guzzi</foaf:surname>
                        <foaf:givenName>Jérôme</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cireşan</foaf:surname>
                        <foaf:givenName>Dan C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Fang-Lin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rodríguez</foaf:surname>
                        <foaf:givenName>Juan P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fontana</foaf:surname>
                        <foaf:givenName>Flavio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Faessler</foaf:surname>
                        <foaf:givenName>Matthias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Forster</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schmidhuber</foaf:surname>
                        <foaf:givenName>Jürgen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Caro</foaf:surname>
                        <foaf:givenName>Gianni Di</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gambardella</foaf:surname>
                        <foaf:givenName>Luca M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_129"/>
        <link:link rdf:resource="#item_130"/>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Dataset</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Trail classification</dc:subject>
        <dc:title>A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots</dc:title>
        <dcterms:abstract>We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.</dcterms:abstract>
        <dc:date>2016-07</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/7358076</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:29:32</dcterms:dateSubmitted>
        <dc:description>Conference Name: IEEE Robotics and Automation Letters</dc:description>
        <bib:pages>661-667</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_129">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=7358076&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:29:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_130">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/7358076</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 11:29:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21423">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1556-4967"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Santana</foaf:surname>
                        <foaf:givenName>Pedro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Correia</foaf:surname>
                        <foaf:givenName>Luís</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mendonça</foaf:surname>
                        <foaf:givenName>Ricardo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alves</foaf:surname>
                        <foaf:givenName>Nelson</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barata</foaf:surname>
                        <foaf:givenName>José</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_134"/>
        <link:link rdf:resource="#item_135"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Traversability</dc:subject>
        <dc:subject>Trail classification</dc:subject>
        <dc:subject>Saliency</dc:subject>
        <dc:subject>Not read</dc:subject>
        <dc:title>Tracking natural trails with swarm-based visual saliency</dc:title>
        <dcterms:abstract>This paper proposes a model for trail detection and tracking that builds upon the observation that trails are salient structures in the robot's visual field. Due to the complexity of natural environments, the straightforward application of bottom-up visual saliency models is not sufficiently robust to predict the location of trails. As for other detection tasks, robustness can be increased by modulating the saliency computation based on a priori knowledge about which pixel-wise visual features are most representative of the object being sought. This paper proposes the use of the object's overall layout as the primary cue instead, as it is more stable and predictable in natural trails. Bearing in mind computational parsimony and detection robustness, this knowledge is specified in terms of perception-action rules, which control the behavior of simple agents performing as a swarm to compute the saliency map of the input image. For the purpose of tracking, multiframe evidence about the trail location is obtained with a motion-compensated dynamic neural field. In addition, to reduce ambiguity between the trail and trail-like distractors, a simple appearance model is learned online and used to influence the agents' activity. Experimental results on a large data set reveal the ability of the model to produce a success rate on the order of 97% at 20 Hz. The model is shown to be robust in situations where previous models would fail, such as when the trail does not emerge from the lower part of the image or when it is considerably interrupted. © 2012 Wiley Periodicals, Inc.</dcterms:abstract>
        <dc:date>2013</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Wiley Online Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21423</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:38:29</dcterms:dateSubmitted>
        <dc:rights>© 2012 Wiley Periodicals, Inc.</dc:rights>
        <dc:description>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21423</dc:description>
        <bib:pages>64-86</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1556-4967">
        <prism:volume>30</prism:volume>
        <dc:title>Journal of Field Robotics</dc:title>
        <dc:identifier>DOI 10.1002/rob.21423</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1556-4967</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_134">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rob.21423</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:38:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_135">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/10.1002/rob.21423</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:38:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-6654-7927-1">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-6654-7927-1</dc:identifier>
                <dc:title>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</dc:title>
                <dc:identifier>DOI 10.1109/IROS47612.2022.9981258</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Kyoto, Japan</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ross</foaf:surname>
                        <foaf:givenName>James</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mendez</foaf:surname>
                        <foaf:givenName>Oscar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saha</foaf:surname>
                        <foaf:givenName>Avishkar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Johnson</foaf:surname>
                        <foaf:givenName>Mark</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bowden</foaf:surname>
                        <foaf:givenName>Richard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_136"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Not read</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:subject>BEV</dc:subject>
        <dc:subject>SLAM</dc:subject>
        <dc:title>BEV-SLAM: Building a Globally-Consistent World Map Using Monocular Vision</dc:title>
        <dcterms:abstract>The ability to produce large-scale maps for navigation, path planning and other tasks is a crucial step for autonomous agents, but has always been challenging. In this work, we introduce BEV-SLAM, a novel type of graph-based SLAM that aligns semantically-segmented Bird’s Eye View (BEV) predictions from monocular cameras. We introduce a novel form of occlusion reasoning into BEV estimation and demonstrate its importance to aid spatial aggregation of BEV predictions. The result is a versatile SLAM system that can operate across arbitrary multi-camera configurations and can be seamlessly integrated with other sensors. We show that the use of multiple cameras significantly increases performance, and achieves lower relative error than high-performance GPS.</dcterms:abstract>
        <dc:date>2022-10-23</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>BEV-SLAM</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9981258/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:40:30</dcterms:dateSubmitted>
        <dc:rights>https://doi.org/10.15223/policy-029</dc:rights>
        <bib:pages>3830-3836</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_136">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://personalpages.surrey.ac.uk/r.bowden/publications/2022/Ross_IROS2022pp.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:40:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1007/s40747-022-00831-5">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2198-6053"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shang</foaf:surname>
                        <foaf:givenName>Zhexiong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Zhigang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_139"/>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Not read</dc:subject>
        <dc:title>Topology-based UAV path planning for multi-view stereo 3D reconstruction of complex structures</dc:title>
        <dcterms:abstract>This paper introduces a new UAV path planning method for creating high-quality 3D reconstruction models of large and complex structures. The core of the new method is incorporating the topology information of the surveyed 3D structure to decompose the multi-view stereo path planning into a collection of overlapped view optimization problems that can be processed in parallel. Different from the existing state-of-the-arts that recursively select the vantage camera views, the new method iteratively resamples all nearby cameras (i.e., positions/orientations) together and achieves a substantial reduction in computation cost while improving reconstruction quality. The new approach also provides a higher-level automation function that facilitates field implementations by eliminating the need for redundant camera initialization as in existing studies. Validations are provided by measuring the variance between the reconstructions to the ground truth models. Results from three synthetic case studies and one real-world application are presented to demonstrate the improved performance. The new method is expected to be instrumental in expanding the adoption of UAV-based multi-view stereo 3D reconstruction of large and complex structures.</dcterms:abstract>
        <dc:date>2023-02-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/s40747-022-00831-5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:41:23</dcterms:dateSubmitted>
        <bib:pages>909-926</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2198-6053">
        <prism:volume>9</prism:volume>
        <dc:title>Complex &amp; Intelligent Systems</dc:title>
        <dc:identifier>DOI 10.1007/s40747-022-00831-5</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Complex Intell. Syst.</dcterms:alternative>
        <dc:identifier>ISSN 2198-6053</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_139">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs40747-022-00831-5.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-19 13:41:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1007/s10514-017-9638-9">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1573-7527"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Huei-Yung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yao</foaf:surname>
                        <foaf:givenName>Chia-Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Kai-Sheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tran</foaf:surname>
                        <foaf:givenName>Van Luan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_141"/>
        <dc:subject>To read</dc:subject>
        <dc:title>Topological map construction and scene recognition for vehicle localization</dc:title>
        <dcterms:abstract>This paper presents a vehicle localization method to assist vehicle navigation based on topological map construction and scene recognition. A topological map is constructed using omni-directional image sequences, and the node information of the topological map is used for place recognition and derivation of vehicle location. In topological map construction and scene change detection, we utilize the Extended-HCT method for semantic description and feature extraction. Content-based and feature-based image retrieval approaches are adopted for place recognition and vehicle localization on the real scene image dataset. The proposed technique is able to construct a real-time image retrieval system for navigation assistance and validate the correctness of the route. Experiments are carried out in both the indoor and outdoor environments using real world images.</dcterms:abstract>
        <dc:date>2018-01-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/s10514-017-9638-9</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:25:28</dcterms:dateSubmitted>
        <bib:pages>65-81</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1573-7527">
        <prism:volume>42</prism:volume>
        <dc:title>Autonomous Robots</dc:title>
        <dc:identifier>DOI 10.1007/s10514-017-9638-9</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Auton Robot</dcterms:alternative>
        <dc:identifier>ISSN 1573-7527</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_141">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs10514-017-9638-9.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:25:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/10550628/?arnumber=10550628">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2024 International Conference on 3D Vision (3DV)</dc:title>
                <dc:identifier>DOI 10.1109/3DV62453.2024.00017</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sautier</foaf:surname>
                        <foaf:givenName>Corentin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Puy</foaf:surname>
                        <foaf:givenName>Gilles</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Boulch</foaf:surname>
                        <foaf:givenName>Alexandre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Marlet</foaf:surname>
                        <foaf:givenName>Renaud</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lepetit</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_144"/>
        <link:link rdf:resource="#item_143"/>
        <dc:subject>Not read</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds</dc:title>
        <dcterms:abstract>We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast [40], which uses a contrast at the level of points, to the state-of-the-art TARL [30], which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of implementation, it is surpassed by the latter, which however requires a costly pre-processing. In BEVContrast, we define our contrast at the level of 2D cells in the Bird’s Eye View plane. Resulting cell-level representations offer a good trade-off between the point-level representations exploited in PointContrast and segment-level representations exploited in TARL: we retain the simplicity of PointContrast (cell representations are cheap to compute) while surpassing the performance of TARL in downstream semantic segmentation. The code is available at github.com/valeoai/BEVContrast</dcterms:abstract>
        <dc:date>2024-03</dc:date>
        <z:shortTitle>BEVContrast</z:shortTitle>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10550628/?arnumber=10550628</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:27:02</dcterms:dateSubmitted>
        <dc:description>ISSN: 2475-7888</dc:description>
        <bib:pages>559-568</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2024 International Conference on 3D Vision (3DV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_144">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=10550628&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:27:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_143">
        <z:itemType>attachment</z:itemType>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/10550628/?arnumber=10550628</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:27:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_149">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>60</prism:volume>
                <dc:title>Robotics and Autonomous Systems</dc:title>
                <dc:identifier>DOI 10.1016/j.robot.2012.09.004</dc:identifier>
                <dcterms:alternative>Robotics and Autonomous Systems</dcterms:alternative>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scherer</foaf:surname>
                        <foaf:givenName>Sebastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chamberlain</foaf:surname>
                        <foaf:givenName>Lyle</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Singh</foaf:surname>
                        <foaf:givenName>Sanjiv</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_150"/>
        <link:link rdf:resource="#item_151"/>
        <dc:subject>Not read</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:title>Autonomous Landing at Unprepared Sites by a Full-Scale Helicopter</dc:title>
        <dcterms:abstract>Helicopters are valuable since they can land at unprepared sites; however, current unmanned helicopters are unable to select or validate landing zones (LZs) and approach paths. For operation in unknown terrain it is necessary to assess the safety of a LZ. In this paper, we describe a lidar-based perception system that enables a full-scale autonomous helicopter to identify and land in previously unmapped terrain with no human input.We describe the problem, real-time algorithms, perception hardware, and results. Our approach has extended the state of the art in terrain assessment by incorporating not only plane fitting, but by also considering factors such as terrain/skid interaction, rotor and tail clearance, wind direction, clear approach/abort paths, and ground paths.In results from urban and natural environments we were able to successfully classify LZs from point cloud maps. We also present results from 8 successful landing experiments with varying ground clutter and approach directions. The helicopter selected its own landing site, approaches, and then proceeds to land. To our knowledge, these experiments were the first demonstration of a full-scale autonomous helicopter that selected its own landing zones and landed.</dcterms:abstract>
        <dc:date>2012-12-01</dc:date>
        <z:libraryCatalog>ResearchGate</z:libraryCatalog>
        <bib:pages>1545–1562</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_150">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Sanjiv-Singh-2/publication/257343605_Autonomous_Landing_at_Unprepared_Sites_by_a_Full-Scale_Helicopter/links/5b88832f4585151fd13dc576/Autonomous-Landing-at-Unprepared-Sites-by-a-Full-Scale-Helicopter.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:28:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_151">
        <z:itemType>attachment</z:itemType>
        <dc:title>ResearchGate Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/publication/257343605_Autonomous_Landing_at_Unprepared_Sites_by_a_Full-Scale_Helicopter</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:28:31</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
    </z:Attachment>
    <bib:Article rdf:about="https://ora.ox.ac.uk/objects/uuid:0758dad0-6b33-40d1-bade-0cc2eb6f989a">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1050-4729"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vineet</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Miksik</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lidegaard</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nießner</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Golodetz</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Prisacariu</foaf:surname>
                        <foaf:givenName>V.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kähler</foaf:surname>
                        <foaf:givenName>O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Murray</foaf:surname>
                        <foaf:givenName>D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Izadi</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pérez</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Torr</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_156"/>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:subject>move_base_flex</dc:subject>
        <dc:subject>Dense</dc:subject>
        <dc:title>Incremental dense semantic stereo fusion for large-scale semantic scene reconstruction.</dc:title>
        <dcterms:abstract>Our abilities in scene understanding, which allow us to perceive the 3D structure of our surroundings and intuitively recognise the objects we see, are things that we largely take for granted, but for robots, the task of understanding large scenes quickly remains extremely challenging. Recently, scene understanding approaches based on 3D reconstruction and semantic segmentation have become popular, but existing methods either do not scale, fail outdoors, provide only sparse reconstructions or are rather slow. In this paper, we build on a recent hash-based technique for large-scale fusion and an efficient mean-field inference algorithm for densely-connected CRFs to present what to our knowledge is the first system that can perform dense, large-scale, outdoor semantic reconstruction of a scene in (near) real time. We also present a 'semantic fusion' approach that allows us to handle dynamic objects more effectively than previous approaches. We demonstrate the effectiveness of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense reconstruction and labelling of a number of scenes.</dcterms:abstract>
        <dc:date>2015</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ora.ox.ac.uk</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ora.ox.ac.uk/objects/uuid:0758dad0-6b33-40d1-bade-0cc2eb6f989a</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:29:52</dcterms:dateSubmitted>
        <dc:description>ISBN: 9781479969234
Publisher: Institute of Electrical and Electronics Engineers</dc:description>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1050-4729">
        <prism:volume>2015-June</prism:volume>
        <dc:title>ICRA 2015: IEEE International Conference on Robotics and Automation</dc:title>
        <prism:number>June</prism:number>
        <dc:identifier>ISSN 1050-4729</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_156">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ora.ox.ac.uk/objects/uuid:0758dad0-6b33-40d1-bade-0cc2eb6f989a/files/m7b65fa0d7751c7be336728e6cd6e6c08</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:29:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4799-3685-4">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-4799-3685-4</dc:identifier>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
                <dc:identifier>DOI 10.1109/ICRA.2014.6907233</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Hong Kong, China</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pizzoli</foaf:surname>
                        <foaf:givenName>Matia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Forster</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_157"/>
        <dc:subject>3D terrain reconstruction</dc:subject>
        <dc:subject>Ignored</dc:subject>
        <dc:subject>move_base_flex</dc:subject>
        <dc:subject>Dense</dc:subject>
        <dc:title>REMODE: Probabilistic, monocular dense reconstruction in real time</dc:title>
        <dcterms:abstract>In this paper, we solve the problem of estimating dense and accurate depth maps from a single moving camera. A probabilistic depth measurement is carried out in real time on a per-pixel basis and the computed uncertainty is used to reject erroneous estimations and provide live feedback on the reconstruction progress. Our contribution is a novel approach to depth map computation that combines Bayesian estimation and recent development on convex optimization for image processing. We demonstrate that our method outperforms stateof-the-art techniques in terms of accuracy, while exhibiting high efﬁciency in memory usage and computing power. We call our approach REMODE (REgularized MOnocular Depth Estimation) and the CUDA-based implementation runs at 30Hz on a laptop computer.</dcterms:abstract>
        <dc:date>5/2014</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>REMODE</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/6907233/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:31:06</dcterms:dateSubmitted>
        <bib:pages>2609-2616</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2014 IEEE International Conference on Robotics and Automation (ICRA)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_157">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://rpg.ifi.uzh.ch/docs/ICRA14_Pizzoli.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:31:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2008.05711">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Philion</foaf:surname>
                        <foaf:givenName>Jonah</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fidler</foaf:surname>
                        <foaf:givenName>Sanja</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_159"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:subject>UGV</dc:subject>
        <dc:subject>To read</dc:subject>
        <dc:subject>BEV</dc:subject>
        <dc:title>Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</dc:title>
        <dcterms:abstract>The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eyeview grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’seye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eyeview cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot.</dcterms:abstract>
        <dc:date>2020-08-13</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Lift, Splat, Shoot</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2008.05711</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:32:23</dcterms:dateSubmitted>
        <dc:description>arXiv:2008.05711 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2008.05711</dc:identifier>
        <prism:number>arXiv:2008.05711</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_159">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2008.05711</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:32:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2202.11271">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>Robotics: Science and Systems XVIII</dc:title>
                <dc:identifier>DOI 10.15607/RSS.2022.XVIII.019</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shah</foaf:surname>
                        <foaf:givenName>Dhruv</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levine</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_282"/>
        <link:link rdf:resource="#item_162"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>Local path</dc:subject>
        <dc:subject>Global path</dc:subject>
        <dc:subject>Vision based navigation</dc:subject>
        <dc:title>ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints</dc:title>
        <dcterms:abstract>Robotic navigation has been approached as a problem of 3D reconstruction and planning, as well as an end-to-end learning problem. However, long-range navigation requires both planning and reasoning about local traversability, as well as being able to utilize general knowledge about global geography, in the form of a roadmap, GPS, or other side information providing important cues. In this work, we propose an approach that integrates learning and planning, and can utilize side information such as schematic roadmaps, satellite maps and GPS coordinates as a planning heuristic, without relying on them being accurate. Our method, ViKiNG, incorporates a local traversability model, which looks at the robot's current camera observation and a potential subgoal to infer how easily that subgoal can be reached, as well as a heuristic model, which looks at overhead maps for hints and attempts to evaluate the appropriateness of these subgoals in order to reach the goal. These models are used by a heuristic planner to identify the best waypoint in order to reach the final destination. Our method performs no explicit geometric reconstruction, utilizing only a topological representation of the environment. Despite having never seen trajectories longer than 80 meters in its training dataset, ViKiNG can leverage its image-based learned controller and goal-directed heuristic to navigate to goals up to 3 kilometers away in previously unseen environments, and exhibit complex behaviors such as probing potential paths and backtracking when they are found to be non-viable. ViKiNG is also robust to unreliable maps and GPS, since the low-level controller ultimately makes decisions based on egocentric image observations, using maps only as planning heuristics. For videos of our experiments, please check out our project page https://sites.google.com/view/viking-release.</dcterms:abstract>
        <dc:date>2022-06-27</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>ViKiNG</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2202.11271</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:33:44</dcterms:dateSubmitted>
        <dc:description>arXiv:2202.11271 [cs]</dc:description>
    </rdf:Description>
    <z:Attachment rdf:about="#item_282">
        <z:itemType>attachment</z:itemType>
        <dc:title>Extension</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://github.com/UT-ADL/milrem_visual_offroad_navigation</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-04-22 07:48:42</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
    </z:Attachment>
    <z:Attachment rdf:about="#item_162">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2202.11271</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:33:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.mdpi.com/2504-446X/6/7/183">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2504-446X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Qi</foaf:surname>
                        <foaf:givenName>Yao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Rendong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Binbing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Youchun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_166"/>
        <dc:subject>To read</dc:subject>
        <dc:subject>BEV</dc:subject>
        <dc:subject>Topological Map</dc:subject>
        <dc:title>Compact and Efficient Topological Mapping for Large-Scale Environment with Pruned Voronoi Diagram</dc:title>
        <dcterms:abstract>Topological maps generated in complex and irregular unknown environments are meaningful for autonomous robots’ navigation. To obtain the skeleton of the environment without obstacle polygon extraction and clustering, we propose a method to obtain high-quality topological maps using only pure Voronoi diagrams in three steps. Supported by Voronoi vertex’s property of the largest empty circle, the method updates the global topological map incrementally in both dynamic and static environments online. The incremental method can be adapted to any fundamental Voronoi diagram generator. We maintain the entire space by two graphs, the pruned Voronoi graph for incremental updates and the reduced approximated generalized Voronoi graph for routing planning requests. We present an extensive benchmark and real-world experiment, and our method completes the environment representation in both indoor and outdoor areas. The proposed method generates a compact topological map in both small- and large-scale scenarios, which is defined as the total length and vertices of topological maps. Additionally, our method has been shortened by several orders of magnitude in terms of the total length and consumes less than 30% of the average time cost compared to state-of-the-art methods.</dcterms:abstract>
        <dc:date>2022/7</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>www.mdpi.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.mdpi.com/2504-446X/6/7/183</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:37:21</dcterms:dateSubmitted>
        <dc:rights>http://creativecommons.org/licenses/by/3.0/</dc:rights>
        <dc:description>Number: 7
Publisher: Multidisciplinary Digital Publishing Institute</dc:description>
        <bib:pages>183</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2504-446X">
        <prism:volume>6</prism:volume>
        <dc:title>Drones</dc:title>
        <dc:identifier>DOI 10.3390/drones6070183</dc:identifier>
        <prism:number>7</prism:number>
        <dc:identifier>ISSN 2504-446X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_166">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.mdpi.com/2504-446X/6/7/183/pdf?version=1658456565</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:37:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0140366419307868">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0140-3664"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>Tao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Almeida</foaf:surname>
                        <foaf:givenName>Jefferson S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>da Silva</foaf:surname>
                        <foaf:givenName>Suane Pires P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Filho</foaf:surname>
                        <foaf:givenName>Paulo Honório</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Oliveira Rodrigues</foaf:surname>
                        <foaf:givenName>Antonio W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Albuquerque</foaf:surname>
                        <foaf:givenName>Victor Hugo C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rebouças Filho</foaf:surname>
                        <foaf:givenName>Pedro P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_171"/>
        <link:link rdf:resource="#item_170"/>
        <dc:subject>Topological Map</dc:subject>
        <dc:title>An effective approach to unmanned aerial vehicle navigation using visual topological map in outdoor and indoor environments</dc:title>
        <dcterms:abstract>Unmanned Aerial Vehicles are constantly being using in professional activities that require higher precision in navigating and positioning the aircraft during operation. Advanced location technologies such as Global Navigation Satellite System and Real-Time Kinematic are widely used, however, they depend on an area with transmission coverage. In this approach, this article presents a visual navigation methodology based on topological maps. We compared the performance of consolidated classifiers such as Bayesian classifier, k-nearest neighbor, Multilayer Perceptron, Optimal Path Forest and Support Vector Machines (SVM). They are evaluated with attributes returned by last generation resource extractors such as Fourier, Gray Level Co-Occurrence and Local Binary Patterns (LBP). After analyzing the results we found that the combination of LBP and SVM obtained the best values in the evaluation metrics considered, among them, 99.99% Specificity and 99.98% Precision in the navigation process. SVM reached 5.49787 s in combination with LBP completes the training in 5.49787 s. Concerning the testing time, SVM achieving 80.91 ms in association with LBP.</dcterms:abstract>
        <dc:date>2020-01-15</dc:date>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0140366419307868</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:40:39</dcterms:dateSubmitted>
        <bib:pages>696-702</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0140-3664">
        <prism:volume>150</prism:volume>
        <dc:title>Computer Communications</dc:title>
        <dc:identifier>DOI 10.1016/j.comcom.2019.12.026</dc:identifier>
        <dcterms:alternative>Computer Communications</dcterms:alternative>
        <dc:identifier>ISSN 0140-3664</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_171">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_170">
        <z:itemType>attachment</z:itemType>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0140366419307868</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:40:46</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_173">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stentz</foaf:surname>
                        <foaf:givenName>Anthony</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_172"/>
        <dc:relation rdf:resource="http://arxiv.org/abs/2004.04697"/>
        <dc:subject>Ignored</dc:subject>
        <dc:title>The Focussed D* Algorithm for Real-Time Replanning</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_172">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://robby.caltech.edu/~jwb/courses/ME132/handouts/Dstar_ijcai95.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:43:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.mdpi.com/1424-8220/13/1/1247">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1424-8220"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Garzón</foaf:surname>
                        <foaf:givenName>Mario</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Valente</foaf:surname>
                        <foaf:givenName>João</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zapata</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barrientos</foaf:surname>
                        <foaf:givenName>Antonio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_175"/>
        <dc:relation rdf:resource="http://arxiv.org/abs/2004.04697"/>
        <dc:subject>To read</dc:subject>
        <dc:title>An Aerial-Ground Robotic System for Navigation and Obstacle Mapping in Large Outdoor Areas</dc:title>
        <dcterms:abstract>There are many outdoor robotic applications where a robot must reach a goal position or explore an area without previous knowledge of the environment around it. Additionally, other applications (like path planning) require the use of known maps or previous information of the environment. This work presents a system composed by a terrestrial and an aerial robot that cooperate and share sensor information in order to address those requirements. The ground robot is able to navigate in an unknown large environment aided by visual feedback from a camera on board the aerial robot. At the same time, the obstacles are mapped in real-time by putting together the information from the camera and the positioning system of the ground robot. A set of experiments were carried out with the purpose of verifying the system applicability. The experiments were performed in a simulation environment and outdoor with a medium-sized ground robot and a mini quad-rotor. The proposed robotic system shows outstanding results in simultaneous navigation and mapping applications in large outdoor environments.</dcterms:abstract>
        <dc:date>2013/1</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>www.mdpi.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.mdpi.com/1424-8220/13/1/1247</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:43:56</dcterms:dateSubmitted>
        <dc:rights>http://creativecommons.org/licenses/by/3.0/</dc:rights>
        <dc:description>Number: 1
Publisher: Multidisciplinary Digital Publishing Institute</dc:description>
        <bib:pages>1247-1267</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1424-8220">
        <prism:volume>13</prism:volume>
        <dc:title>Sensors</dc:title>
        <dc:identifier>DOI 10.3390/s130101247</dc:identifier>
        <prism:number>1</prism:number>
        <dc:identifier>ISSN 1424-8220</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_175">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.mdpi.com/1424-8220/13/1/1247/pdf?version=1403322339</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-02-21 17:43:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://joss.theoj.org/papers/10.21105/joss.02172">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bosch</foaf:surname>
                        <foaf:givenName>Martí</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_281"/>
        <dc:subject>Terrain classification</dc:subject>
        <dc:title>Journal of Open Source Software: DetecTree: Tree detection from aerial imagery in Python</dc:title>
        <dc:date>2020</dc:date>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://joss.theoj.org/papers/10.21105/joss.02172</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-03-04 08:32:01</dcterms:dateSubmitted>
        <dc:description>https://github.com/martibosch/detectree</dc:description>
    </bib:Document>
    <z:Attachment rdf:about="#item_281">
        <z:itemType>attachment</z:itemType>
        <dc:title>Journal of Open Source Software: DetecTree: Tree detection from aerial imagery in Python</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://joss.theoj.org/papers/10.21105/joss.02172</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-03-04 08:32:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.mdpi.com/2224-2708/13/6/81">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2224-2708"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Munasinghe</foaf:surname>
                        <foaf:givenName>Isuru</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Perera</foaf:surname>
                        <foaf:givenName>Asanka</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deo</foaf:surname>
                        <foaf:givenName>Ravinesh C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_291"/>
        <dc:subject>UGV</dc:subject>
        <dc:subject>UAV</dc:subject>
        <dc:subject>Collaboration</dc:subject>
        <dc:subject>Not read</dc:subject>
        <dc:subject>To read</dc:subject>
        <dc:title>A Comprehensive Review of UAV-UGV Collaboration: Advancements and Challenges</dc:title>
        <dcterms:abstract>Unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) have rapidly evolved, becoming integral to various applications such as environmental monitoring, disaster response, and precision agriculture. This paper provides a comprehensive review of the advancements and the challenges in UAV-UGV collaboration and its potential applications. These systems offer enhanced situational awareness and operational efficiency, enabling complex tasks that are beyond the capabilities of individual systems by leveraging the complementary strengths of UAVs and UGVs. Key areas explored in this review include multi-UAV and multi-UGV systems, collaborative aerial and ground operations, and the communication and coordination mechanisms that support these collaborative efforts. Furthermore, this paper discusses potential limitations, challenges and future research directions, and considers issues such as computational constraints, communication network instability, and environmental adaptability. The review also provides a detailed analysis of how these issues impact the effectiveness of UAV-UGV collaboration.</dcterms:abstract>
        <dc:date>2024-11-28</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>A Comprehensive Review of UAV-UGV Collaboration</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.mdpi.com/2224-2708/13/6/81</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-05-14 13:49:39</dcterms:dateSubmitted>
        <dc:rights>https://creativecommons.org/licenses/by/4.0/</dc:rights>
        <bib:pages>81</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2224-2708">
        <prism:volume>13</prism:volume>
        <dc:title>Journal of Sensor and Actuator Networks</dc:title>
        <dc:identifier>DOI 10.3390/jsan13060081</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>JSAN</dcterms:alternative>
        <dc:identifier>ISSN 2224-2708</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_291">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.mdpi.com/2224-2708/13/6/81/pdf?version=1732788319</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-05-14 13:51:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/9363693">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>IEEE Std 802.11-2020 (Revision of IEEE Std 802.11-2016)</dc:title>
                <dc:identifier>DOI 10.1109/IEEESTD.2021.9363693</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_295"/>
        <dc:title>IEEE Standard for Information Technology–Telecommunications and Information Exchange between Systems - Local and Metropolitan Area Networks–Specific Requirements - Part 11: Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specifications</dc:title>
        <dcterms:abstract>Technical corrections and clarifications to IEEE Std 802.11 for wireless local area networks (WLANs) as well as enhancements to the existing medium access control (MAC) and physical layer (PHY) functions are specified in this revision. Amendments 1 to 5 published in 2016 and 2018 have also been incorporated into this revision.</dcterms:abstract>
        <dc:date>2021-02</dc:date>
        <z:shortTitle>IEEE Standard for Information Technology–Telecommunications and Information Exchange between Systems - Local and Metropolitan Area Networks–Specific Requirements - Part 11</z:shortTitle>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://ieeexplore.ieee.org/document/9363693</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-16 16:27:22</dcterms:dateSubmitted>
        <bib:pages>1-4379</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_295">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=9363693&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-16 16:28:48</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="urn:isbn:978-3-319-50114-7%20978-3-319-50115-4">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>1</prism:volume>
                <dc:identifier>ISBN 978-3-319-50114-7 978-3-319-50115-4</dc:identifier>
                <dc:title>2016 International Symposium on Experimental Robotics</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kulić</foaf:surname>
                        <foaf:givenName>Dana</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nakamura</foaf:surname>
                        <foaf:givenName>Yoshihiko</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khatib</foaf:surname>
                        <foaf:givenName>Oussama</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Venture</foaf:surname>
                        <foaf:givenName>Gentiane</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Delmerico</foaf:surname>
                        <foaf:givenName>Jeffrey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Giusti</foaf:surname>
                        <foaf:givenName>Alessandro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mueggler</foaf:surname>
                        <foaf:givenName>Elias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gambardella</foaf:surname>
                        <foaf:givenName>Luca Maria</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Scaramuzza</foaf:surname>
                        <foaf:givenName>Davide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_300"/>
        <dc:title>“On-the-Spot Training” for Terrain Classification in Autonomous Air-Ground Collaborative Teams</dc:title>
        <dcterms:abstract>We consider the problem of performing rapid training of a terrain classiﬁer in the context of a collaborative robotic search and rescue system. Our system uses a vision-based ﬂying robot to guide a ground robot through unknown terrain to a goal location by building a map of terrain class and elevation. However, due to the unknown environments present in search and rescue scenarios, our system requires a terrain classiﬁer that can be trained and deployed quickly, based on data collected on the spot. We investigate the relationship of training set size and complexity on training time and accuracy, for both feature-based and convolutional neural network classiﬁers in this scenario. Our goal is to minimize the deployment time of the classiﬁer in our terrain mapping system within acceptable classiﬁcation accuracy tolerances. So we are not concerned with training a classiﬁer that generalizes well, only one that works well for this particular environment. We demonstrate that we can launch our aerial robot, gather data, train a classiﬁer, and begin building a terrain map after only 60 seconds of ﬂight.</dcterms:abstract>
        <dc:date>2017</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-319-50115-4_50</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-18 12:52:27</dcterms:dateSubmitted>
        <dc:rights>http://www.springer.com/tdm</dc:rights>
        <dc:description>Series Title: Springer Proceedings in Advanced Robotics
DOI: 10.1007/978-3-319-50115-4_50</dc:description>
        <bib:pages>574-585</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_300">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://rpg.ifi.uzh.ch/docs/ISER16_Delmerico.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-18 12:52:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_309">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Zhenping</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yang</foaf:surname>
                        <foaf:givenName>Chuang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bu</foaf:surname>
                        <foaf:givenName>Yafeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Bokai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zeng</foaf:surname>
                        <foaf:givenName>Jun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Xiaohui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_307"/>
        <dc:subject>To read</dc:subject>
        <dc:title>Road Similarity-Based BEV-Satellite Image Matching for UGV Localization</dc:title>
        <dcterms:abstract>To address the challenge of autonomous UGV localization in GNSS-denied off-road environments, this study proposes a matching-based localization method that leverages BEV perception image and satellite map within a road similarity space to achieve high-precision positioning.We first implement a robust LiDAR-inertial odometry system, followed by the fusion of LiDAR and image data to generate a local BEV perception image of the UGV. This approach mitigates the significant viewpoint discrepancy between ground-view images and satellite map. The BEV image and satellite map are then projected into the road similarity space, where normalized cross correlation (NCC) is computed to assess the matching score.Finally, a particle filter is employed to estimate the probability distribution of the vehicle’s pose.By comparing with GNSS ground truth, our localization system demonstrated stability without divergence over a long-distance test of 10 km, achieving an average lateral error of only 0.89 meters and an average planar Euclidean error of 3.41 meters. Furthermore, it maintained accurate and stable global localization even under nighttime conditions, further validating its robustness and adaptability.</dcterms:abstract>
        <dc:date>04/2025</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_307">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2504.16346</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-18 13:48:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2506.05250">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Zhiyun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Dongmyeong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Adkins</foaf:surname>
                        <foaf:givenName>Amanda</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Quattrociocchi</foaf:surname>
                        <foaf:givenName>Jesse</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ellis</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Biswas</foaf:surname>
                        <foaf:givenName>Joydeep</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_308"/>
        <dc:subject>To read</dc:subject>
        <dc:title>Spatiotemporal Contrastive Learning for Cross-View Video Localization in Unstructured Off-road Terrains</dc:title>
        <dcterms:abstract>Robust cross-view 3-DoF localization in GPS-denied, off-road environments remains a fundamental challenge due to two key factors: (1) perceptual ambiguities arising from repetitive vegetation and unstructured terrain that lack distinctive visual features, and (2) seasonal appearance shifts that alter scene characteristics well beyond chromatic variation, making it difficult to match current ground-view observations to outdated satellite imagery. To address these challenges, we introduce MoViX, a selfsupervised cross-view video localization framework that learns representations robust to viewpoint and seasonal variation, while preserving directional awareness critical for accurate localization. MoViX employs a pose-dependent positive sampling strategy to enhance directional discrimination and temporally aligned hard negative mining to discourage shortcut learning from seasonal appearance cues. A motion-informed frame sampler selects spatially diverse video frames, and a lightweight, quality-aware temporal aggregator prioritizes frames with strong geometric alignment while downweighting ambiguous ones. At inference, MoViX operates within a Monte Carlo Localization framework, replacing traditional handcrafted measurement models with a learned cross-view neural matching module. Belief updates are modulated through entropy-guided temperature scaling, allowing the filter to maintain multiple pose hypotheses under visual ambiguity and converge confidently when reliable evidence is observed. We evaluate MoViX on the TartanDrive 2.0 dataset, training on less than 30 minutes of driving data and testing over 12.29 km of trajectories. Despite using temporally mismatched satellite imagery, MoViX localizes within 25 meters of ground truth for 93% of the time and within 50 meters for 100% in unseen regions, outperforming state-of-the-art baselines without environment-specific tuning. We also showcase its generalization on a real-world off-road dataset collected in a geographically distinct location with a different robot platform. The code will be made publicly available upon publication. A demonstration video is available at: https://youtu.be/y5wL8nUEuH0.</dcterms:abstract>
        <dc:date>2025-06-05</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2506.05250</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-18 13:48:32</dcterms:dateSubmitted>
        <dc:description>arXiv:2506.05250 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2506.05250</dc:identifier>
        <prism:number>arXiv:2506.05250</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_308">
        <z:itemType>attachment</z:itemType>
        <dc:title>PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2506.05250</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-18 13:48:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2503.11651">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Jianyuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Minghao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karaev</foaf:surname>
                        <foaf:givenName>Nikita</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vedaldi</foaf:surname>
                        <foaf:givenName>Andrea</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rupprecht</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Novotny</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_314"/>
        <link:link rdf:resource="#item_313"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>VGGT: Visual Geometry Grounded Transformer</dc:title>
        <dcterms:abstract>We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.</dcterms:abstract>
        <dc:date>2025-03-14</dc:date>
        <z:shortTitle>VGGT</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2503.11651</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-18 14:02:13</dcterms:dateSubmitted>
        <dc:description>arXiv:2503.11651 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2503.11651</dc:identifier>
        <prism:number>arXiv:2503.11651</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_314">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2503.11651v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-18 14:02:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_313">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2503.11651</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-18 14:02:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Data rdf:about="https://github.com/Whojo/Spatially-Coherent-Costmap">
        <z:itemType>computerProgram</z:itemType>
        <z:programmers>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>THOMAS</foaf:surname>
                        <foaf:givenName>Guillaume</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </z:programmers>
        <dc:title>Whojo/Spatially-Coherent-Costmap</dc:title>
        <dc:date>2024-08-14T08:31:17Z</dc:date>
        <z:libraryCatalog>GitHub</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://github.com/Whojo/Spatially-Coherent-Costmap</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 10:11:24</dcterms:dateSubmitted>
        <dc:rights>MIT</dc:rights>
        <dc:description>original-date: 2023-09-19T07:35:28Z</dc:description>
        <z:programmingLanguage>Jupyter Notebook</z:programmingLanguage>
    </bib:Data>
    <bib:Document rdf:about="https://arxiv.org/abs/2204.12876v1">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>arXiv.org</dc:title></z:Website>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Miki</foaf:surname>
                        <foaf:givenName>Takahiro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wellhausen</foaf:surname>
                        <foaf:givenName>Lorenz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grandia</foaf:surname>
                        <foaf:givenName>Ruben</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jenelten</foaf:surname>
                        <foaf:givenName>Fabian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Homberger</foaf:surname>
                        <foaf:givenName>Timon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hutter</foaf:surname>
                        <foaf:givenName>Marco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_319"/>
        <dc:title>Elevation Mapping for Locomotion and Navigation using GPU</dc:title>
        <dcterms:abstract>Perceiving the surrounding environment is crucial for autonomous mobile robots. An elevation map provides a memory-efficient and simple yet powerful geometric representation for ground robots. The robots can use this information for navigation in an unknown environment or perceptive locomotion control over rough terrain. Depending on the application, various post processing steps may be incorporated, such as smoothing, inpainting or plane segmentation. In this work, we present an elevation mapping pipeline leveraging GPU for fast and efficient processing with additional features both for navigation and locomotion. We demonstrated our mapping framework through extensive hardware experiments. Our mapping software was successfully deployed for underground exploration during DARPA Subterranean Challenge and for various experiments of quadrupedal locomotion.</dcterms:abstract>
        <dc:date>2022/04/27</dc:date>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2204.12876v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 10:28:07</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_319">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2204.12876</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 10:28:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="http://wiki.ros.org/teb_local_planner">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_321"/>
        <dc:title>teb_local_planner - ROS Wiki</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://wiki.ros.org/teb_local_planner</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 10:34:21</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_321">
        <z:itemType>attachment</z:itemType>
        <dc:title>teb_local_planner - ROS Wiki</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://wiki.ros.org/teb_local_planner</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 10:34:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="http://wiki.ros.org/carrot_planner">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_323"/>
        <dc:title>carrot_planner - ROS Wiki</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://wiki.ros.org/carrot_planner</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 10:35:13</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_323">
        <z:itemType>attachment</z:itemType>
        <dc:title>carrot_planner - ROS Wiki</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://wiki.ros.org/carrot_planner</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 10:35:17</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://hub.dronedb.app/r/odm/aukerman">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_325"/>
        <dc:title>aukerman - DroneDB</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://hub.dronedb.app/r/odm/aukerman</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 11:38:21</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_325">
        <z:itemType>attachment</z:itemType>
        <dc:title>aukerman - DroneDB</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://hub.dronedb.app/r/odm/aukerman</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 11:38:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://arxiv.org/abs/1505.04597v1">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>arXiv.org</dc:title></z:Website>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ronneberger</foaf:surname>
                        <foaf:givenName>Olaf</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fischer</foaf:surname>
                        <foaf:givenName>Philipp</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brox</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_327"/>
        <dc:title>U-Net: Convolutional Networks for Biomedical Image Segmentation</dc:title>
        <dcterms:abstract>There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .</dcterms:abstract>
        <dc:date>2015/05/18</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>U-Net</z:shortTitle>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1505.04597v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 12:25:17</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_327">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/1505.04597</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-19 12:25:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-4503-4005-2">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dcterms:isPartOf>
                   <bib:Series><dc:title>LLVM '15</dc:title></bib:Series>
                </dcterms:isPartOf>
                <dc:identifier>ISBN 978-1-4503-4005-2</dc:identifier>
                <dc:title>Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC</dc:title>
                <dc:identifier>DOI 10.1145/2833157.2833162</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New York, NY, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Association for Computing Machinery</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lam</foaf:surname>
                        <foaf:givenName>Siu Kwan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pitrou</foaf:surname>
                        <foaf:givenName>Antoine</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Seibert</foaf:surname>
                        <foaf:givenName>Stanley</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_330"/>
        <dc:title>Numba: a LLVM-based Python JIT compiler</dc:title>
        <dcterms:abstract>Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].</dcterms:abstract>
        <dc:date>novembre 15, 2015</dc:date>
        <z:shortTitle>Numba</z:shortTitle>
        <z:libraryCatalog>ACM Digital Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/10.1145/2833157.2833162</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-24</dcterms:dateSubmitted>
        <bib:pages>1–6</bib:pages>
    </rdf:Description>
    <z:Attachment rdf:about="#item_330">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/2833157.2833162</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-24 08:00:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/2206.06435">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2755-2721,%202755-273X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bai</foaf:surname>
                        <foaf:givenName>Hao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_334"/>
        <link:link rdf:resource="#item_333"/>
        <dc:title>ICP Algorithm: Theory, Practice And Its SLAM-oriented Taxonomy</dc:title>
        <dcterms:abstract>The Iterative Closest Point (ICP) algorithm is one of the most important algorithms for geometric alignment of three-dimensional surface registration, which is frequently used in computer vision tasks, including the Simultaneous Localization And Mapping (SLAM) tasks. In this paper, we illustrate the theoretical principles of the ICP algorithm, how it can be used in surface registration tasks, and the traditional taxonomy of the variants of the ICP algorithm. As SLAM is becoming a popular topic, we also introduce a SLAM-oriented taxonomy of the ICP algorithm, based on the characteristics of each type of SLAM task, including whether the SLAM task is online or not and whether the landmarks are present as features in the SLAM task. We make a synthesis of each type of SLAM task by comparing several up-to-date research papers and analyzing their implementation details.</dcterms:abstract>
        <dc:date>2023-3-22</dc:date>
        <z:shortTitle>ICP Algorithm</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2206.06435</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-24 10:10:57</dcterms:dateSubmitted>
        <dc:description>arXiv:2206.06435 [cs]</dc:description>
        <bib:pages>10-21</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2755-2721,%202755-273X">
        <prism:volume>2</prism:volume>
        <dc:title>Applied and Computational Engineering</dc:title>
        <dc:identifier>DOI 10.54254/2755-2721/2/20220512</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>ACE</dcterms:alternative>
        <dc:identifier>ISSN 2755-2721, 2755-273X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_334">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2206.06435v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-24 10:10:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_333">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2206.06435</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-24 10:10:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ieeexplore.ieee.org/document/121791/citations">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1939-3539"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Besl</foaf:surname>
                        <foaf:givenName>P.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McKay</foaf:surname>
                        <foaf:givenName>Neil D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_336"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Convergence</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Inspection</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Iterative algorithms</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Iterative closest point algorithm</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Iterative methods</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Motion estimation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Quaternions</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Shape measurement</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Solid modeling</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Testing</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>A method for registration of 3-D shapes</dc:title>
        <dcterms:abstract>The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.&lt;&gt;</dcterms:abstract>
        <dc:date>1992-02</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/121791/citations</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-24 10:18:05</dcterms:dateSubmitted>
        <bib:pages>239-256</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1939-3539">
        <prism:volume>14</prism:volume>
        <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
        <dc:identifier>DOI 10.1109/34.121791</dc:identifier>
        <prism:number>2</prism:number>
        <dc:identifier>ISSN 1939-3539</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_336">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=121791&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-24 10:18:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2405.12979">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Hanwen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karpur</foaf:surname>
                        <foaf:givenName>Arjun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cao</foaf:surname>
                        <foaf:givenName>Bingyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Qixing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Araujo</foaf:surname>
                        <foaf:givenName>Andre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_340"/>
        <link:link rdf:resource="#item_339"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</dc:title>
        <dcterms:abstract>The image matching field has been witnessing a continuous emergence of novel learnable feature matching techniques, with ever-improving performance on conventional benchmarks. However, our investigation shows that despite these gains, their potential for real-world applications is restricted by their limited generalization capabilities to novel image domains. In this paper, we introduce OmniGlue, the first learnable image matcher that is designed with generalization as a core principle. OmniGlue leverages broad knowledge from a vision foundation model to guide the feature matching process, boosting generalization to domains not seen at training time. Additionally, we propose a novel keypoint position-guided attention mechanism which disentangles spatial and appearance information, leading to enhanced matching descriptors. We perform comprehensive experiments on a suite of $7$ datasets with varied image domains, including scene-level, object-centric and aerial images. OmniGlue's novel components lead to relative gains on unseen domains of $20.9\%$ with respect to a directly comparable reference model, while also outperforming the recent LightGlue method by $9.5\%$ relatively.Code and model can be found at https://hwjiang1510.github.io/OmniGlue</dcterms:abstract>
        <dc:date>2024-05-21</dc:date>
        <z:shortTitle>OmniGlue</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2405.12979</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-25 07:28:39</dcterms:dateSubmitted>
        <dc:description>arXiv:2405.12979 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2405.12979</dc:identifier>
        <prism:number>arXiv:2405.12979</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_340">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2405.12979v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-25 07:28:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_339">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2405.12979</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-25 07:28:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2404.19174">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Potje</foaf:surname>
                        <foaf:givenName>Guilherme</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cadar</foaf:surname>
                        <foaf:givenName>Felipe</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Araujo</foaf:surname>
                        <foaf:givenName>Andre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Martins</foaf:surname>
                        <foaf:givenName>Renato</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nascimento</foaf:surname>
                        <foaf:givenName>Erickson R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_344"/>
        <link:link rdf:resource="#item_343"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>XFeat: Accelerated Features for Lightweight Image Matching</dc:title>
        <dcterms:abstract>We introduce a lightweight and accurate architecture for resource-efficient visual correspondence. Our method, dubbed XFeat (Accelerated Features), revisits fundamental design choices in convolutional neural networks for detecting, extracting, and matching local features. Our new model satisfies a critical need for fast and robust algorithms suitable to resource-limited devices. In particular, accurate image matching requires sufficiently large image resolutions - for this reason, we keep the resolution as large as possible while limiting the number of channels in the network. Besides, our model is designed to offer the choice of matching at the sparse or semi-dense levels, each of which may be more suitable for different downstream applications, such as visual navigation and augmented reality. Our model is the first to offer semi-dense matching efficiently, leveraging a novel match refinement module that relies on coarse local descriptors. XFeat is versatile and hardware-independent, surpassing current deep learning-based local features in speed (up to 5x faster) with comparable or better accuracy, proven in pose estimation and visual localization. We showcase it running in real-time on an inexpensive laptop CPU without specialized hardware optimizations. Code and weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24.</dcterms:abstract>
        <dc:date>2024-04-30</dc:date>
        <z:shortTitle>XFeat</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2404.19174</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-25 07:28:58</dcterms:dateSubmitted>
        <dc:description>arXiv:2404.19174 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2404.19174</dc:identifier>
        <prism:number>arXiv:2404.19174</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_344">
        <z:itemType>attachment</z:itemType>
        <dc:title>Preprint PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/pdf/2404.19174v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-25 07:29:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_343">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2404.19174</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-25 07:28:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://dl.acm.org/doi/10.1145/358669.358692">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0001-0782"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fischler</foaf:surname>
                        <foaf:givenName>Martin A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bolles</foaf:surname>
                        <foaf:givenName>Robert C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_346"/>
        <dc:title>Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</dc:title>
        <dcterms:abstract>A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing</dcterms:abstract>
        <dc:date>juin 1, 1981</dc:date>
        <z:shortTitle>Random sample consensus</z:shortTitle>
        <z:libraryCatalog>ACM Digital Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://dl.acm.org/doi/10.1145/358669.358692</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-29 13:51:17</dcterms:dateSubmitted>
        <bib:pages>381–395</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0001-0782">
        <prism:volume>24</prism:volume>
        <dc:title>Commun. ACM</dc:title>
        <dc:identifier>DOI 10.1145/358669.358692</dc:identifier>
        <prism:number>6</prism:number>
        <dc:identifier>ISSN 0001-0782</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_346">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/358669.358692</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-29 13:51:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="http://www.drdobbs.com/open-source/the-opencv-library/184404319">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>Dr. Dobb's</dc:title></z:Website>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bradski</foaf:surname>
                        <foaf:givenName>Gary</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_348"/>
        <dc:title>The OpenCV Library</dc:title>
        <dcterms:abstract>OpenCV is an open-source, computer-vision library for extracting and processing meaningful data from images.</dcterms:abstract>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.drdobbs.com/open-source/the-opencv-library/184404319</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-29 15:42:34</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_348">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.drdobbs.com/open-source/the-opencv-library/184404319</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-06-29 15:42:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Data rdf:about="https://github.com/GStreamer/gstreamer">
        <z:itemType>computerProgram</z:itemType>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>GStreamer GitHub mirrors</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <dc:title>GStreamer/gstreamer</dc:title>
        <dcterms:abstract>GStreamer open-source multimedia framework</dcterms:abstract>
        <dc:date>2025-07-02T07:43:45Z</dc:date>
        <z:libraryCatalog>GitHub</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://github.com/GStreamer/gstreamer</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-02 08:12:32</dcterms:dateSubmitted>
        <dc:description>original-date: 2015-12-01T21:49:40Z</dc:description>
        <z:programmingLanguage>C</z:programmingLanguage>
    </bib:Data>
    <bib:Document rdf:about="https://www.shark-robotics.com/robots/barakuda-mule-robot">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_357"/>
        <dc:title>Barakuda mule robot for security | Shark Robotics</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.shark-robotics.com/robots/barakuda-mule-robot</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:26:46</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_357">
        <z:itemType>attachment</z:itemType>
        <dc:title>Barakuda mule robot for security | Shark Robotics</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.shark-robotics.com/robots/barakuda-mule-robot</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:26:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://www.hexadrone.fr/produits/drone-tundra/">
        <z:itemType>blogPost</z:itemType>
        <dcterms:isPartOf>
           <z:Blog><dc:title>Hexadrone</dc:title></z:Blog>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_359"/>
        <dc:title>Le TUNDRA 2</dc:title>
        <dcterms:abstract>Le TUNDRA 2 est un drone multirotor, véritable plateforme porte-outils, ultra modulable, multimétiers et plug &amp; play.</dcterms:abstract>
        <z:language>fr-FR</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.hexadrone.fr/produits/drone-tundra/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:27:05</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_359">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.hexadrone.fr/produits/drone-tundra/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:27:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://clearpathrobotics.com/husky-a300-unmanned-ground-vehicle-robot/">
        <z:itemType>blogPost</z:itemType>
        <dcterms:isPartOf>
           <z:Blog><dc:title>Clearpath Robotics</dc:title></z:Blog>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_363"/>
        <dc:title>Husky A300</dc:title>
        <dcterms:abstract>Husky A300 is a rugged, customizable and open-source mobile robotic platform that accelerates robotics research and streamlines the development of robotic solutions for commerical deployment.</dcterms:abstract>
        <z:language>en-US</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://clearpathrobotics.com/husky-a300-unmanned-ground-vehicle-robot/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:27:22</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_363">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://clearpathrobotics.com/husky-a300-unmanned-ground-vehicle-robot/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:27:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://www.stereolabs.com/en-fr/products/zed-2">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_362"/>
        <dc:title>ZED 2 - AI Stereo Camera | Stereolabs</dc:title>
        <dcterms:abstract>The ZED 2 family is a next-generation series of USB 3.1 stereo cameras that seamlessly integrate advanced depth sensing with AI capabilities. This combination empowers you to develop cutting-edge spatial intelligence applications.</dcterms:abstract>
        <z:language>en-FR</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.stereolabs.com/en-fr/products/zed-2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:27:25</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_362">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.stereolabs.com/en-fr/products/zed-2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:27:35</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://www.axis.com/products/axis-q62-series">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_365"/>
        <dc:title>AXIS Q62 PTZ Camera Series | Axis Communications</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.axis.com/products/axis-q62-series</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:27:35</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_365">
        <z:itemType>attachment</z:itemType>
        <dc:title>AXIS Q62 PTZ Camera Series | Axis Communications</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.axis.com/products/axis-q62-series</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:27:41</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://ouster.com/products/hardware/osdome-lidar-sensor">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <dc:title>Ouster OSDome: 180º FOV Hemispherical Lidar for Warehouse &amp; Security | Ouster</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ouster.com/products/hardware/osdome-lidar-sensor</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:31:11</dcterms:dateSubmitted>
    </bib:Document>
    <bib:Document rdf:about="https://ouster.com/products/hardware/os1-lidar-sensor">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_369"/>
        <dc:title>OS1: High-Res Mid-Range Lidar Sensor for Automation &amp; Security | Ouster</dc:title>
        <dcterms:abstract>Discover OS1 by Ouster: a mid-range lidar sensor designed for precision mapping, robotics, trucking, and security. Learn more here.</dcterms:abstract>
        <z:language>en</z:language>
        <z:shortTitle>OS1</z:shortTitle>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ouster.com/products/hardware/os1-lidar-sensor</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:32:29</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_369">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ouster.com/products/hardware/os1-lidar-sensor</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:32:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://www.sbg-systems.com/ins/ellipse-d/">
        <z:itemType>blogPost</z:itemType>
        <dcterms:isPartOf>
           <z:Blog><dc:title>SBG Systems</dc:title></z:Blog>
        </dcterms:isPartOf>
        <dc:title>Ellipse-D</dc:title>
        <dcterms:abstract>Get precise heading and centimeter level position accuracy with Ellipse-D, the smallest dual-antenna RTK INS with multi-band GNSS. Read more.</dcterms:abstract>
        <z:language>en-US</z:language>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.sbg-systems.com/ins/ellipse-d/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:32:46</dcterms:dateSubmitted>
    </bib:Document>
    <bib:Document rdf:about="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website><dc:title>NVIDIA</dc:title></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_372"/>
        <dc:title>NVIDIA Jetson AGX Orin</dc:title>
        <dcterms:abstract>Next-level AI performance for next-gen robotics.</dcterms:abstract>
        <z:language>en-us</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:32:48</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_372">
        <z:itemType>attachment</z:itemType>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:32:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://www.viewprotech.com/index.php?ac=article&amp;at=read&amp;did=279">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <dc:title>Q10F-Single Sensor Series-Viewpro Ltd</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.viewprotech.com/index.php?ac=article&amp;at=read&amp;did=279</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:34:44</dcterms:dateSubmitted>
    </bib:Document>
    <bib:Document rdf:about="https://www.cubepilot.com/#/cube/features">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <link:link rdf:resource="#item_375"/>
        <dc:title>CubePilot | Autopilot-on-Module | Blue Manufactured in USA | Blue Assembled in USA | Pixhawk Original Team</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.cubepilot.com/#/cube/features</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:34:45</dcterms:dateSubmitted>
    </bib:Document>
    <z:Attachment rdf:about="#item_375">
        <z:itemType>attachment</z:itemType>
        <dc:title>CubePilot | Autopilot-on-Module | Blue Manufactured in USA | Blue Assembled in USA | Pixhawk Original Team</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://www.cubepilot.com/#/cube/features</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:34:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Data rdf:about="https://github.com/ArduPilot/ardupilot">
        <z:itemType>computerProgram</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>ArduPilot</foaf:name></foaf:Organization>
        </dc:publisher>
        <dc:subject>
           <z:AutomaticTag><rdf:value>arducopter</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>ardupilot</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>arduplane</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>ardurover</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>ardusub</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>autopilot</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>auv</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>copter</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>drone</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>dronekit</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>mavlink</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>plane</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>robotics</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>ros</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>rov</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>rover</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>sub</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>uas</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>uav</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>ugv</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>ArduPilot/ardupilot</dc:title>
        <dcterms:abstract>ArduPlane, ArduCopter, ArduRover, ArduSub source</dcterms:abstract>
        <dc:date>2025-07-03T11:48:48Z</dc:date>
        <z:libraryCatalog>GitHub</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://github.com/ArduPilot/ardupilot</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2025-07-03 12:35:00</dcterms:dateSubmitted>
        <dc:rights>GPL-3.0</dc:rights>
        <dc:description>original-date: 2013-01-09T00:58:52Z</dc:description>
        <z:programmingLanguage>C++</z:programmingLanguage>
    </bib:Data>
</rdf:RDF>
