<!-- TOC -->
* [Aero-terrestrial collaboration](#aero-terrestrial-collaboration)
  * [For global planning](#for-global-planning)
    * [Active Autonomous Aerial Exploration for Ground Robot Path Planning](#active-autonomous-aerial-exploration-for-ground-robot-path-planning)
  * [For local planning](#for-local-planning)
    * [Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments](#dual-bev-nav-dual-layer-bev-based-heuristic-path-planning-for-robotic-navigation-in-unstructured-outdoor-environments)
      * [LBPM](#lbpm)
        * [Local BEV Perception encoder](#local-bev-perception-encoder)
        * [Task-driven goal decoder](#task-driven-goal-decoder)
      * [GBPM](#gbpm)
        * [Use GBPM with potential trajectories generated by LBPM](#use-gbpm-with-potential-trajectories-generated-by-lbpm)
    * [Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images](#learning-to-drive-off-road-on-smooth-terrain-in-unstructured-environments-using-an-on-board-camera-and-sparse-aerial-images)
    * [UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation](#uav-assisted-self-supervised-terrain-awareness-for-off-road-navigation)
  * [Exploration](#exploration)
    * [A Collaborative Aerial-Ground Robotic System for Fast Exploration](#a-collaborative-aerial-ground-robotic-system-for-fast-exploration)
* [Terrain classification/segmentation with BEV](#terrain-classificationsegmentation-with-bev)
  * [General terrain](#general-terrain)
    * [Visual Terrain Classification by Flying Robots](#visual-terrain-classification-by-flying-robots)
    * [Experimental Analysis of Overhead Data Processing To Support Long Range Navigation](#experimental-analysis-of-overhead-data-processing-to-support-long-range-navigation)
    * [A two-stage level set evolution scheme for man-made objects detection in aerial images](#a-two-stage-level-set-evolution-scheme-for-man-made-objects-detection-in-aerial-images)
    * [3D Convolutional Neural Networks for Landing Zone Detection from LiDAR](#3d-convolutional-neural-networks-for-landing-zone-detection-from-lidar)
  * [Forest trails](#forest-trails)
    * [A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots](#a-machine-learning-approach-to-visual-perception-of-forest-trails-for-mobile-robots)
    * [Appearance Contrast for Fast, Robust Trail-Following](#appearance-contrast-for-fast-robust-trail-following)
    * [A Trail-Following Robot Which Uses Appearance and Structural Cues](#a-trail-following-robot-which-uses-appearance-and-structural-cues)
<!-- TOC -->

# Aero-terrestrial collaboration

## For global planning

### Active Autonomous Aerial Exploration for Ground Robot Path Planning

> [!NOTE]
> https://ieeexplore.ieee.org/document/7812671


> [!TIP]
> \[Exploration] \[Collaboration] \[Terrain classification] \[3D terrain reconstruction]

1. Initial manual fly to see goal. Camera imagery is also used to obtain initial classification of the terrain.
2. Then vision-guided flights (to a series of waypoints) chosen actively. For each waypoint 3D reconstruction and ground
   robot path (to optimize total duration of the mission).
3. Repeat 2. until path is complete


- visual odometry for loc&nav [^3], [^4]
- [^3] also seems to mention matching of FPV (ground drone) and BEV (aerial drone) (p191)
- classifier is trained on the spot [^1]. Two models type tried, feature-based and CNN. CNN is significantly slower
  without giving significant better result (surprising).
- in the 2. not exhaustive exploration just along the _global path_ from the manually generated map.
  The next waypoints for the aerial drone are chosen as to minimize
  $T_{\text{ground robot}, s \rightarrow b_i} + T_{\text{ground robot}, b_i \rightarrow g} + T_{extend 3d reconstructed region}$,
  respectively _time of ground robot from s to next ground robot waypoint (uses 3d reconstructed area)_, the _time of
  ground robot from next ground robot waypoint (uses initial partial map)_, the _time to extend the 3d reconstructed
  region (in the correct direction)_.
- 7,8,9 are ref for high altitude, high resolution aerial images
- use of monocular camera to reconstruct 3D ground in real time [^2] (could be used with move_base_flex).
- use of [ANYbotics Grid Map](https://github.com/ANYbotics/grid_map) [^5]
- Terrain classification, dense 3D reconstruction and exploration algorithm run on a laptop computer not on the drone.

[^1]: https://rpg.ifi.uzh.ch/docs/ISER16_Delmerico.pdf

[^2]: https://rpg.ifi.uzh.ch/docs/ICRA14_Pizzoli.pdf

[^3]: https://rpg.ifi.uzh.ch/docs/PhD16_Forster.pdf

[^4]: https://ieeexplore.ieee.org/document/6906584

[^5]: https://www.researchgate.net/publication/284415855_A_Universal_Grid_Map_Library_Implementation_and_Use_Case_for_Rough_Terrain_Navigation

<details>
<summary>BibTex</summary>

```latex
@ARTICLE{delmerico2017activeautonomousaerialexplorationforgroundrobotpathplanning,
author = {Delmerico, Jeffrey and Mueggler, Elias and Nitsch, Julia and Scaramuzza, Davide},
journal = {IEEE Robotics and Automation Letters},
title = {Active Autonomous Aerial Exploration for Ground Robot Path Planning},
year = {2017},
volume = {2},
number = {2},
pages = {664-671},
keywords = {Three-dimensional displays;Path planning;Time factors;Cameras;Robot vision systems;Training;Search and rescue robots;motion and path planning;visual-based navigation},
doi = {10.1109/LRA.2017.2651163}
}
```

</details>

## For local planning

### Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments

> [!NOTE]
> https://arxiv.org/pdf/2501.18351

LBPM Local BEV plannning model = local BEV perception encoder + task-driven goal decoder

#### LBPM

##### Local BEV Perception encoder

inputs: (i) context observation $o_{t-P:t-1}$ (ii) current observation $o_t$.

- BEV transformation on the observation
- Feature extract based on LSS method. Uses EfficientNet
- Uses LSS and BEVDet to predict discrete depth distribution for each pixel
- BEV transformation (If multiple feature -> BEV pooling using BEVFusion)

_I'm confused about what is new compared to LSS. At least until the computation of the
BEV features (see Fig. 2), it is every similar. Also (this is one of the difference compared to LSS), I
don't understand the point of the traversability features, is it to make the system
keep in mind what was traversable?_

##### Task-driven goal decoder

based on the ViKiNG architecture

inputs: environmental context $o_{t-P}$, current observation $o_t$, goal observation $o_\omega$.

#### GBPM

Provide traversability hint and overall direction.

- Use an overhead map from satellite images
- segmentation that gradually increase as they approach impassable.

> GBPM uses trajectory data to learn traversability in the
> overhead map [...], the more easily accessible areas will
> be covered by a larger number of trajectories.

- This step uses the probability predictions as a map (not a thresholds regulated
  output). [U-net](https://github.com/milesial/Pytorch-UNet) is used to do the segmentation.

##### Use GBPM with potential trajectories generated by LBPM

> First, the LBPM generates multiple potential traversable paths, providing information including temporal distance, GPS
> offsets, and waypoints from the current position to the goal. The GBPM encodes the overhead map into a global
> probability map, projecting traversable paths of LBPM onto this map.
>
> $cost = k \times score + (1-k) \times temporal\_distance$


<details>

<summary>BibTex</summary>

```latex
@misc{zhang2025dualbevnavduallayerbevbased,
title = {Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic Navigation in Unstructured Outdoor Environments},
author = {Jianfeng Zhang and Hanlin Dong and Jian Yang and Jiahui Liu and Shibo Huang and Ke Li and Xuan Tang and Xian Wei and Xiong You},
year = {2025},
eprint = {2501.18351},
archivePrefix = {arXiv},
primaryClass = {cs.RO},
url = {https://arxiv.org/abs/2501.18351},
}
```

</details>

### Learning to Drive Off Road on Smooth Terrain in Unstructured Environments Using an On-Board Camera and Sparse Aerial Images

> [!NOTE]
> https://arxiv.org/pdf/2004.04697


> [!TIP]
> \[Reinforcement learning] \[Terrain roughness classification] \[IMU] \[BEV & FPV]


> In this paper, we present a system for learning a navigation
> policy that **preferentially chooses smooth terrain** [...]. The
> emphasis of the paper, however, is not road classification per
> se, but rather to propose an approach for **online adaptive self-
> supervised learning** for off-road driving in rough terrain and
> to explore the synthesis of aerial and first-person (ground)
> sensing in this context.

- From BEV, FPS images, use CNN to predict the rougher terrain for $H$ steps (supplying an action for every step)
- use of Value Prediction Networks, a hybrid model-based and model-free reinforcement learning architecture.
    - > It is model-based as it implicitly learns a dynamics model for abstract states optimized for predicting future
      rewards and value functions.
    - > It is also model-free as it maps these encoded abstract states to rewards and value functions using direct
      experience with the environment prior to the planning phase.
- In training terrain roughness is estimated using IMU and obstacles using short-range LiDAR.
- Reward based on the difference between the prediction and the actual roughness
- It is classification as type of terrain is associated with different number and the model tries to predict the correct
  one

### UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation

> [!TIP]
> \[ResNet18] \[Traversability] \[BEV & FPV]


> [!NOTE]
> https://arxiv.org/pdf/2409.18253

- good documentation of related work
- datasets created could be very useful
- makes me think of the work done by Tom.

use of ResNet18 and a "homemade" MLP to predict $M_z$, $M_\omega$ and $M_p$
respectively vibration metric, bumpiness and electrical energy consumption.

BEV from UAV gives better prediction than FPS from UGV.

Using the BEV from UAV, can obtain maps (similar to occupancy grid) (see Fig5)
than can be used to choose "better" path.

<details>
<summary>BibTex</summary>

```latex
@misc{fortin2024uavassistedselfsupervisedterrainawareness,
title = {UAV-Assisted Self-Supervised Terrain Awareness for Off-Road Navigation},
author = {Jean-Michel Fortin and Olivier Gamache and William Fecteau and Effie Daum and William Larrivée-Hardy and François Pomerleau and Philippe Giguère},
year = {2024},
eprint = {2409.18253},
archivePrefix = {arXiv},
primaryClass = {cs.RO},
url = {https://arxiv.org/abs/2409.18253},
}
```

</details>

## Exploration

### A Collaborative Aerial-Ground Robotic System for Fast Exploration

> [!NOTE]
> https://typeset.io/pdf/a-collaborative-aerial-ground-robotic-system-for-fast-4l6aejojb1.pdf

> [!TIP]
> \[Exploration] \[Collaboration]

Collaboration of aerial and ground robots to speed up exploration process of a region.
Uses field to do path planning.

Seems like it won't help for the case with a Tundra/Barakuda and the case at hand.

# Terrain classification/segmentation with BEV

## General terrain

### Visual Terrain Classification by Flying Robots

> [!NOTE]
> https://ieeexplore.ieee.org/document/6224988

> [!TIP]
> \[Terrain segmentation] \[TSURF] \[LBP] \[LTP]

Comparison of SURF descriptor, LBP, LTP for visual outdoor terrain classification

- Random forest gives the best results
- TSURF good with small grid size and lower training time than LBP, LTP

<details>
<summary>BibTex</summary>

```latex
@INPROCEEDINGS{khan2012visualterrainclassificationbyflyingrobots,
author = {Khan, Yasir Niaz and Masselli, Andreas and Zell, Andreas},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
title = {Visual terrain classification by flying robots},
year = {2012},
volume = {},
number = {},
pages = {498-503},
keywords = {Robots;Vegetation;Visualization;Cameras;Accuracy;Feature extraction;Image resolution},
doi = {10.1109/ICRA.2012.6224988} }

```

</details>

### Experimental Analysis of Overhead Data Processing To Support Long Range Navigation

> [!NOTE]
> (application to DARPA) https://ieeexplore.ieee.org/document/4058754
> (classifier description)
> https://kilthub.cmu.edu/articles/journal_contribution/Terrain_Classification_from_Aerial_Data_to_Support_Ground_Vehicle_Navigation/6561173?file=12043478

> [!TIPS]
> \[only FPV] \[only terrestrial robot]

usage of LiDAR and imagery on (only) terrestrial robot and prior data (from variety of sources)
to achieve robust navigation.

### A two-stage level set evolution scheme for man-made objects detection in aerial images

> [!NOTE]
> https://www.researchgate.net/publication/4156206_A_two-stage_level_set_evolution_scheme_for_man-made_objects_detection_in_aerial_images

Detect region in images of man-made object

### 3D Convolutional Neural Networks for Landing Zone Detection from LiDAR

> [!NOTE]
> https://dimatura.net/publications/3dcnn_lz_maturana_scherer_icra15.pdf

Coupling of a volumetric occupancy map with a 3D CNN to distinguish between vegetation that may be landed on and solid
objects that should be avoided.

## Forest trails

### A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots

> [!NOTE]
> https://rpg.ifi.uzh.ch/docs/RAL16_Giusti.pdf
> http://bit.ly/perceivingtrails

> [!TIP]
> \[Forest trails direction classification] \[DNN] \[CNN]

- Clever technic to generate dataset: human with 3 cameras oriented left, center, right walks on a forest trails making
  sure to face the direction of the path. It generates, respectively, images with trails on the right side, center and
  left side
- The model predict if the trail is on the right side, center or left side
- 15+ GB of training testing dataset available (images left, center and right of forest trails)

### Appearance Contrast for Fast, Robust Trail-Following

> [!NOTE]
> http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_IROS_2009/papers/1405.pdf

> [!TIP]
> \[Forest trails segmentation]  \[Image] \[Lidar] \[only FPV]

### A Trail-Following Robot Which Uses Appearance and Structural Cues

> [!NOTE]
> http://nameless.cis.udel.edu/pubs/2012/RLK12/fsr2012_published.pdf


